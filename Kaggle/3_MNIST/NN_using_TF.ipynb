{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "369551ed-4df2-4d51-8d84-5b752615df1f",
    "_uuid": "8e8333fba32b23ae95b48117e8fb57d7d22f9581"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "a28c3504-39e6-4776-9167-fb447c6d666a",
    "_uuid": "414f81fd51c8d941497ee4251ca26a8e55bb1e31",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "ed442386-4b2a-4c04-a4f2-59e42d87ef00",
    "_uuid": "703effc985b839a8e1adb441146c8aa70b67deb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "2e0b7f95-1565-478c-b5d9-8c487cee389e",
    "_uuid": "27170aba8df9e7b1e4abac06bd5d438a7aab9f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Instances 42000\n",
      "Training Instances 28000\n",
      "Atrributes 784\n"
     ]
    }
   ],
   "source": [
    "print (\"Training Instances\", train.shape[0])\n",
    "print (\"Training Instances\", test.shape[0])\n",
    "print (\"Atrributes\", test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "bfae82aa-a666-47bc-8a14-a227cce125c1",
    "_uuid": "aabf018b267dce7ef233edd311382c76859c9814",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train.pop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "2c6e4836-3d75-4899-b056-350491c39b04",
    "_uuid": "e22cc170910693d977e06036e1e4f7b01882972a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, y_train, \n",
    "                                                    stratify=y_train,\n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b55657d2-787d-4075-85de-be70b1e69fbd",
    "_uuid": "9f4eb960703c51cd457eb65b5212b9e2a79c20cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset dimensions= (31500, 784) \tTraining labels= (31500, 10)\n",
      "Validation dataset dimensions= (10500, 784) \tValidation labels= (10500, 10)\n",
      "Testing Dataset dimensions= (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "def data_transform(data, labels):\n",
    "    data = data.astype(np.float32)\n",
    "    if labels is not None:\n",
    "        labels = (np.arange(10) == labels[:,None]).astype(np.float32)\n",
    "    return data, labels\n",
    "\n",
    "X_train, y_train = data_transform(X_train.values, y_train)\n",
    "X_val, y_val = data_transform(X_val.values, y_val)\n",
    "X_test, _ = data_transform(test.values, None)\n",
    "\n",
    "print (\"Training dataset dimensions=\",X_train.shape, \"\\tTraining labels=\",y_train.shape)\n",
    "print (\"Validation dataset dimensions=\",X_val.shape, \"\\tValidation labels=\",y_val.shape)\n",
    "print (\"Testing Dataset dimensions=\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3b476d44-f617-4e2a-bcae-f74cbd0f1b9c",
    "_uuid": "80200b109a1ee39f4987f0a1ef325f8cc737c5c0"
   },
   "source": [
    "## Building the Neural Network\n",
    "\n",
    "network consists of 5 fully connected layers with final softmax layer : \n",
    "1st layer = 200 x 784, \n",
    " 2nd layer = 200 x 100, \n",
    " 3rd layer = 100 x 60, \n",
    " 4th layer = 60 x 30, \n",
    " 5th layer = 30 x 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "e63a3f9d-fa0d-49e2-a5bc-d00d62c9816f",
    "_uuid": "6f704fa17a46987d6f119cd786c27477efa53b57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashish/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "K, L, M, N = 200, 100, 60, 30\n",
    "\n",
    "# input\n",
    "X = tf.placeholder(tf.float32, [None, 28*28])\n",
    "\n",
    "# weights and biases of 5 fully connected layers\n",
    "w1 = tf.Variable(tf.truncated_normal([28*28, K], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([K]))\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([K, L], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([L]))\n",
    "\n",
    "w3 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([M]))\n",
    "\n",
    "w4 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([N]))\n",
    "\n",
    "w5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "b5 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "bdb02c61-28ee-4d89-bb49-a2841bc6b2b1",
    "_uuid": "4477d2f42999420e67819fb42244008bd2e9247f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y1 = tf.nn.relu(tf.matmul(X, w1)+b1)\n",
    "y2 = tf.nn.relu(tf.matmul(y1, w2)+ b2)\n",
    "y3 = tf.nn.relu(tf.matmul(y2, w3)+ b3)\n",
    "y4 = tf.nn.relu(tf.matmul(y3, w4)+ b4)\n",
    "\n",
    "Y = tf.matmul(y4, w5)+ b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "863b1134-c1c5-4ff0-8c7e-408249377bae",
    "_uuid": "6f1d96ed6031a2ddbdd10a8a42e741d337f05d28",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_true = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "a5655e14-56a6-4c65-94aa-207706a64e7e",
    "_uuid": "87ee2c7b85b1a3dc5cca52028993a374499ac655",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=Y, labels=Y_true)\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "is_correct = tf.equal(tf.argmax(Y, 1),tf.argmax(Y_true,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "058f9b93bfe7889155a43f08f870552f78bca0e1"
   },
   "source": [
    "#### decaying(exponential)  learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "2904037a5a70fe8b4d7c8487609d6e7ced2ef12c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.003\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           10000, 0.8, staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "911ca779-58a4-40a7-be4a-dbb60c262388",
    "_uuid": "23579aac4e9aab5559536420f52827983e490247",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimize = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e770b096-89f9-4c3b-a4c3-406b4f2e2144",
    "_uuid": "caf148e90e3656c5d6b994875fc9f28db1dee01d",
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "ee72c645-225b-43b2-95f3-7988286bdfa1",
    "_uuid": "06bd4e39ade858939793330f3be406ea4b78c7ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "ed60f06d-ec2a-4a82-8aad-9d321c86536c",
    "_uuid": "ed9b93bad6b96a90cd2a5057474676b7c948c176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train_Loss=0.7822  Val_Loss=0.2571  Val_Acc=0.9247  eta=0.003000  global_step=315\n",
      "Epoch 2: Train_Loss=0.1952  Val_Loss=0.2312  Val_Acc=0.9352  eta=0.003000  global_step=630\n",
      "Epoch 3: Train_Loss=0.1455  Val_Loss=0.2134  Val_Acc=0.9431  eta=0.003000  global_step=945\n",
      "Epoch 4: Train_Loss=0.1270  Val_Loss=0.2427  Val_Acc=0.9384  eta=0.003000  global_step=1260\n",
      "Epoch 5: Train_Loss=0.1098  Val_Loss=0.2005  Val_Acc=0.9501  eta=0.003000  global_step=1575\n",
      "Epoch 6: Train_Loss=0.0910  Val_Loss=0.1766  Val_Acc=0.9565  eta=0.003000  global_step=1890\n",
      "Epoch 7: Train_Loss=0.0898  Val_Loss=0.1702  Val_Acc=0.9586  eta=0.003000  global_step=2205\n",
      "Epoch 8: Train_Loss=0.0781  Val_Loss=0.2083  Val_Acc=0.9520  eta=0.003000  global_step=2520\n",
      "Epoch 9: Train_Loss=0.0842  Val_Loss=0.1769  Val_Acc=0.9555  eta=0.003000  global_step=2835\n",
      "Epoch 10: Train_Loss=0.0706  Val_Loss=0.1548  Val_Acc=0.9642  eta=0.003000  global_step=3150\n",
      "Epoch 11: Train_Loss=0.0594  Val_Loss=0.1805  Val_Acc=0.9595  eta=0.003000  global_step=3465\n",
      "Epoch 12: Train_Loss=0.0659  Val_Loss=0.1677  Val_Acc=0.9632  eta=0.003000  global_step=3780\n",
      "Epoch 13: Train_Loss=0.0690  Val_Loss=0.1977  Val_Acc=0.9587  eta=0.003000  global_step=4095\n",
      "Epoch 14: Train_Loss=0.0588  Val_Loss=0.1558  Val_Acc=0.9662  eta=0.003000  global_step=4410\n",
      "Epoch 15: Train_Loss=0.0550  Val_Loss=0.2032  Val_Acc=0.9630  eta=0.003000  global_step=4725\n",
      "Epoch 16: Train_Loss=0.0589  Val_Loss=0.1998  Val_Acc=0.9573  eta=0.003000  global_step=5040\n",
      "Epoch 17: Train_Loss=0.0478  Val_Loss=0.1863  Val_Acc=0.9654  eta=0.003000  global_step=5355\n",
      "Epoch 18: Train_Loss=0.0471  Val_Loss=0.1817  Val_Acc=0.9661  eta=0.003000  global_step=5670\n",
      "Epoch 19: Train_Loss=0.0497  Val_Loss=0.2089  Val_Acc=0.9596  eta=0.003000  global_step=5985\n",
      "Epoch 20: Train_Loss=0.0529  Val_Loss=0.1823  Val_Acc=0.9643  eta=0.003000  global_step=6300\n",
      "Epoch 21: Train_Loss=0.0497  Val_Loss=0.1625  Val_Acc=0.9688  eta=0.003000  global_step=6615\n",
      "Epoch 22: Train_Loss=0.0560  Val_Loss=0.2596  Val_Acc=0.9456  eta=0.003000  global_step=6930\n",
      "Epoch 23: Train_Loss=0.0651  Val_Loss=0.2187  Val_Acc=0.9651  eta=0.003000  global_step=7245\n",
      "Epoch 24: Train_Loss=0.0512  Val_Loss=0.2197  Val_Acc=0.9661  eta=0.003000  global_step=7560\n",
      "Epoch 25: Train_Loss=0.0377  Val_Loss=0.1628  Val_Acc=0.9711  eta=0.003000  global_step=7875\n",
      "Epoch 26: Train_Loss=0.0437  Val_Loss=0.1742  Val_Acc=0.9707  eta=0.003000  global_step=8190\n",
      "Epoch 27: Train_Loss=0.0416  Val_Loss=0.2011  Val_Acc=0.9632  eta=0.003000  global_step=8505\n",
      "Epoch 28: Train_Loss=0.0502  Val_Loss=0.2207  Val_Acc=0.9651  eta=0.003000  global_step=8820\n",
      "Epoch 29: Train_Loss=0.0351  Val_Loss=0.1967  Val_Acc=0.9686  eta=0.003000  global_step=9135\n",
      "Epoch 30: Train_Loss=0.0417  Val_Loss=0.2086  Val_Acc=0.9675  eta=0.003000  global_step=9450\n",
      "Epoch 31: Train_Loss=0.0445  Val_Loss=0.1864  Val_Acc=0.9659  eta=0.003000  global_step=9765\n",
      "Epoch 32: Train_Loss=0.0388  Val_Loss=0.1930  Val_Acc=0.9667  eta=0.002400  global_step=10080\n",
      "Epoch 33: Train_Loss=0.0233  Val_Loss=0.1989  Val_Acc=0.9719  eta=0.002400  global_step=10395\n",
      "Epoch 34: Train_Loss=0.0161  Val_Loss=0.2002  Val_Acc=0.9727  eta=0.002400  global_step=10710\n",
      "Epoch 35: Train_Loss=0.0197  Val_Loss=0.2046  Val_Acc=0.9697  eta=0.002400  global_step=11025\n",
      "Epoch 36: Train_Loss=0.0258  Val_Loss=0.2625  Val_Acc=0.9629  eta=0.002400  global_step=11340\n",
      "Epoch 37: Train_Loss=0.0203  Val_Loss=0.2055  Val_Acc=0.9735  eta=0.002400  global_step=11655\n",
      "Epoch 38: Train_Loss=0.0190  Val_Loss=0.2042  Val_Acc=0.9711  eta=0.002400  global_step=11970\n",
      "Epoch 39: Train_Loss=0.0147  Val_Loss=0.2257  Val_Acc=0.9718  eta=0.002400  global_step=12285\n",
      "Epoch 40: Train_Loss=0.0198  Val_Loss=0.2981  Val_Acc=0.9651  eta=0.002400  global_step=12600\n",
      "Epoch 41: Train_Loss=0.0222  Val_Loss=0.2166  Val_Acc=0.9717  eta=0.002400  global_step=12915\n",
      "Epoch 42: Train_Loss=0.0226  Val_Loss=0.3122  Val_Acc=0.9693  eta=0.002400  global_step=13230\n",
      "Epoch 43: Train_Loss=0.0576  Val_Loss=0.2253  Val_Acc=0.9726  eta=0.002400  global_step=13545\n",
      "Epoch 44: Train_Loss=0.0433  Val_Loss=0.3018  Val_Acc=0.9660  eta=0.002400  global_step=13860\n",
      "Epoch 45: Train_Loss=0.0291  Val_Loss=0.2759  Val_Acc=0.9693  eta=0.002400  global_step=14175\n",
      "Epoch 46: Train_Loss=0.0267  Val_Loss=0.2295  Val_Acc=0.9709  eta=0.002400  global_step=14490\n",
      "Epoch 47: Train_Loss=0.0183  Val_Loss=0.2139  Val_Acc=0.9732  eta=0.002400  global_step=14805\n",
      "Epoch 48: Train_Loss=0.0092  Val_Loss=0.2558  Val_Acc=0.9730  eta=0.002400  global_step=15120\n",
      "Epoch 49: Train_Loss=0.0259  Val_Loss=0.2000  Val_Acc=0.9694  eta=0.002400  global_step=15435\n",
      "Epoch 50: Train_Loss=0.0225  Val_Loss=0.2617  Val_Acc=0.9720  eta=0.002400  global_step=15750\n",
      "Epoch 51: Train_Loss=0.0206  Val_Loss=0.2241  Val_Acc=0.9702  eta=0.002400  global_step=16065\n",
      "Epoch 52: Train_Loss=0.0149  Val_Loss=0.2312  Val_Acc=0.9760  eta=0.002400  global_step=16380\n",
      "Epoch 53: Train_Loss=0.0115  Val_Loss=0.2575  Val_Acc=0.9745  eta=0.002400  global_step=16695\n",
      "Epoch 54: Train_Loss=0.0251  Val_Loss=0.2753  Val_Acc=0.9731  eta=0.002400  global_step=17010\n",
      "Epoch 55: Train_Loss=0.0399  Val_Loss=0.2632  Val_Acc=0.9670  eta=0.002400  global_step=17325\n",
      "Epoch 56: Train_Loss=0.0144  Val_Loss=0.2572  Val_Acc=0.9733  eta=0.002400  global_step=17640\n",
      "Epoch 57: Train_Loss=0.0234  Val_Loss=0.2017  Val_Acc=0.9729  eta=0.002400  global_step=17955\n",
      "Epoch 58: Train_Loss=0.0247  Val_Loss=0.2364  Val_Acc=0.9706  eta=0.002400  global_step=18270\n",
      "Epoch 59: Train_Loss=0.0214  Val_Loss=0.2406  Val_Acc=0.9718  eta=0.002400  global_step=18585\n",
      "Epoch 60: Train_Loss=0.0144  Val_Loss=0.1975  Val_Acc=0.9753  eta=0.002400  global_step=18900\n",
      "Epoch 61: Train_Loss=0.0152  Val_Loss=0.2338  Val_Acc=0.9729  eta=0.002400  global_step=19215\n",
      "Epoch 62: Train_Loss=0.0098  Val_Loss=0.2508  Val_Acc=0.9743  eta=0.002400  global_step=19530\n",
      "Epoch 63: Train_Loss=0.0106  Val_Loss=0.2827  Val_Acc=0.9716  eta=0.002400  global_step=19845\n",
      "Epoch 64: Train_Loss=0.0123  Val_Loss=0.2693  Val_Acc=0.9734  eta=0.001920  global_step=20160\n",
      "Epoch 65: Train_Loss=0.0071  Val_Loss=0.2569  Val_Acc=0.9746  eta=0.001920  global_step=20475\n",
      "Epoch 66: Train_Loss=0.0096  Val_Loss=0.2814  Val_Acc=0.9737  eta=0.001920  global_step=20790\n",
      "Epoch 67: Train_Loss=0.0056  Val_Loss=0.2870  Val_Acc=0.9742  eta=0.001920  global_step=21105\n",
      "Epoch 68: Train_Loss=0.0051  Val_Loss=0.3363  Val_Acc=0.9737  eta=0.001920  global_step=21420\n",
      "Epoch 69: Train_Loss=0.0172  Val_Loss=0.3270  Val_Acc=0.9654  eta=0.001920  global_step=21735\n",
      "Epoch 70: Train_Loss=0.0156  Val_Loss=0.2771  Val_Acc=0.9728  eta=0.001920  global_step=22050\n",
      "Epoch 71: Train_Loss=0.0131  Val_Loss=0.2817  Val_Acc=0.9700  eta=0.001920  global_step=22365\n",
      "Epoch 72: Train_Loss=0.0076  Val_Loss=0.2800  Val_Acc=0.9733  eta=0.001920  global_step=22680\n",
      "Epoch 73: Train_Loss=0.0048  Val_Loss=0.2804  Val_Acc=0.9739  eta=0.001920  global_step=22995\n",
      "Epoch 74: Train_Loss=0.0023  Val_Loss=0.3462  Val_Acc=0.9731  eta=0.001920  global_step=23310\n",
      "Epoch 75: Train_Loss=0.0056  Val_Loss=0.2582  Val_Acc=0.9745  eta=0.001920  global_step=23625\n",
      "Epoch 76: Train_Loss=0.0065  Val_Loss=0.3745  Val_Acc=0.9709  eta=0.001920  global_step=23940\n",
      "Epoch 77: Train_Loss=0.0193  Val_Loss=0.2726  Val_Acc=0.9726  eta=0.001920  global_step=24255\n",
      "Epoch 78: Train_Loss=0.0091  Val_Loss=0.3684  Val_Acc=0.9707  eta=0.001920  global_step=24570\n",
      "Epoch 79: Train_Loss=0.0185  Val_Loss=0.2407  Val_Acc=0.9745  eta=0.001920  global_step=24885\n",
      "Epoch 80: Train_Loss=0.0077  Val_Loss=0.2543  Val_Acc=0.9765  eta=0.001920  global_step=25200\n",
      "Epoch 81: Train_Loss=0.0055  Val_Loss=0.3757  Val_Acc=0.9746  eta=0.001920  global_step=25515\n",
      "Epoch 82: Train_Loss=0.0103  Val_Loss=0.2812  Val_Acc=0.9758  eta=0.001920  global_step=25830\n",
      "Epoch 83: Train_Loss=0.0125  Val_Loss=0.3416  Val_Acc=0.9668  eta=0.001920  global_step=26145\n",
      "Epoch 84: Train_Loss=0.0099  Val_Loss=0.2699  Val_Acc=0.9748  eta=0.001920  global_step=26460\n",
      "Epoch 85: Train_Loss=0.0074  Val_Loss=0.3868  Val_Acc=0.9734  eta=0.001920  global_step=26775\n",
      "Epoch 86: Train_Loss=0.0145  Val_Loss=0.3323  Val_Acc=0.9656  eta=0.001920  global_step=27090\n",
      "Epoch 87: Train_Loss=0.0327  Val_Loss=0.3952  Val_Acc=0.9710  eta=0.001920  global_step=27405\n",
      "Epoch 88: Train_Loss=0.0121  Val_Loss=0.3252  Val_Acc=0.9744  eta=0.001920  global_step=27720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: Train_Loss=0.0079  Val_Loss=0.3365  Val_Acc=0.9752  eta=0.001920  global_step=28035\n",
      "Epoch 90: Train_Loss=0.0045  Val_Loss=0.3373  Val_Acc=0.9750  eta=0.001920  global_step=28350\n",
      "Epoch 91: Train_Loss=0.0021  Val_Loss=0.4020  Val_Acc=0.9751  eta=0.001920  global_step=28665\n",
      "Epoch 92: Train_Loss=0.0021  Val_Loss=0.4395  Val_Acc=0.9743  eta=0.001920  global_step=28980\n",
      "Epoch 93: Train_Loss=0.0115  Val_Loss=0.4629  Val_Acc=0.9715  eta=0.001920  global_step=29295\n",
      "Epoch 94: Train_Loss=0.0273  Val_Loss=0.3096  Val_Acc=0.9730  eta=0.001920  global_step=29610\n",
      "Epoch 95: Train_Loss=0.0373  Val_Loss=0.3737  Val_Acc=0.9699  eta=0.001920  global_step=29925\n",
      "Epoch 96: Train_Loss=0.0233  Val_Loss=0.2735  Val_Acc=0.9725  eta=0.001536  global_step=30240\n",
      "Epoch 97: Train_Loss=0.0076  Val_Loss=0.3262  Val_Acc=0.9739  eta=0.001536  global_step=30555\n",
      "Epoch 98: Train_Loss=0.0053  Val_Loss=0.3149  Val_Acc=0.9740  eta=0.001536  global_step=30870\n",
      "Epoch 99: Train_Loss=0.0060  Val_Loss=0.3935  Val_Acc=0.9748  eta=0.001536  global_step=31185\n",
      "Epoch 100: Train_Loss=0.0112  Val_Loss=0.3576  Val_Acc=0.9733  eta=0.001536  global_step=31500\n",
      "Epoch 101: Train_Loss=0.0069  Val_Loss=0.3911  Val_Acc=0.9748  eta=0.001536  global_step=31815\n",
      "Epoch 102: Train_Loss=0.0053  Val_Loss=0.4469  Val_Acc=0.9736  eta=0.001536  global_step=32130\n",
      "Epoch 103: Train_Loss=0.0140  Val_Loss=0.3762  Val_Acc=0.9709  eta=0.001536  global_step=32445\n",
      "Epoch 104: Train_Loss=0.0052  Val_Loss=0.3795  Val_Acc=0.9752  eta=0.001536  global_step=32760\n",
      "Epoch 105: Train_Loss=0.0058  Val_Loss=0.4772  Val_Acc=0.9692  eta=0.001536  global_step=33075\n",
      "Epoch 106: Train_Loss=0.0093  Val_Loss=0.5084  Val_Acc=0.9734  eta=0.001536  global_step=33390\n",
      "Epoch 107: Train_Loss=0.0151  Val_Loss=0.4119  Val_Acc=0.9723  eta=0.001536  global_step=33705\n",
      "Epoch 108: Train_Loss=0.0090  Val_Loss=0.3444  Val_Acc=0.9750  eta=0.001536  global_step=34020\n",
      "Epoch 109: Train_Loss=0.0049  Val_Loss=0.4515  Val_Acc=0.9740  eta=0.001536  global_step=34335\n",
      "Epoch 110: Train_Loss=0.0170  Val_Loss=0.5306  Val_Acc=0.9696  eta=0.001536  global_step=34650\n",
      "Epoch 111: Train_Loss=0.0117  Val_Loss=0.5054  Val_Acc=0.9722  eta=0.001536  global_step=34965\n",
      "Epoch 112: Train_Loss=0.0034  Val_Loss=0.4969  Val_Acc=0.9730  eta=0.001536  global_step=35280\n",
      "Epoch 113: Train_Loss=0.0030  Val_Loss=0.4472  Val_Acc=0.9750  eta=0.001536  global_step=35595\n",
      "Epoch 114: Train_Loss=0.0031  Val_Loss=0.5503  Val_Acc=0.9736  eta=0.001536  global_step=35910\n",
      "Epoch 115: Train_Loss=0.0076  Val_Loss=0.4776  Val_Acc=0.9708  eta=0.001536  global_step=36225\n",
      "Epoch 116: Train_Loss=0.0199  Val_Loss=0.6300  Val_Acc=0.9719  eta=0.001536  global_step=36540\n",
      "Epoch 117: Train_Loss=0.0387  Val_Loss=0.2877  Val_Acc=0.9720  eta=0.001536  global_step=36855\n",
      "Epoch 118: Train_Loss=0.0058  Val_Loss=0.3771  Val_Acc=0.9736  eta=0.001536  global_step=37170\n",
      "Epoch 119: Train_Loss=0.0023  Val_Loss=0.4320  Val_Acc=0.9722  eta=0.001536  global_step=37485\n",
      "Epoch 120: Train_Loss=0.0034  Val_Loss=0.4102  Val_Acc=0.9754  eta=0.001536  global_step=37800\n",
      "Epoch 121: Train_Loss=0.0007  Val_Loss=0.4239  Val_Acc=0.9761  eta=0.001536  global_step=38115\n",
      "Epoch 122: Train_Loss=0.0003  Val_Loss=0.4499  Val_Acc=0.9757  eta=0.001536  global_step=38430\n",
      "Epoch 123: Train_Loss=0.0056  Val_Loss=0.4095  Val_Acc=0.9741  eta=0.001536  global_step=38745\n",
      "Epoch 124: Train_Loss=0.0046  Val_Loss=0.4613  Val_Acc=0.9731  eta=0.001536  global_step=39060\n",
      "Epoch 125: Train_Loss=0.0109  Val_Loss=0.4161  Val_Acc=0.9755  eta=0.001536  global_step=39375\n",
      "Epoch 126: Train_Loss=0.0161  Val_Loss=0.3931  Val_Acc=0.9725  eta=0.001536  global_step=39690\n",
      "Epoch 127: Train_Loss=0.0062  Val_Loss=0.3933  Val_Acc=0.9737  eta=0.001229  global_step=40005\n",
      "Epoch 128: Train_Loss=0.0054  Val_Loss=0.3622  Val_Acc=0.9739  eta=0.001229  global_step=40320\n",
      "Epoch 129: Train_Loss=0.0022  Val_Loss=0.3372  Val_Acc=0.9748  eta=0.001229  global_step=40635\n",
      "Epoch 130: Train_Loss=0.0004  Val_Loss=0.3466  Val_Acc=0.9747  eta=0.001229  global_step=40950\n",
      "Epoch 131: Train_Loss=0.0004  Val_Loss=0.3523  Val_Acc=0.9746  eta=0.001229  global_step=41265\n",
      "Epoch 132: Train_Loss=0.0004  Val_Loss=0.3565  Val_Acc=0.9746  eta=0.001229  global_step=41580\n",
      "Epoch 133: Train_Loss=0.0004  Val_Loss=0.3603  Val_Acc=0.9745  eta=0.001229  global_step=41895\n",
      "Epoch 134: Train_Loss=0.0004  Val_Loss=0.3640  Val_Acc=0.9748  eta=0.001229  global_step=42210\n",
      "Epoch 135: Train_Loss=0.0004  Val_Loss=0.3668  Val_Acc=0.9748  eta=0.001229  global_step=42525\n",
      "Epoch 136: Train_Loss=0.0004  Val_Loss=0.3706  Val_Acc=0.9748  eta=0.001229  global_step=42840\n",
      "Epoch 137: Train_Loss=0.0004  Val_Loss=0.3740  Val_Acc=0.9748  eta=0.001229  global_step=43155\n",
      "Epoch 138: Train_Loss=0.0004  Val_Loss=0.3774  Val_Acc=0.9748  eta=0.001229  global_step=43470\n",
      "Epoch 139: Train_Loss=0.0004  Val_Loss=0.3808  Val_Acc=0.9748  eta=0.001229  global_step=43785\n",
      "Epoch 140: Train_Loss=0.0004  Val_Loss=0.3851  Val_Acc=0.9748  eta=0.001229  global_step=44100\n",
      "Epoch 141: Train_Loss=0.0004  Val_Loss=0.3888  Val_Acc=0.9748  eta=0.001229  global_step=44415\n",
      "Epoch 142: Train_Loss=0.0004  Val_Loss=0.3927  Val_Acc=0.9748  eta=0.001229  global_step=44730\n",
      "Epoch 143: Train_Loss=0.0004  Val_Loss=0.3966  Val_Acc=0.9749  eta=0.001229  global_step=45045\n",
      "Epoch 144: Train_Loss=0.0004  Val_Loss=0.4007  Val_Acc=0.9750  eta=0.001229  global_step=45360\n",
      "Epoch 145: Train_Loss=0.0004  Val_Loss=0.4050  Val_Acc=0.9750  eta=0.001229  global_step=45675\n",
      "Epoch 146: Train_Loss=0.0004  Val_Loss=0.4093  Val_Acc=0.9750  eta=0.001229  global_step=45990\n",
      "Epoch 147: Train_Loss=0.0004  Val_Loss=0.4166  Val_Acc=0.9751  eta=0.001229  global_step=46305\n",
      "Epoch 148: Train_Loss=0.0004  Val_Loss=0.4227  Val_Acc=0.9751  eta=0.001229  global_step=46620\n",
      "Epoch 149: Train_Loss=0.0004  Val_Loss=0.4273  Val_Acc=0.9751  eta=0.001229  global_step=46935\n",
      "Epoch 150: Train_Loss=0.0004  Val_Loss=0.4320  Val_Acc=0.9751  eta=0.001229  global_step=47250\n",
      "Epoch 151: Train_Loss=0.0004  Val_Loss=0.4367  Val_Acc=0.9751  eta=0.001229  global_step=47565\n",
      "Epoch 152: Train_Loss=0.0004  Val_Loss=0.4416  Val_Acc=0.9754  eta=0.001229  global_step=47880\n",
      "Epoch 153: Train_Loss=0.0004  Val_Loss=0.4466  Val_Acc=0.9753  eta=0.001229  global_step=48195\n",
      "Epoch 154: Train_Loss=0.0004  Val_Loss=0.4516  Val_Acc=0.9753  eta=0.001229  global_step=48510\n",
      "Epoch 155: Train_Loss=0.0004  Val_Loss=0.4566  Val_Acc=0.9753  eta=0.001229  global_step=48825\n",
      "Epoch 156: Train_Loss=0.0004  Val_Loss=0.4618  Val_Acc=0.9753  eta=0.001229  global_step=49140\n",
      "Epoch 157: Train_Loss=0.0004  Val_Loss=0.4670  Val_Acc=0.9754  eta=0.001229  global_step=49455\n",
      "Epoch 158: Train_Loss=0.0004  Val_Loss=0.4722  Val_Acc=0.9754  eta=0.001229  global_step=49770\n",
      "Epoch 159: Train_Loss=0.0004  Val_Loss=0.4762  Val_Acc=0.9754  eta=0.000983  global_step=50085\n",
      "Epoch 160: Train_Loss=0.0004  Val_Loss=0.4803  Val_Acc=0.9754  eta=0.000983  global_step=50400\n",
      "Epoch 161: Train_Loss=0.0004  Val_Loss=0.4845  Val_Acc=0.9754  eta=0.000983  global_step=50715\n",
      "Epoch 162: Train_Loss=0.0004  Val_Loss=0.4889  Val_Acc=0.9754  eta=0.000983  global_step=51030\n",
      "Epoch 163: Train_Loss=0.0004  Val_Loss=0.5113  Val_Acc=0.9754  eta=0.000983  global_step=51345\n",
      "Epoch 164: Train_Loss=0.0004  Val_Loss=0.5159  Val_Acc=0.9754  eta=0.000983  global_step=51660\n",
      "Epoch 165: Train_Loss=0.0004  Val_Loss=0.5207  Val_Acc=0.9754  eta=0.000983  global_step=51975\n",
      "Epoch 166: Train_Loss=0.0004  Val_Loss=0.5257  Val_Acc=0.9756  eta=0.000983  global_step=52290\n",
      "Epoch 167: Train_Loss=0.0004  Val_Loss=0.5309  Val_Acc=0.9756  eta=0.000983  global_step=52605\n",
      "Epoch 168: Train_Loss=0.0004  Val_Loss=0.5362  Val_Acc=0.9755  eta=0.000983  global_step=52920\n",
      "Epoch 169: Train_Loss=0.0004  Val_Loss=0.5417  Val_Acc=0.9755  eta=0.000983  global_step=53235\n",
      "Epoch 170: Train_Loss=0.0004  Val_Loss=0.5474  Val_Acc=0.9756  eta=0.000983  global_step=53550\n",
      "Epoch 171: Train_Loss=0.0004  Val_Loss=0.5532  Val_Acc=0.9756  eta=0.000983  global_step=53865\n",
      "Epoch 172: Train_Loss=0.0004  Val_Loss=0.5592  Val_Acc=0.9756  eta=0.000983  global_step=54180\n",
      "Epoch 173: Train_Loss=0.0004  Val_Loss=0.5655  Val_Acc=0.9755  eta=0.000983  global_step=54495\n",
      "Epoch 174: Train_Loss=0.0003  Val_Loss=0.5718  Val_Acc=0.9755  eta=0.000983  global_step=54810\n",
      "Epoch 175: Train_Loss=0.0003  Val_Loss=0.5784  Val_Acc=0.9755  eta=0.000983  global_step=55125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176: Train_Loss=0.0003  Val_Loss=0.5851  Val_Acc=0.9755  eta=0.000983  global_step=55440\n",
      "Epoch 177: Train_Loss=0.0003  Val_Loss=0.5920  Val_Acc=0.9754  eta=0.000983  global_step=55755\n",
      "Epoch 178: Train_Loss=0.0003  Val_Loss=0.5989  Val_Acc=0.9754  eta=0.000983  global_step=56070\n",
      "Epoch 179: Train_Loss=0.0003  Val_Loss=0.6060  Val_Acc=0.9754  eta=0.000983  global_step=56385\n",
      "Epoch 180: Train_Loss=0.0003  Val_Loss=0.6132  Val_Acc=0.9754  eta=0.000983  global_step=56700\n",
      "Epoch 181: Train_Loss=0.0003  Val_Loss=0.6207  Val_Acc=0.9754  eta=0.000983  global_step=57015\n",
      "Epoch 182: Train_Loss=0.0003  Val_Loss=0.6283  Val_Acc=0.9753  eta=0.000983  global_step=57330\n",
      "Epoch 183: Train_Loss=0.0003  Val_Loss=0.6359  Val_Acc=0.9753  eta=0.000983  global_step=57645\n",
      "Epoch 184: Train_Loss=0.0003  Val_Loss=0.6435  Val_Acc=0.9753  eta=0.000983  global_step=57960\n",
      "Epoch 185: Train_Loss=0.0003  Val_Loss=0.6515  Val_Acc=0.9753  eta=0.000983  global_step=58275\n",
      "Epoch 186: Train_Loss=0.0003  Val_Loss=0.6594  Val_Acc=0.9753  eta=0.000983  global_step=58590\n",
      "Epoch 187: Train_Loss=0.0003  Val_Loss=0.6673  Val_Acc=0.9752  eta=0.000983  global_step=58905\n",
      "Epoch 188: Train_Loss=0.0003  Val_Loss=0.6754  Val_Acc=0.9752  eta=0.000983  global_step=59220\n",
      "Epoch 189: Train_Loss=0.0003  Val_Loss=0.6837  Val_Acc=0.9752  eta=0.000983  global_step=59535\n",
      "Epoch 190: Train_Loss=0.0003  Val_Loss=0.6923  Val_Acc=0.9752  eta=0.000983  global_step=59850\n",
      "Epoch 191: Train_Loss=0.0004  Val_Loss=0.6982  Val_Acc=0.9752  eta=0.000786  global_step=60165\n",
      "Epoch 192: Train_Loss=0.0003  Val_Loss=0.7058  Val_Acc=0.9752  eta=0.000786  global_step=60480\n",
      "Epoch 193: Train_Loss=0.0003  Val_Loss=0.7136  Val_Acc=0.9752  eta=0.000786  global_step=60795\n",
      "Epoch 194: Train_Loss=0.0003  Val_Loss=0.7216  Val_Acc=0.9752  eta=0.000786  global_step=61110\n",
      "Epoch 195: Train_Loss=0.0003  Val_Loss=0.7297  Val_Acc=0.9752  eta=0.000786  global_step=61425\n",
      "Epoch 196: Train_Loss=0.0003  Val_Loss=0.7394  Val_Acc=0.9752  eta=0.000786  global_step=61740\n",
      "Epoch 197: Train_Loss=0.0003  Val_Loss=0.7474  Val_Acc=0.9752  eta=0.000786  global_step=62055\n",
      "Epoch 198: Train_Loss=0.0003  Val_Loss=0.7563  Val_Acc=0.9752  eta=0.000786  global_step=62370\n",
      "Epoch 199: Train_Loss=0.0003  Val_Loss=0.7653  Val_Acc=0.9753  eta=0.000786  global_step=62685\n",
      "Epoch 200: Train_Loss=0.0003  Val_Loss=0.7739  Val_Acc=0.9753  eta=0.000786  global_step=63000\n",
      "Epoch 201: Train_Loss=0.0003  Val_Loss=0.7813  Val_Acc=0.9753  eta=0.000786  global_step=63315\n",
      "Epoch 202: Train_Loss=0.0003  Val_Loss=0.7929  Val_Acc=0.9753  eta=0.000786  global_step=63630\n",
      "Epoch 203: Train_Loss=0.0003  Val_Loss=0.8025  Val_Acc=0.9754  eta=0.000786  global_step=63945\n",
      "Epoch 204: Train_Loss=0.0003  Val_Loss=0.8104  Val_Acc=0.9754  eta=0.000786  global_step=64260\n",
      "Epoch 205: Train_Loss=0.0003  Val_Loss=0.8165  Val_Acc=0.9754  eta=0.000786  global_step=64575\n",
      "Epoch 206: Train_Loss=0.0003  Val_Loss=0.8309  Val_Acc=0.9754  eta=0.000786  global_step=64890\n",
      "Epoch 207: Train_Loss=0.0003  Val_Loss=0.8448  Val_Acc=0.9754  eta=0.000786  global_step=65205\n",
      "Epoch 208: Train_Loss=0.0003  Val_Loss=0.8509  Val_Acc=0.9754  eta=0.000786  global_step=65520\n",
      "Epoch 209: Train_Loss=0.0003  Val_Loss=0.8569  Val_Acc=0.9754  eta=0.000786  global_step=65835\n",
      "Epoch 210: Train_Loss=0.0003  Val_Loss=0.8642  Val_Acc=0.9756  eta=0.000786  global_step=66150\n",
      "Epoch 211: Train_Loss=0.0003  Val_Loss=0.8733  Val_Acc=0.9755  eta=0.000786  global_step=66465\n",
      "Epoch 212: Train_Loss=0.0003  Val_Loss=0.8832  Val_Acc=0.9756  eta=0.000786  global_step=66780\n",
      "Epoch 213: Train_Loss=0.0003  Val_Loss=0.9064  Val_Acc=0.9756  eta=0.000786  global_step=67095\n",
      "Epoch 214: Train_Loss=0.0003  Val_Loss=0.9112  Val_Acc=0.9754  eta=0.000786  global_step=67410\n",
      "Epoch 215: Train_Loss=0.0232  Val_Loss=1.3405  Val_Acc=0.9737  eta=0.000786  global_step=67725\n",
      "Epoch 216: Train_Loss=0.0188  Val_Loss=0.9835  Val_Acc=0.9745  eta=0.000786  global_step=68040\n",
      "Epoch 217: Train_Loss=0.0178  Val_Loss=0.8844  Val_Acc=0.9745  eta=0.000786  global_step=68355\n",
      "Epoch 218: Train_Loss=0.0039  Val_Loss=0.8261  Val_Acc=0.9746  eta=0.000786  global_step=68670\n",
      "Epoch 219: Train_Loss=0.0008  Val_Loss=0.8554  Val_Acc=0.9750  eta=0.000786  global_step=68985\n",
      "Epoch 220: Train_Loss=0.0005  Val_Loss=0.8876  Val_Acc=0.9750  eta=0.000786  global_step=69300\n",
      "Epoch 221: Train_Loss=0.0004  Val_Loss=0.8982  Val_Acc=0.9750  eta=0.000786  global_step=69615\n",
      "Epoch 222: Train_Loss=0.0004  Val_Loss=0.9030  Val_Acc=0.9750  eta=0.000786  global_step=69930\n",
      "Epoch 223: Train_Loss=0.0004  Val_Loss=0.9070  Val_Acc=0.9750  eta=0.000629  global_step=70245\n",
      "Epoch 224: Train_Loss=0.0004  Val_Loss=0.9107  Val_Acc=0.9750  eta=0.000629  global_step=70560\n",
      "Epoch 225: Train_Loss=0.0004  Val_Loss=0.9145  Val_Acc=0.9750  eta=0.000629  global_step=70875\n",
      "Epoch 226: Train_Loss=0.0004  Val_Loss=0.9137  Val_Acc=0.9750  eta=0.000629  global_step=71190\n",
      "Epoch 227: Train_Loss=0.0004  Val_Loss=0.9181  Val_Acc=0.9751  eta=0.000629  global_step=71505\n",
      "Epoch 228: Train_Loss=0.0004  Val_Loss=0.9220  Val_Acc=0.9751  eta=0.000629  global_step=71820\n",
      "Epoch 229: Train_Loss=0.0004  Val_Loss=0.9286  Val_Acc=0.9750  eta=0.000629  global_step=72135\n",
      "Epoch 230: Train_Loss=0.0004  Val_Loss=0.9359  Val_Acc=0.9750  eta=0.000629  global_step=72450\n",
      "Epoch 231: Train_Loss=0.0004  Val_Loss=0.9415  Val_Acc=0.9751  eta=0.000629  global_step=72765\n",
      "Epoch 232: Train_Loss=0.0004  Val_Loss=0.9462  Val_Acc=0.9753  eta=0.000629  global_step=73080\n",
      "Epoch 233: Train_Loss=0.0004  Val_Loss=0.9491  Val_Acc=0.9753  eta=0.000629  global_step=73395\n",
      "Epoch 234: Train_Loss=0.0004  Val_Loss=0.9558  Val_Acc=0.9753  eta=0.000629  global_step=73710\n",
      "Epoch 235: Train_Loss=0.0004  Val_Loss=0.9625  Val_Acc=0.9754  eta=0.000629  global_step=74025\n",
      "Epoch 236: Train_Loss=0.0004  Val_Loss=0.9699  Val_Acc=0.9754  eta=0.000629  global_step=74340\n",
      "Epoch 237: Train_Loss=0.0004  Val_Loss=0.9760  Val_Acc=0.9752  eta=0.000629  global_step=74655\n",
      "Epoch 238: Train_Loss=0.0004  Val_Loss=0.9816  Val_Acc=0.9753  eta=0.000629  global_step=74970\n",
      "Epoch 239: Train_Loss=0.0004  Val_Loss=0.9838  Val_Acc=0.9753  eta=0.000629  global_step=75285\n",
      "Epoch 240: Train_Loss=0.0004  Val_Loss=0.9930  Val_Acc=0.9752  eta=0.000629  global_step=75600\n",
      "Epoch 241: Train_Loss=0.0004  Val_Loss=1.0009  Val_Acc=0.9752  eta=0.000629  global_step=75915\n",
      "Epoch 242: Train_Loss=0.0004  Val_Loss=1.0077  Val_Acc=0.9751  eta=0.000629  global_step=76230\n",
      "Epoch 243: Train_Loss=0.0004  Val_Loss=1.0152  Val_Acc=0.9750  eta=0.000629  global_step=76545\n",
      "Epoch 244: Train_Loss=0.0004  Val_Loss=1.0210  Val_Acc=0.9750  eta=0.000629  global_step=76860\n",
      "Epoch 245: Train_Loss=0.0004  Val_Loss=1.0271  Val_Acc=0.9750  eta=0.000629  global_step=77175\n",
      "Epoch 246: Train_Loss=0.0012  Val_Loss=1.3033  Val_Acc=0.9728  eta=0.000629  global_step=77490\n",
      "Epoch 247: Train_Loss=0.0167  Val_Loss=1.2839  Val_Acc=0.9723  eta=0.000629  global_step=77805\n",
      "Epoch 248: Train_Loss=0.0048  Val_Loss=1.2057  Val_Acc=0.9734  eta=0.000629  global_step=78120\n",
      "Epoch 249: Train_Loss=0.0010  Val_Loss=1.2082  Val_Acc=0.9734  eta=0.000629  global_step=78435\n",
      "Epoch 250: Train_Loss=0.0009  Val_Loss=1.2054  Val_Acc=0.9732  eta=0.000629  global_step=78750\n",
      "Epoch 251: Train_Loss=0.0009  Val_Loss=1.2024  Val_Acc=0.9732  eta=0.000629  global_step=79065\n",
      "Epoch 252: Train_Loss=0.0009  Val_Loss=1.1999  Val_Acc=0.9732  eta=0.000629  global_step=79380\n",
      "Epoch 253: Train_Loss=0.0009  Val_Loss=1.1979  Val_Acc=0.9732  eta=0.000629  global_step=79695\n",
      "Epoch 254: Train_Loss=0.0009  Val_Loss=1.1961  Val_Acc=0.9731  eta=0.000503  global_step=80010\n",
      "Epoch 255: Train_Loss=0.0009  Val_Loss=1.1952  Val_Acc=0.9734  eta=0.000503  global_step=80325\n",
      "Epoch 256: Train_Loss=0.0008  Val_Loss=1.1945  Val_Acc=0.9733  eta=0.000503  global_step=80640\n",
      "Epoch 257: Train_Loss=0.0008  Val_Loss=1.1944  Val_Acc=0.9733  eta=0.000503  global_step=80955\n",
      "Epoch 258: Train_Loss=0.0008  Val_Loss=1.1963  Val_Acc=0.9733  eta=0.000503  global_step=81270\n",
      "Epoch 259: Train_Loss=0.0008  Val_Loss=1.1966  Val_Acc=0.9733  eta=0.000503  global_step=81585\n",
      "Epoch 260: Train_Loss=0.0008  Val_Loss=1.1971  Val_Acc=0.9734  eta=0.000503  global_step=81900\n",
      "Epoch 261: Train_Loss=0.0008  Val_Loss=1.1980  Val_Acc=0.9734  eta=0.000503  global_step=82215\n",
      "Epoch 262: Train_Loss=0.0008  Val_Loss=1.2006  Val_Acc=0.9734  eta=0.000503  global_step=82530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 263: Train_Loss=0.0008  Val_Loss=1.2047  Val_Acc=0.9734  eta=0.000503  global_step=82845\n",
      "Epoch 264: Train_Loss=0.0008  Val_Loss=1.2141  Val_Acc=0.9737  eta=0.000503  global_step=83160\n",
      "Epoch 265: Train_Loss=0.0008  Val_Loss=1.2235  Val_Acc=0.9736  eta=0.000503  global_step=83475\n",
      "Epoch 266: Train_Loss=0.0008  Val_Loss=1.2297  Val_Acc=0.9736  eta=0.000503  global_step=83790\n",
      "Epoch 267: Train_Loss=0.0008  Val_Loss=1.2326  Val_Acc=0.9737  eta=0.000503  global_step=84105\n",
      "Epoch 268: Train_Loss=0.0007  Val_Loss=1.2410  Val_Acc=0.9739  eta=0.000503  global_step=84420\n",
      "Epoch 269: Train_Loss=0.0007  Val_Loss=1.2500  Val_Acc=0.9738  eta=0.000503  global_step=84735\n",
      "Epoch 270: Train_Loss=0.0007  Val_Loss=1.2579  Val_Acc=0.9735  eta=0.000503  global_step=85050\n",
      "Epoch 271: Train_Loss=0.0054  Val_Loss=1.2579  Val_Acc=0.9739  eta=0.000503  global_step=85365\n",
      "Epoch 272: Train_Loss=0.0077  Val_Loss=1.3024  Val_Acc=0.9759  eta=0.000503  global_step=85680\n",
      "Epoch 273: Train_Loss=0.0014  Val_Loss=1.2189  Val_Acc=0.9761  eta=0.000503  global_step=85995\n",
      "Epoch 274: Train_Loss=0.0003  Val_Loss=1.2314  Val_Acc=0.9760  eta=0.000503  global_step=86310\n",
      "Epoch 275: Train_Loss=0.0003  Val_Loss=1.2372  Val_Acc=0.9761  eta=0.000503  global_step=86625\n",
      "Epoch 276: Train_Loss=0.0003  Val_Loss=1.2426  Val_Acc=0.9760  eta=0.000503  global_step=86940\n",
      "Epoch 277: Train_Loss=0.0003  Val_Loss=1.2476  Val_Acc=0.9760  eta=0.000503  global_step=87255\n",
      "Epoch 278: Train_Loss=0.0003  Val_Loss=1.2525  Val_Acc=0.9761  eta=0.000503  global_step=87570\n",
      "Epoch 279: Train_Loss=0.0003  Val_Loss=1.2572  Val_Acc=0.9762  eta=0.000503  global_step=87885\n",
      "Epoch 280: Train_Loss=0.0003  Val_Loss=1.2617  Val_Acc=0.9762  eta=0.000503  global_step=88200\n",
      "Epoch 281: Train_Loss=0.0003  Val_Loss=1.2661  Val_Acc=0.9762  eta=0.000503  global_step=88515\n",
      "Epoch 282: Train_Loss=0.0003  Val_Loss=1.2704  Val_Acc=0.9763  eta=0.000503  global_step=88830\n",
      "Epoch 283: Train_Loss=0.0003  Val_Loss=1.2746  Val_Acc=0.9763  eta=0.000503  global_step=89145\n",
      "Epoch 284: Train_Loss=0.0003  Val_Loss=1.2788  Val_Acc=0.9763  eta=0.000503  global_step=89460\n",
      "Epoch 285: Train_Loss=0.0003  Val_Loss=1.2829  Val_Acc=0.9761  eta=0.000503  global_step=89775\n",
      "Epoch 286: Train_Loss=0.0003  Val_Loss=1.2868  Val_Acc=0.9762  eta=0.000403  global_step=90090\n",
      "Epoch 287: Train_Loss=0.0003  Val_Loss=1.2900  Val_Acc=0.9762  eta=0.000403  global_step=90405\n",
      "Epoch 288: Train_Loss=0.0003  Val_Loss=1.2932  Val_Acc=0.9762  eta=0.000403  global_step=90720\n",
      "Epoch 289: Train_Loss=0.0003  Val_Loss=1.2965  Val_Acc=0.9762  eta=0.000403  global_step=91035\n",
      "Epoch 290: Train_Loss=0.0003  Val_Loss=1.2995  Val_Acc=0.9762  eta=0.000403  global_step=91350\n",
      "Epoch 291: Train_Loss=0.0003  Val_Loss=1.3029  Val_Acc=0.9762  eta=0.000403  global_step=91665\n",
      "Epoch 292: Train_Loss=0.0003  Val_Loss=1.3063  Val_Acc=0.9762  eta=0.000403  global_step=91980\n",
      "Epoch 293: Train_Loss=0.0003  Val_Loss=1.3098  Val_Acc=0.9762  eta=0.000403  global_step=92295\n",
      "Epoch 294: Train_Loss=0.0003  Val_Loss=1.3134  Val_Acc=0.9761  eta=0.000403  global_step=92610\n",
      "Epoch 295: Train_Loss=0.0003  Val_Loss=1.3170  Val_Acc=0.9761  eta=0.000403  global_step=92925\n",
      "Epoch 296: Train_Loss=0.0003  Val_Loss=1.3208  Val_Acc=0.9760  eta=0.000403  global_step=93240\n",
      "Epoch 297: Train_Loss=0.0003  Val_Loss=1.3413  Val_Acc=0.9760  eta=0.000403  global_step=93555\n",
      "Epoch 298: Train_Loss=0.0003  Val_Loss=1.3449  Val_Acc=0.9760  eta=0.000403  global_step=93870\n",
      "Epoch 299: Train_Loss=0.0003  Val_Loss=1.3487  Val_Acc=0.9760  eta=0.000403  global_step=94185\n",
      "Epoch 300: Train_Loss=0.0003  Val_Loss=1.3526  Val_Acc=0.9760  eta=0.000403  global_step=94500\n",
      "Epoch 301: Train_Loss=0.0003  Val_Loss=1.3567  Val_Acc=0.9760  eta=0.000403  global_step=94815\n",
      "Epoch 302: Train_Loss=0.0003  Val_Loss=1.3611  Val_Acc=0.9759  eta=0.000403  global_step=95130\n",
      "Epoch 303: Train_Loss=0.0003  Val_Loss=1.3656  Val_Acc=0.9759  eta=0.000403  global_step=95445\n",
      "Epoch 304: Train_Loss=0.0003  Val_Loss=1.3703  Val_Acc=0.9759  eta=0.000403  global_step=95760\n",
      "Epoch 305: Train_Loss=0.0003  Val_Loss=1.3753  Val_Acc=0.9759  eta=0.000403  global_step=96075\n",
      "Epoch 306: Train_Loss=0.0003  Val_Loss=1.3804  Val_Acc=0.9759  eta=0.000403  global_step=96390\n",
      "Epoch 307: Train_Loss=0.0003  Val_Loss=1.3857  Val_Acc=0.9759  eta=0.000403  global_step=96705\n",
      "Epoch 308: Train_Loss=0.0003  Val_Loss=1.3909  Val_Acc=0.9759  eta=0.000403  global_step=97020\n",
      "Epoch 309: Train_Loss=0.0003  Val_Loss=1.3962  Val_Acc=0.9757  eta=0.000403  global_step=97335\n",
      "Epoch 310: Train_Loss=0.0003  Val_Loss=1.4016  Val_Acc=0.9757  eta=0.000403  global_step=97650\n",
      "Epoch 311: Train_Loss=0.0003  Val_Loss=1.4073  Val_Acc=0.9757  eta=0.000403  global_step=97965\n",
      "Epoch 312: Train_Loss=0.0003  Val_Loss=1.4132  Val_Acc=0.9757  eta=0.000403  global_step=98280\n",
      "Epoch 313: Train_Loss=0.0003  Val_Loss=1.4193  Val_Acc=0.9756  eta=0.000403  global_step=98595\n",
      "Epoch 314: Train_Loss=0.0003  Val_Loss=1.4256  Val_Acc=0.9756  eta=0.000403  global_step=98910\n",
      "Epoch 315: Train_Loss=0.0003  Val_Loss=1.4325  Val_Acc=0.9756  eta=0.000403  global_step=99225\n",
      "Epoch 316: Train_Loss=0.0003  Val_Loss=1.4522  Val_Acc=0.9757  eta=0.000403  global_step=99540\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-70a1e7ef1012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mcurr_epoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "batch_number = X_train.shape[0]//batch_size\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "for epoch_counter in range(500):\n",
    "    curr_epoch_loss = 0\n",
    "    start = 0\n",
    "    end = start + batch_size\n",
    "    \n",
    "    # training the network on batches\n",
    "    for batch_counter in range(batch_number):\n",
    "        batch_x = X_train[start:end]\n",
    "        batch_y = y_train[start:end]\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        \n",
    "        train_data = {X: batch_x, Y_true: batch_y}\n",
    "        _, batch_loss = sess.run([optimize,mean_loss], feed_dict=train_data)\n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    curr_epoch_loss /= batch_number   \n",
    "    val_data = {X: X_val, Y_true: y_val}\n",
    "    val_loss, val_accuracy = sess.run([mean_loss,accuracy], feed_dict=val_data)\n",
    "    \n",
    "    \n",
    "    if (epoch_counter + 1) % 100 == 0:\n",
    "        saver.save(sess, 'checkpoint_directory/neural_network',\n",
    "                   global_step = global_step.eval())\n",
    "    \n",
    "    print (\"Epoch %d: Train_Loss=%0.4f  Val_Loss=%0.4f  Val_Acc=%0.4f  eta=%0.6f  global_step=%d\"\n",
    "      % (epoch_counter+1, \n",
    "         curr_epoch_loss, \n",
    "         val_loss, \n",
    "         val_accuracy, \n",
    "         learning_rate.eval(session=sess),\n",
    "         global_step.eval(session=sess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ebba1dcc-4b9c-4127-bb57-3fd23a5d9345",
    "_uuid": "286e72b1423b91a46d02186074c81ca36db28b80"
   },
   "source": [
    "### applying softmax to generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3f8faa1e-c1f9-432b-9582-c1751896d1d2",
    "_uuid": "89559b82c4960c589ce9762be14fe6ed3b40c84f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = tf.argmax(tf.nn.softmax(Y),1)\n",
    "predictions = predict.eval(feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5e41b513-8d74-4efb-815f-105bfeb73df6",
    "_uuid": "178bf4270acfc9474dd91e596f09e6abe8000b09"
   },
   "outputs": [],
   "source": [
    "test_id = np.arange(1, len(X_test)+1)\n",
    "submission = pd.DataFrame({'ImageId': test_id, 'Label':predictions})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0325611-6c1b-430a-9974-75d708d17fd9",
    "_uuid": "4d487463df8ec38c6a1e6ce641c4eebad460f218",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('my_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
