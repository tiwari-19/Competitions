{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 11), (418, 11))"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "labels = train.pop('Survived')\n",
    "test_id = test.PassengerId\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_rows = len(train)\n",
    "data = pd.concat([train, test])\n",
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId       0\n",
       "Pclass            0\n",
       "Name              0\n",
       "Sex               0\n",
       "Age             263\n",
       "SibSp             0\n",
       "Parch             0\n",
       "Ticket            0\n",
       "Fare              1\n",
       "Cabin          1014\n",
       "Embarked          2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to assign sex cateogry for adults and children\n",
    "def assign_sex(age, sex, value):\n",
    "    if age >=18:\n",
    "        if sex == value:\n",
    "            return True\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "# function to assign title for a name\n",
    "def get_pos(nam, dic):\n",
    "    nam = nam.split(',')[1]\n",
    "    first_nam = nam.split('.')[0].strip()\n",
    "    return dic[first_nam]\n",
    "\n",
    "\n",
    "# main function to apply all preprocessing\n",
    "def preprocessing(X):\n",
    "    \n",
    "    # dropping unecessary columns\n",
    "    try:\n",
    "        X.drop(['PassengerId', 'Ticket'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # handling null values and normalizing values\n",
    "    from sklearn.preprocessing import Imputer, MinMaxScaler\n",
    "    im = Imputer(missing_values='NaN', strategy='median')\n",
    "    X[\"Age\"] = im.fit_transform(X[[\"Age\"]])\n",
    "\n",
    "    im = Imputer(missing_values='NaN', strategy='mean')\n",
    "    X[\"Fare\"] = im.fit_transform(X[['Fare']])\n",
    "\n",
    "    mm = MinMaxScaler()\n",
    "    X['Fare'] = mm.fit_transform(X[['Fare']])\n",
    "    X['Age'] = mm.fit_transform(X[['Age']])\n",
    "    \n",
    "    \n",
    "    # Creating new column Child\n",
    "    X['Child'], X['Man'], X['Woman'] = 0, 0, 0\n",
    "    X['Child'] = X['Age'].apply(lambda age: 1 if age<18 else 0)\n",
    "    X['Man'] = X.apply(lambda row: 1 if assign_sex(row[\"Age\"], row[\"Sex\"], 'male') else 0, axis=1)\n",
    "    X['Woman'] = X.apply(lambda row: 1 if assign_sex(row[\"Age\"], row[\"Sex\"], 'female') else 0, axis=1)\n",
    "    \n",
    "    \n",
    "    X['Cabin'] = X['Cabin'].apply(lambda x: 0 if type(x)==float else 1)  \n",
    "    \n",
    "    \n",
    "    # getting titles inplace of names   \n",
    "    dic = {             \"Capt\":       \"Officer\",\n",
    "                        \"Col\":        \"Officer\",\n",
    "                        \"Major\":      \"Officer\",\n",
    "                        \"Jonkheer\":   \"Royalty\",\n",
    "                        \"Don\":        \"Royalty\",\n",
    "                        \"Sir\" :       \"Royalty\",\n",
    "                        \"Dr\":         \"Officer\",\n",
    "                        \"Rev\":        \"Officer\",\n",
    "                        \"the Countess\":\"Royalty\",\n",
    "                        \"Dona\":       \"Royalty\",\n",
    "                        \"Mme\":        \"Mrs\",\n",
    "                        \"Mlle\":       \"Miss\",\n",
    "                        \"Ms\":         \"Mrs\",\n",
    "                        \"Mr\" :        \"Mr\",\n",
    "                        \"Mrs\" :       \"Mrs\",\n",
    "                        \"Miss\" :      \"Miss\",\n",
    "                        \"Master\" :    \"Master\",\n",
    "                        \"Lady\" :      \"Royalty\"}\n",
    "    \n",
    "    title = X['Name'].apply(lambda nam: get_pos(nam, dic))\n",
    "    df_title = pd.get_dummies(title, prefix='Title')\n",
    "    X = pd.concat([df_title, X], axis=1)\n",
    "    \n",
    "    X['Embarked'].fillna(value='S', inplace=True)\n",
    "    df_embarked = pd.get_dummies(X['Embarked'], prefix='Embarked')\n",
    "    X = pd.concat([df_embarked, X], axis=1)\n",
    "    \n",
    "    df_class = pd.get_dummies(X['Pclass'], prefix='Pclass')\n",
    "    X = pd.concat([df_class, X], axis=1)\n",
    "    \n",
    "    X.drop(['Name', 'Sex', 'Embarked', 'Pclass'], axis=1, inplace=True)\n",
    "    X.index += 1\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1309, 20)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = preprocessing(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data[:train_rows]\n",
    "test = data[train_rows:]\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_for_nn(data, labels):\n",
    "    data = data.values\n",
    "    if labels is not None:\n",
    "        labels = (np.arange(2) == labels[:, None]).astype(np.float32)\n",
    "        \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, labels = data_for_nn(train, labels)\n",
    "X_test, _ = data_for_nn(test, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((757, 20), (134, 20))"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, labels, test_size=0.15, stratify=labels)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n1, n2, n3 = 20, 10, 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n1])\n",
    "\n",
    "w1 = tf.Variable(tf.truncated_normal([n1, n2], stddev=0.1))\n",
    "w2 = tf.Variable(tf.truncated_normal([n2, n3], stddev=0.1))\n",
    "w3 = tf.Variable(tf.truncated_normal([n3, 2], stddev=0.1))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([n2]))\n",
    "b2 = tf.Variable(tf.zeros([n3]))\n",
    "b3 = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "\n",
    "y1 = tf.nn.relu(tf.matmul(X, w1)+b1)\n",
    "y2 = tf.nn.relu(tf.matmul(y1, w2)+b2)\n",
    "\n",
    "Y = tf.matmul(y2, w3)+b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_true = tf.placeholder(tf.float32, shape=[None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=Y, labels=Y_true)\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "'''starter_learning_rate = 0.003\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, \n",
    "                                           500, 0.7, staircase=True)\n",
    "'''\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step = optimizer.minimize(mean_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  Train_loss=0.693  Val_loss=0.693  Val_Acc=0.612  eta=0.00010 global_step=1\n",
      "Epoch 2:  Train_loss=0.693  Val_loss=0.693  Val_Acc=0.619  eta=0.00010 global_step=2\n",
      "Epoch 3:  Train_loss=0.693  Val_loss=0.693  Val_Acc=0.619  eta=0.00010 global_step=3\n",
      "Epoch 4:  Train_loss=0.693  Val_loss=0.693  Val_Acc=0.619  eta=0.00010 global_step=4\n",
      "Epoch 5:  Train_loss=0.693  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=5\n",
      "Epoch 6:  Train_loss=0.693  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=6\n",
      "Epoch 7:  Train_loss=0.693  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=7\n",
      "Epoch 8:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=8\n",
      "Epoch 9:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=9\n",
      "Epoch 10:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=10\n",
      "Epoch 11:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=11\n",
      "Epoch 12:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=12\n",
      "Epoch 13:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=13\n",
      "Epoch 14:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=14\n",
      "Epoch 15:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=15\n",
      "Epoch 16:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=16\n",
      "Epoch 17:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=17\n",
      "Epoch 18:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=18\n",
      "Epoch 19:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=19\n",
      "Epoch 20:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=20\n",
      "Epoch 21:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=21\n",
      "Epoch 22:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=22\n",
      "Epoch 23:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=23\n",
      "Epoch 24:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=24\n",
      "Epoch 25:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=25\n",
      "Epoch 26:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=26\n",
      "Epoch 27:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=27\n",
      "Epoch 28:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=28\n",
      "Epoch 29:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=29\n",
      "Epoch 30:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=30\n",
      "Epoch 31:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=31\n",
      "Epoch 32:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=32\n",
      "Epoch 33:  Train_loss=0.692  Val_loss=0.692  Val_Acc=0.619  eta=0.00010 global_step=33\n",
      "Epoch 34:  Train_loss=0.692  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=34\n",
      "Epoch 35:  Train_loss=0.692  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=35\n",
      "Epoch 36:  Train_loss=0.692  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=36\n",
      "Epoch 37:  Train_loss=0.692  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=37\n",
      "Epoch 38:  Train_loss=0.692  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=38\n",
      "Epoch 39:  Train_loss=0.692  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=39\n",
      "Epoch 40:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=40\n",
      "Epoch 41:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=41\n",
      "Epoch 42:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=42\n",
      "Epoch 43:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=43\n",
      "Epoch 44:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=44\n",
      "Epoch 45:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=45\n",
      "Epoch 46:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=46\n",
      "Epoch 47:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=47\n",
      "Epoch 48:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=48\n",
      "Epoch 49:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=49\n",
      "Epoch 50:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=50\n",
      "Epoch 51:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=51\n",
      "Epoch 52:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=52\n",
      "Epoch 53:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=53\n",
      "Epoch 54:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=54\n",
      "Epoch 55:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=55\n",
      "Epoch 56:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=56\n",
      "Epoch 57:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=57\n",
      "Epoch 58:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=58\n",
      "Epoch 59:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=59\n",
      "Epoch 60:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=60\n",
      "Epoch 61:  Train_loss=0.691  Val_loss=0.691  Val_Acc=0.619  eta=0.00010 global_step=61\n",
      "Epoch 62:  Train_loss=0.691  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=62\n",
      "Epoch 63:  Train_loss=0.691  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=63\n",
      "Epoch 64:  Train_loss=0.691  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=64\n",
      "Epoch 65:  Train_loss=0.691  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=65\n",
      "Epoch 66:  Train_loss=0.691  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=66\n",
      "Epoch 67:  Train_loss=0.691  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=67\n",
      "Epoch 68:  Train_loss=0.691  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=68\n",
      "Epoch 69:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=69\n",
      "Epoch 70:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=70\n",
      "Epoch 71:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=71\n",
      "Epoch 72:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=72\n",
      "Epoch 73:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=73\n",
      "Epoch 74:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=74\n",
      "Epoch 75:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=75\n",
      "Epoch 76:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=76\n",
      "Epoch 77:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=77\n",
      "Epoch 78:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=78\n",
      "Epoch 79:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=79\n",
      "Epoch 80:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=80\n",
      "Epoch 81:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=81\n",
      "Epoch 82:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=82\n",
      "Epoch 83:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=83\n",
      "Epoch 84:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=84\n",
      "Epoch 85:  Train_loss=0.690  Val_loss=0.690  Val_Acc=0.619  eta=0.00010 global_step=85\n",
      "Epoch 86:  Train_loss=0.690  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=86\n",
      "Epoch 87:  Train_loss=0.690  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=87\n",
      "Epoch 88:  Train_loss=0.690  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=88\n",
      "Epoch 89:  Train_loss=0.690  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=89\n",
      "Epoch 90:  Train_loss=0.690  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=90\n",
      "Epoch 91:  Train_loss=0.690  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=91\n",
      "Epoch 92:  Train_loss=0.690  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=92\n",
      "Epoch 93:  Train_loss=0.690  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=93\n",
      "Epoch 94:  Train_loss=0.690  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=94\n",
      "Epoch 95:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=95\n",
      "Epoch 96:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=96\n",
      "Epoch 97:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=97\n",
      "Epoch 98:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=98\n",
      "Epoch 99:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=99\n",
      "Epoch 100:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=100\n",
      "Epoch 101:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=101\n",
      "Epoch 102:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=102\n",
      "Epoch 103:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=103\n",
      "Epoch 104:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=104\n",
      "Epoch 105:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=105\n",
      "Epoch 106:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=106\n",
      "Epoch 107:  Train_loss=0.689  Val_loss=0.689  Val_Acc=0.619  eta=0.00010 global_step=107\n",
      "Epoch 108:  Train_loss=0.689  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=108\n",
      "Epoch 109:  Train_loss=0.689  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=109\n",
      "Epoch 110:  Train_loss=0.689  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=110\n",
      "Epoch 111:  Train_loss=0.689  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=111\n",
      "Epoch 112:  Train_loss=0.689  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=112\n",
      "Epoch 113:  Train_loss=0.689  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=113\n",
      "Epoch 114:  Train_loss=0.689  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=114\n",
      "Epoch 115:  Train_loss=0.689  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=115\n",
      "Epoch 116:  Train_loss=0.689  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=116\n",
      "Epoch 117:  Train_loss=0.689  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=117\n",
      "Epoch 118:  Train_loss=0.688  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=118\n",
      "Epoch 119:  Train_loss=0.688  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=119\n",
      "Epoch 120:  Train_loss=0.688  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=120\n",
      "Epoch 121:  Train_loss=0.688  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=121\n",
      "Epoch 122:  Train_loss=0.688  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=122\n",
      "Epoch 123:  Train_loss=0.688  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=123\n",
      "Epoch 124:  Train_loss=0.688  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=124\n",
      "Epoch 125:  Train_loss=0.688  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=125\n",
      "Epoch 126:  Train_loss=0.688  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=126\n",
      "Epoch 127:  Train_loss=0.688  Val_loss=0.688  Val_Acc=0.619  eta=0.00010 global_step=127\n",
      "Epoch 128:  Train_loss=0.688  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=128\n",
      "Epoch 129:  Train_loss=0.688  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=129\n",
      "Epoch 130:  Train_loss=0.688  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=130\n",
      "Epoch 131:  Train_loss=0.688  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=131\n",
      "Epoch 132:  Train_loss=0.688  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=132\n",
      "Epoch 133:  Train_loss=0.688  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=133\n",
      "Epoch 134:  Train_loss=0.688  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=134\n",
      "Epoch 135:  Train_loss=0.688  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=135\n",
      "Epoch 136:  Train_loss=0.688  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=136\n",
      "Epoch 137:  Train_loss=0.688  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=137\n",
      "Epoch 138:  Train_loss=0.688  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=138\n",
      "Epoch 139:  Train_loss=0.687  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=139\n",
      "Epoch 140:  Train_loss=0.687  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=140\n",
      "Epoch 141:  Train_loss=0.687  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=141\n",
      "Epoch 142:  Train_loss=0.687  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=142\n",
      "Epoch 143:  Train_loss=0.687  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=143\n",
      "Epoch 144:  Train_loss=0.687  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=144\n",
      "Epoch 145:  Train_loss=0.687  Val_loss=0.687  Val_Acc=0.619  eta=0.00010 global_step=145\n",
      "Epoch 146:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=146\n",
      "Epoch 147:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=147\n",
      "Epoch 148:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=149\n",
      "Epoch 150:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=150\n",
      "Epoch 151:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=151\n",
      "Epoch 152:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=152\n",
      "Epoch 153:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=153\n",
      "Epoch 154:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=154\n",
      "Epoch 155:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=155\n",
      "Epoch 156:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=156\n",
      "Epoch 157:  Train_loss=0.687  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=157\n",
      "Epoch 158:  Train_loss=0.686  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=158\n",
      "Epoch 159:  Train_loss=0.686  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=159\n",
      "Epoch 160:  Train_loss=0.686  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=160\n",
      "Epoch 161:  Train_loss=0.686  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=161\n",
      "Epoch 162:  Train_loss=0.686  Val_loss=0.686  Val_Acc=0.619  eta=0.00010 global_step=162\n",
      "Epoch 163:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=163\n",
      "Epoch 164:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=164\n",
      "Epoch 165:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=165\n",
      "Epoch 166:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=166\n",
      "Epoch 167:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=167\n",
      "Epoch 168:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=168\n",
      "Epoch 169:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=169\n",
      "Epoch 170:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=170\n",
      "Epoch 171:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=171\n",
      "Epoch 172:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=172\n",
      "Epoch 173:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=173\n",
      "Epoch 174:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=174\n",
      "Epoch 175:  Train_loss=0.686  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=175\n",
      "Epoch 176:  Train_loss=0.685  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=176\n",
      "Epoch 177:  Train_loss=0.685  Val_loss=0.685  Val_Acc=0.619  eta=0.00010 global_step=177\n",
      "Epoch 178:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=178\n",
      "Epoch 179:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=179\n",
      "Epoch 180:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=180\n",
      "Epoch 181:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=181\n",
      "Epoch 182:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=182\n",
      "Epoch 183:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=183\n",
      "Epoch 184:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=184\n",
      "Epoch 185:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=185\n",
      "Epoch 186:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=186\n",
      "Epoch 187:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=187\n",
      "Epoch 188:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=188\n",
      "Epoch 189:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=189\n",
      "Epoch 190:  Train_loss=0.685  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=190\n",
      "Epoch 191:  Train_loss=0.684  Val_loss=0.684  Val_Acc=0.619  eta=0.00010 global_step=191\n",
      "Epoch 192:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=192\n",
      "Epoch 193:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=193\n",
      "Epoch 194:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=194\n",
      "Epoch 195:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=195\n",
      "Epoch 196:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=196\n",
      "Epoch 197:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=197\n",
      "Epoch 198:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=198\n",
      "Epoch 199:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=199\n",
      "Epoch 200:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=200\n",
      "Epoch 201:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=201\n",
      "Epoch 202:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=202\n",
      "Epoch 203:  Train_loss=0.684  Val_loss=0.683  Val_Acc=0.619  eta=0.00010 global_step=203\n",
      "Epoch 204:  Train_loss=0.684  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=204\n",
      "Epoch 205:  Train_loss=0.683  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=205\n",
      "Epoch 206:  Train_loss=0.683  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=206\n",
      "Epoch 207:  Train_loss=0.683  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=207\n",
      "Epoch 208:  Train_loss=0.683  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=208\n",
      "Epoch 209:  Train_loss=0.683  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=209\n",
      "Epoch 210:  Train_loss=0.683  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=210\n",
      "Epoch 211:  Train_loss=0.683  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=211\n",
      "Epoch 212:  Train_loss=0.683  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=212\n",
      "Epoch 213:  Train_loss=0.683  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=213\n",
      "Epoch 214:  Train_loss=0.683  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=214\n",
      "Epoch 215:  Train_loss=0.683  Val_loss=0.682  Val_Acc=0.619  eta=0.00010 global_step=215\n",
      "Epoch 216:  Train_loss=0.683  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=216\n",
      "Epoch 217:  Train_loss=0.683  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=217\n",
      "Epoch 218:  Train_loss=0.682  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=218\n",
      "Epoch 219:  Train_loss=0.682  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=219\n",
      "Epoch 220:  Train_loss=0.682  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=220\n",
      "Epoch 221:  Train_loss=0.682  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=221\n",
      "Epoch 222:  Train_loss=0.682  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=222\n",
      "Epoch 223:  Train_loss=0.682  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=223\n",
      "Epoch 224:  Train_loss=0.682  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=224\n",
      "Epoch 225:  Train_loss=0.682  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=225\n",
      "Epoch 226:  Train_loss=0.682  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=226\n",
      "Epoch 227:  Train_loss=0.682  Val_loss=0.681  Val_Acc=0.619  eta=0.00010 global_step=227\n",
      "Epoch 228:  Train_loss=0.682  Val_loss=0.680  Val_Acc=0.619  eta=0.00010 global_step=228\n",
      "Epoch 229:  Train_loss=0.682  Val_loss=0.680  Val_Acc=0.619  eta=0.00010 global_step=229\n",
      "Epoch 230:  Train_loss=0.681  Val_loss=0.680  Val_Acc=0.619  eta=0.00010 global_step=230\n",
      "Epoch 231:  Train_loss=0.681  Val_loss=0.680  Val_Acc=0.619  eta=0.00010 global_step=231\n",
      "Epoch 232:  Train_loss=0.681  Val_loss=0.680  Val_Acc=0.619  eta=0.00010 global_step=232\n",
      "Epoch 233:  Train_loss=0.681  Val_loss=0.680  Val_Acc=0.619  eta=0.00010 global_step=233\n",
      "Epoch 234:  Train_loss=0.681  Val_loss=0.680  Val_Acc=0.619  eta=0.00010 global_step=234\n",
      "Epoch 235:  Train_loss=0.681  Val_loss=0.680  Val_Acc=0.619  eta=0.00010 global_step=235\n",
      "Epoch 236:  Train_loss=0.681  Val_loss=0.680  Val_Acc=0.619  eta=0.00010 global_step=236\n",
      "Epoch 237:  Train_loss=0.681  Val_loss=0.680  Val_Acc=0.619  eta=0.00010 global_step=237\n",
      "Epoch 238:  Train_loss=0.681  Val_loss=0.679  Val_Acc=0.619  eta=0.00010 global_step=238\n",
      "Epoch 239:  Train_loss=0.681  Val_loss=0.679  Val_Acc=0.619  eta=0.00010 global_step=239\n",
      "Epoch 240:  Train_loss=0.681  Val_loss=0.679  Val_Acc=0.619  eta=0.00010 global_step=240\n",
      "Epoch 241:  Train_loss=0.681  Val_loss=0.679  Val_Acc=0.619  eta=0.00010 global_step=241\n",
      "Epoch 242:  Train_loss=0.680  Val_loss=0.679  Val_Acc=0.619  eta=0.00010 global_step=242\n",
      "Epoch 243:  Train_loss=0.680  Val_loss=0.679  Val_Acc=0.619  eta=0.00010 global_step=243\n",
      "Epoch 244:  Train_loss=0.680  Val_loss=0.679  Val_Acc=0.619  eta=0.00010 global_step=244\n",
      "Epoch 245:  Train_loss=0.680  Val_loss=0.679  Val_Acc=0.619  eta=0.00010 global_step=245\n",
      "Epoch 246:  Train_loss=0.680  Val_loss=0.679  Val_Acc=0.619  eta=0.00010 global_step=246\n",
      "Epoch 247:  Train_loss=0.680  Val_loss=0.679  Val_Acc=0.619  eta=0.00010 global_step=247\n",
      "Epoch 248:  Train_loss=0.680  Val_loss=0.679  Val_Acc=0.619  eta=0.00010 global_step=248\n",
      "Epoch 249:  Train_loss=0.680  Val_loss=0.678  Val_Acc=0.619  eta=0.00010 global_step=249\n",
      "Epoch 250:  Train_loss=0.680  Val_loss=0.678  Val_Acc=0.619  eta=0.00010 global_step=250\n",
      "Epoch 251:  Train_loss=0.680  Val_loss=0.678  Val_Acc=0.619  eta=0.00010 global_step=251\n",
      "Epoch 252:  Train_loss=0.680  Val_loss=0.678  Val_Acc=0.619  eta=0.00010 global_step=252\n",
      "Epoch 253:  Train_loss=0.679  Val_loss=0.678  Val_Acc=0.619  eta=0.00010 global_step=253\n",
      "Epoch 254:  Train_loss=0.679  Val_loss=0.678  Val_Acc=0.619  eta=0.00010 global_step=254\n",
      "Epoch 255:  Train_loss=0.679  Val_loss=0.678  Val_Acc=0.619  eta=0.00010 global_step=255\n",
      "Epoch 256:  Train_loss=0.679  Val_loss=0.678  Val_Acc=0.619  eta=0.00010 global_step=256\n",
      "Epoch 257:  Train_loss=0.679  Val_loss=0.678  Val_Acc=0.619  eta=0.00010 global_step=257\n",
      "Epoch 258:  Train_loss=0.679  Val_loss=0.677  Val_Acc=0.619  eta=0.00010 global_step=258\n",
      "Epoch 259:  Train_loss=0.679  Val_loss=0.677  Val_Acc=0.619  eta=0.00010 global_step=259\n",
      "Epoch 260:  Train_loss=0.679  Val_loss=0.677  Val_Acc=0.619  eta=0.00010 global_step=260\n",
      "Epoch 261:  Train_loss=0.679  Val_loss=0.677  Val_Acc=0.619  eta=0.00010 global_step=261\n",
      "Epoch 262:  Train_loss=0.679  Val_loss=0.677  Val_Acc=0.619  eta=0.00010 global_step=262\n",
      "Epoch 263:  Train_loss=0.678  Val_loss=0.677  Val_Acc=0.619  eta=0.00010 global_step=263\n",
      "Epoch 264:  Train_loss=0.678  Val_loss=0.677  Val_Acc=0.619  eta=0.00010 global_step=264\n",
      "Epoch 265:  Train_loss=0.678  Val_loss=0.677  Val_Acc=0.619  eta=0.00010 global_step=265\n",
      "Epoch 266:  Train_loss=0.678  Val_loss=0.677  Val_Acc=0.619  eta=0.00010 global_step=266\n",
      "Epoch 267:  Train_loss=0.678  Val_loss=0.677  Val_Acc=0.619  eta=0.00010 global_step=267\n",
      "Epoch 268:  Train_loss=0.678  Val_loss=0.676  Val_Acc=0.619  eta=0.00010 global_step=268\n",
      "Epoch 269:  Train_loss=0.678  Val_loss=0.676  Val_Acc=0.619  eta=0.00010 global_step=269\n",
      "Epoch 270:  Train_loss=0.678  Val_loss=0.676  Val_Acc=0.619  eta=0.00010 global_step=270\n",
      "Epoch 271:  Train_loss=0.678  Val_loss=0.676  Val_Acc=0.619  eta=0.00010 global_step=271\n",
      "Epoch 272:  Train_loss=0.678  Val_loss=0.676  Val_Acc=0.619  eta=0.00010 global_step=272\n",
      "Epoch 273:  Train_loss=0.677  Val_loss=0.676  Val_Acc=0.619  eta=0.00010 global_step=273\n",
      "Epoch 274:  Train_loss=0.677  Val_loss=0.676  Val_Acc=0.619  eta=0.00010 global_step=274\n",
      "Epoch 275:  Train_loss=0.677  Val_loss=0.676  Val_Acc=0.619  eta=0.00010 global_step=275\n",
      "Epoch 276:  Train_loss=0.677  Val_loss=0.675  Val_Acc=0.619  eta=0.00010 global_step=276\n",
      "Epoch 277:  Train_loss=0.677  Val_loss=0.675  Val_Acc=0.619  eta=0.00010 global_step=277\n",
      "Epoch 278:  Train_loss=0.677  Val_loss=0.675  Val_Acc=0.619  eta=0.00010 global_step=278\n",
      "Epoch 279:  Train_loss=0.677  Val_loss=0.675  Val_Acc=0.619  eta=0.00010 global_step=279\n",
      "Epoch 280:  Train_loss=0.677  Val_loss=0.675  Val_Acc=0.619  eta=0.00010 global_step=280\n",
      "Epoch 281:  Train_loss=0.677  Val_loss=0.675  Val_Acc=0.619  eta=0.00010 global_step=281\n",
      "Epoch 282:  Train_loss=0.676  Val_loss=0.675  Val_Acc=0.619  eta=0.00010 global_step=282\n",
      "Epoch 283:  Train_loss=0.676  Val_loss=0.675  Val_Acc=0.619  eta=0.00010 global_step=283\n",
      "Epoch 284:  Train_loss=0.676  Val_loss=0.674  Val_Acc=0.619  eta=0.00010 global_step=284\n",
      "Epoch 285:  Train_loss=0.676  Val_loss=0.674  Val_Acc=0.619  eta=0.00010 global_step=285\n",
      "Epoch 286:  Train_loss=0.676  Val_loss=0.674  Val_Acc=0.619  eta=0.00010 global_step=286\n",
      "Epoch 287:  Train_loss=0.676  Val_loss=0.674  Val_Acc=0.619  eta=0.00010 global_step=287\n",
      "Epoch 288:  Train_loss=0.676  Val_loss=0.674  Val_Acc=0.619  eta=0.00010 global_step=288\n",
      "Epoch 289:  Train_loss=0.676  Val_loss=0.674  Val_Acc=0.619  eta=0.00010 global_step=289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290:  Train_loss=0.676  Val_loss=0.674  Val_Acc=0.619  eta=0.00010 global_step=290\n",
      "Epoch 291:  Train_loss=0.675  Val_loss=0.674  Val_Acc=0.619  eta=0.00010 global_step=291\n",
      "Epoch 292:  Train_loss=0.675  Val_loss=0.673  Val_Acc=0.619  eta=0.00010 global_step=292\n",
      "Epoch 293:  Train_loss=0.675  Val_loss=0.673  Val_Acc=0.619  eta=0.00010 global_step=293\n",
      "Epoch 294:  Train_loss=0.675  Val_loss=0.673  Val_Acc=0.619  eta=0.00010 global_step=294\n",
      "Epoch 295:  Train_loss=0.675  Val_loss=0.673  Val_Acc=0.619  eta=0.00010 global_step=295\n",
      "Epoch 296:  Train_loss=0.675  Val_loss=0.673  Val_Acc=0.619  eta=0.00010 global_step=296\n",
      "Epoch 297:  Train_loss=0.675  Val_loss=0.673  Val_Acc=0.619  eta=0.00010 global_step=297\n",
      "Epoch 298:  Train_loss=0.675  Val_loss=0.673  Val_Acc=0.619  eta=0.00010 global_step=298\n",
      "Epoch 299:  Train_loss=0.675  Val_loss=0.673  Val_Acc=0.619  eta=0.00010 global_step=299\n",
      "Epoch 300:  Train_loss=0.674  Val_loss=0.672  Val_Acc=0.619  eta=0.00010 global_step=300\n",
      "Epoch 301:  Train_loss=0.674  Val_loss=0.672  Val_Acc=0.619  eta=0.00010 global_step=301\n",
      "Epoch 302:  Train_loss=0.674  Val_loss=0.672  Val_Acc=0.619  eta=0.00010 global_step=302\n",
      "Epoch 303:  Train_loss=0.674  Val_loss=0.672  Val_Acc=0.619  eta=0.00010 global_step=303\n",
      "Epoch 304:  Train_loss=0.674  Val_loss=0.672  Val_Acc=0.619  eta=0.00010 global_step=304\n",
      "Epoch 305:  Train_loss=0.674  Val_loss=0.672  Val_Acc=0.619  eta=0.00010 global_step=305\n",
      "Epoch 306:  Train_loss=0.674  Val_loss=0.672  Val_Acc=0.619  eta=0.00010 global_step=306\n",
      "Epoch 307:  Train_loss=0.674  Val_loss=0.671  Val_Acc=0.619  eta=0.00010 global_step=307\n",
      "Epoch 308:  Train_loss=0.673  Val_loss=0.671  Val_Acc=0.619  eta=0.00010 global_step=308\n",
      "Epoch 309:  Train_loss=0.673  Val_loss=0.671  Val_Acc=0.619  eta=0.00010 global_step=309\n",
      "Epoch 310:  Train_loss=0.673  Val_loss=0.671  Val_Acc=0.619  eta=0.00010 global_step=310\n",
      "Epoch 311:  Train_loss=0.673  Val_loss=0.671  Val_Acc=0.619  eta=0.00010 global_step=311\n",
      "Epoch 312:  Train_loss=0.673  Val_loss=0.671  Val_Acc=0.619  eta=0.00010 global_step=312\n",
      "Epoch 313:  Train_loss=0.673  Val_loss=0.671  Val_Acc=0.619  eta=0.00010 global_step=313\n",
      "Epoch 314:  Train_loss=0.673  Val_loss=0.670  Val_Acc=0.619  eta=0.00010 global_step=314\n",
      "Epoch 315:  Train_loss=0.672  Val_loss=0.670  Val_Acc=0.619  eta=0.00010 global_step=315\n",
      "Epoch 316:  Train_loss=0.672  Val_loss=0.670  Val_Acc=0.619  eta=0.00010 global_step=316\n",
      "Epoch 317:  Train_loss=0.672  Val_loss=0.670  Val_Acc=0.619  eta=0.00010 global_step=317\n",
      "Epoch 318:  Train_loss=0.672  Val_loss=0.670  Val_Acc=0.619  eta=0.00010 global_step=318\n",
      "Epoch 319:  Train_loss=0.672  Val_loss=0.670  Val_Acc=0.619  eta=0.00010 global_step=319\n",
      "Epoch 320:  Train_loss=0.672  Val_loss=0.670  Val_Acc=0.619  eta=0.00010 global_step=320\n",
      "Epoch 321:  Train_loss=0.672  Val_loss=0.669  Val_Acc=0.619  eta=0.00010 global_step=321\n",
      "Epoch 322:  Train_loss=0.672  Val_loss=0.669  Val_Acc=0.619  eta=0.00010 global_step=322\n",
      "Epoch 323:  Train_loss=0.671  Val_loss=0.669  Val_Acc=0.619  eta=0.00010 global_step=323\n",
      "Epoch 324:  Train_loss=0.671  Val_loss=0.669  Val_Acc=0.619  eta=0.00010 global_step=324\n",
      "Epoch 325:  Train_loss=0.671  Val_loss=0.669  Val_Acc=0.619  eta=0.00010 global_step=325\n",
      "Epoch 326:  Train_loss=0.671  Val_loss=0.669  Val_Acc=0.619  eta=0.00010 global_step=326\n",
      "Epoch 327:  Train_loss=0.671  Val_loss=0.668  Val_Acc=0.619  eta=0.00010 global_step=327\n",
      "Epoch 328:  Train_loss=0.671  Val_loss=0.668  Val_Acc=0.619  eta=0.00010 global_step=328\n",
      "Epoch 329:  Train_loss=0.671  Val_loss=0.668  Val_Acc=0.619  eta=0.00010 global_step=329\n",
      "Epoch 330:  Train_loss=0.670  Val_loss=0.668  Val_Acc=0.619  eta=0.00010 global_step=330\n",
      "Epoch 331:  Train_loss=0.670  Val_loss=0.668  Val_Acc=0.619  eta=0.00010 global_step=331\n",
      "Epoch 332:  Train_loss=0.670  Val_loss=0.668  Val_Acc=0.619  eta=0.00010 global_step=332\n",
      "Epoch 333:  Train_loss=0.670  Val_loss=0.667  Val_Acc=0.619  eta=0.00010 global_step=333\n",
      "Epoch 334:  Train_loss=0.670  Val_loss=0.667  Val_Acc=0.619  eta=0.00010 global_step=334\n",
      "Epoch 335:  Train_loss=0.670  Val_loss=0.667  Val_Acc=0.619  eta=0.00010 global_step=335\n",
      "Epoch 336:  Train_loss=0.669  Val_loss=0.667  Val_Acc=0.619  eta=0.00010 global_step=336\n",
      "Epoch 337:  Train_loss=0.669  Val_loss=0.667  Val_Acc=0.619  eta=0.00010 global_step=337\n",
      "Epoch 338:  Train_loss=0.669  Val_loss=0.667  Val_Acc=0.619  eta=0.00010 global_step=338\n",
      "Epoch 339:  Train_loss=0.669  Val_loss=0.666  Val_Acc=0.619  eta=0.00010 global_step=339\n",
      "Epoch 340:  Train_loss=0.669  Val_loss=0.666  Val_Acc=0.619  eta=0.00010 global_step=340\n",
      "Epoch 341:  Train_loss=0.669  Val_loss=0.666  Val_Acc=0.619  eta=0.00010 global_step=341\n",
      "Epoch 342:  Train_loss=0.668  Val_loss=0.666  Val_Acc=0.619  eta=0.00010 global_step=342\n",
      "Epoch 343:  Train_loss=0.668  Val_loss=0.666  Val_Acc=0.619  eta=0.00010 global_step=343\n",
      "Epoch 344:  Train_loss=0.668  Val_loss=0.665  Val_Acc=0.619  eta=0.00010 global_step=344\n",
      "Epoch 345:  Train_loss=0.668  Val_loss=0.665  Val_Acc=0.619  eta=0.00010 global_step=345\n",
      "Epoch 346:  Train_loss=0.668  Val_loss=0.665  Val_Acc=0.619  eta=0.00010 global_step=346\n",
      "Epoch 347:  Train_loss=0.668  Val_loss=0.665  Val_Acc=0.619  eta=0.00010 global_step=347\n",
      "Epoch 348:  Train_loss=0.667  Val_loss=0.665  Val_Acc=0.619  eta=0.00010 global_step=348\n",
      "Epoch 349:  Train_loss=0.667  Val_loss=0.665  Val_Acc=0.619  eta=0.00010 global_step=349\n",
      "Epoch 350:  Train_loss=0.667  Val_loss=0.664  Val_Acc=0.619  eta=0.00010 global_step=350\n",
      "Epoch 351:  Train_loss=0.667  Val_loss=0.664  Val_Acc=0.619  eta=0.00010 global_step=351\n",
      "Epoch 352:  Train_loss=0.667  Val_loss=0.664  Val_Acc=0.619  eta=0.00010 global_step=352\n",
      "Epoch 353:  Train_loss=0.667  Val_loss=0.664  Val_Acc=0.619  eta=0.00010 global_step=353\n",
      "Epoch 354:  Train_loss=0.666  Val_loss=0.664  Val_Acc=0.619  eta=0.00010 global_step=354\n",
      "Epoch 355:  Train_loss=0.666  Val_loss=0.663  Val_Acc=0.619  eta=0.00010 global_step=355\n",
      "Epoch 356:  Train_loss=0.666  Val_loss=0.663  Val_Acc=0.619  eta=0.00010 global_step=356\n",
      "Epoch 357:  Train_loss=0.666  Val_loss=0.663  Val_Acc=0.619  eta=0.00010 global_step=357\n",
      "Epoch 358:  Train_loss=0.666  Val_loss=0.663  Val_Acc=0.619  eta=0.00010 global_step=358\n",
      "Epoch 359:  Train_loss=0.666  Val_loss=0.663  Val_Acc=0.619  eta=0.00010 global_step=359\n",
      "Epoch 360:  Train_loss=0.665  Val_loss=0.662  Val_Acc=0.619  eta=0.00010 global_step=360\n",
      "Epoch 361:  Train_loss=0.665  Val_loss=0.662  Val_Acc=0.619  eta=0.00010 global_step=361\n",
      "Epoch 362:  Train_loss=0.665  Val_loss=0.662  Val_Acc=0.619  eta=0.00010 global_step=362\n",
      "Epoch 363:  Train_loss=0.665  Val_loss=0.662  Val_Acc=0.619  eta=0.00010 global_step=363\n",
      "Epoch 364:  Train_loss=0.665  Val_loss=0.662  Val_Acc=0.619  eta=0.00010 global_step=364\n",
      "Epoch 365:  Train_loss=0.664  Val_loss=0.661  Val_Acc=0.619  eta=0.00010 global_step=365\n",
      "Epoch 366:  Train_loss=0.664  Val_loss=0.661  Val_Acc=0.619  eta=0.00010 global_step=366\n",
      "Epoch 367:  Train_loss=0.664  Val_loss=0.661  Val_Acc=0.619  eta=0.00010 global_step=367\n",
      "Epoch 368:  Train_loss=0.664  Val_loss=0.661  Val_Acc=0.619  eta=0.00010 global_step=368\n",
      "Epoch 369:  Train_loss=0.664  Val_loss=0.661  Val_Acc=0.619  eta=0.00010 global_step=369\n",
      "Epoch 370:  Train_loss=0.664  Val_loss=0.660  Val_Acc=0.619  eta=0.00010 global_step=370\n",
      "Epoch 371:  Train_loss=0.663  Val_loss=0.660  Val_Acc=0.619  eta=0.00010 global_step=371\n",
      "Epoch 372:  Train_loss=0.663  Val_loss=0.660  Val_Acc=0.619  eta=0.00010 global_step=372\n",
      "Epoch 373:  Train_loss=0.663  Val_loss=0.660  Val_Acc=0.619  eta=0.00010 global_step=373\n",
      "Epoch 374:  Train_loss=0.663  Val_loss=0.660  Val_Acc=0.619  eta=0.00010 global_step=374\n",
      "Epoch 375:  Train_loss=0.663  Val_loss=0.659  Val_Acc=0.619  eta=0.00010 global_step=375\n",
      "Epoch 376:  Train_loss=0.662  Val_loss=0.659  Val_Acc=0.619  eta=0.00010 global_step=376\n",
      "Epoch 377:  Train_loss=0.662  Val_loss=0.659  Val_Acc=0.619  eta=0.00010 global_step=377\n",
      "Epoch 378:  Train_loss=0.662  Val_loss=0.659  Val_Acc=0.619  eta=0.00010 global_step=378\n",
      "Epoch 379:  Train_loss=0.662  Val_loss=0.659  Val_Acc=0.619  eta=0.00010 global_step=379\n",
      "Epoch 380:  Train_loss=0.662  Val_loss=0.658  Val_Acc=0.619  eta=0.00010 global_step=380\n",
      "Epoch 381:  Train_loss=0.662  Val_loss=0.658  Val_Acc=0.619  eta=0.00010 global_step=381\n",
      "Epoch 382:  Train_loss=0.661  Val_loss=0.658  Val_Acc=0.619  eta=0.00010 global_step=382\n",
      "Epoch 383:  Train_loss=0.661  Val_loss=0.658  Val_Acc=0.619  eta=0.00010 global_step=383\n",
      "Epoch 384:  Train_loss=0.661  Val_loss=0.658  Val_Acc=0.619  eta=0.00010 global_step=384\n",
      "Epoch 385:  Train_loss=0.661  Val_loss=0.657  Val_Acc=0.619  eta=0.00010 global_step=385\n",
      "Epoch 386:  Train_loss=0.661  Val_loss=0.657  Val_Acc=0.619  eta=0.00010 global_step=386\n",
      "Epoch 387:  Train_loss=0.660  Val_loss=0.657  Val_Acc=0.619  eta=0.00010 global_step=387\n",
      "Epoch 388:  Train_loss=0.660  Val_loss=0.657  Val_Acc=0.619  eta=0.00010 global_step=388\n",
      "Epoch 389:  Train_loss=0.660  Val_loss=0.657  Val_Acc=0.619  eta=0.00010 global_step=389\n",
      "Epoch 390:  Train_loss=0.660  Val_loss=0.656  Val_Acc=0.619  eta=0.00010 global_step=390\n",
      "Epoch 391:  Train_loss=0.660  Val_loss=0.656  Val_Acc=0.619  eta=0.00010 global_step=391\n",
      "Epoch 392:  Train_loss=0.659  Val_loss=0.656  Val_Acc=0.619  eta=0.00010 global_step=392\n",
      "Epoch 393:  Train_loss=0.659  Val_loss=0.656  Val_Acc=0.619  eta=0.00010 global_step=393\n",
      "Epoch 394:  Train_loss=0.659  Val_loss=0.655  Val_Acc=0.619  eta=0.00010 global_step=394\n",
      "Epoch 395:  Train_loss=0.659  Val_loss=0.655  Val_Acc=0.619  eta=0.00010 global_step=395\n",
      "Epoch 396:  Train_loss=0.659  Val_loss=0.655  Val_Acc=0.619  eta=0.00010 global_step=396\n",
      "Epoch 397:  Train_loss=0.658  Val_loss=0.655  Val_Acc=0.619  eta=0.00010 global_step=397\n",
      "Epoch 398:  Train_loss=0.658  Val_loss=0.655  Val_Acc=0.619  eta=0.00010 global_step=398\n",
      "Epoch 399:  Train_loss=0.658  Val_loss=0.654  Val_Acc=0.619  eta=0.00010 global_step=399\n",
      "Epoch 400:  Train_loss=0.658  Val_loss=0.654  Val_Acc=0.619  eta=0.00010 global_step=400\n",
      "Epoch 401:  Train_loss=0.658  Val_loss=0.654  Val_Acc=0.619  eta=0.00010 global_step=401\n",
      "Epoch 402:  Train_loss=0.657  Val_loss=0.654  Val_Acc=0.619  eta=0.00010 global_step=402\n",
      "Epoch 403:  Train_loss=0.657  Val_loss=0.654  Val_Acc=0.619  eta=0.00010 global_step=403\n",
      "Epoch 404:  Train_loss=0.657  Val_loss=0.653  Val_Acc=0.619  eta=0.00010 global_step=404\n",
      "Epoch 405:  Train_loss=0.657  Val_loss=0.653  Val_Acc=0.619  eta=0.00010 global_step=405\n",
      "Epoch 406:  Train_loss=0.657  Val_loss=0.653  Val_Acc=0.619  eta=0.00010 global_step=406\n",
      "Epoch 407:  Train_loss=0.656  Val_loss=0.653  Val_Acc=0.619  eta=0.00010 global_step=407\n",
      "Epoch 408:  Train_loss=0.656  Val_loss=0.652  Val_Acc=0.619  eta=0.00010 global_step=408\n",
      "Epoch 409:  Train_loss=0.656  Val_loss=0.652  Val_Acc=0.619  eta=0.00010 global_step=409\n",
      "Epoch 410:  Train_loss=0.656  Val_loss=0.652  Val_Acc=0.619  eta=0.00010 global_step=410\n",
      "Epoch 411:  Train_loss=0.656  Val_loss=0.652  Val_Acc=0.619  eta=0.00010 global_step=411\n",
      "Epoch 412:  Train_loss=0.655  Val_loss=0.652  Val_Acc=0.619  eta=0.00010 global_step=412\n",
      "Epoch 413:  Train_loss=0.655  Val_loss=0.651  Val_Acc=0.619  eta=0.00010 global_step=413\n",
      "Epoch 414:  Train_loss=0.655  Val_loss=0.651  Val_Acc=0.619  eta=0.00010 global_step=414\n",
      "Epoch 415:  Train_loss=0.655  Val_loss=0.651  Val_Acc=0.619  eta=0.00010 global_step=415\n",
      "Epoch 416:  Train_loss=0.655  Val_loss=0.651  Val_Acc=0.619  eta=0.00010 global_step=416\n",
      "Epoch 417:  Train_loss=0.654  Val_loss=0.650  Val_Acc=0.619  eta=0.00010 global_step=417\n",
      "Epoch 418:  Train_loss=0.654  Val_loss=0.650  Val_Acc=0.619  eta=0.00010 global_step=418\n",
      "Epoch 419:  Train_loss=0.654  Val_loss=0.650  Val_Acc=0.619  eta=0.00010 global_step=419\n",
      "Epoch 420:  Train_loss=0.654  Val_loss=0.650  Val_Acc=0.619  eta=0.00010 global_step=420\n",
      "Epoch 421:  Train_loss=0.653  Val_loss=0.649  Val_Acc=0.619  eta=0.00010 global_step=421\n",
      "Epoch 422:  Train_loss=0.653  Val_loss=0.649  Val_Acc=0.619  eta=0.00010 global_step=422\n",
      "Epoch 423:  Train_loss=0.653  Val_loss=0.649  Val_Acc=0.619  eta=0.00010 global_step=423\n",
      "Epoch 424:  Train_loss=0.653  Val_loss=0.649  Val_Acc=0.619  eta=0.00010 global_step=424\n",
      "Epoch 425:  Train_loss=0.653  Val_loss=0.648  Val_Acc=0.619  eta=0.00010 global_step=425\n",
      "Epoch 426:  Train_loss=0.652  Val_loss=0.648  Val_Acc=0.619  eta=0.00010 global_step=426\n",
      "Epoch 427:  Train_loss=0.652  Val_loss=0.648  Val_Acc=0.619  eta=0.00010 global_step=427\n",
      "Epoch 428:  Train_loss=0.652  Val_loss=0.648  Val_Acc=0.619  eta=0.00010 global_step=428\n",
      "Epoch 429:  Train_loss=0.652  Val_loss=0.647  Val_Acc=0.619  eta=0.00010 global_step=429\n",
      "Epoch 430:  Train_loss=0.651  Val_loss=0.647  Val_Acc=0.619  eta=0.00010 global_step=430\n",
      "Epoch 431:  Train_loss=0.651  Val_loss=0.647  Val_Acc=0.619  eta=0.00010 global_step=431\n",
      "Epoch 432:  Train_loss=0.651  Val_loss=0.647  Val_Acc=0.619  eta=0.00010 global_step=432\n",
      "Epoch 433:  Train_loss=0.651  Val_loss=0.646  Val_Acc=0.619  eta=0.00010 global_step=433\n",
      "Epoch 434:  Train_loss=0.651  Val_loss=0.646  Val_Acc=0.619  eta=0.00010 global_step=434\n",
      "Epoch 435:  Train_loss=0.650  Val_loss=0.646  Val_Acc=0.619  eta=0.00010 global_step=435\n",
      "Epoch 436:  Train_loss=0.650  Val_loss=0.646  Val_Acc=0.619  eta=0.00010 global_step=436\n",
      "Epoch 437:  Train_loss=0.650  Val_loss=0.645  Val_Acc=0.619  eta=0.00010 global_step=437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438:  Train_loss=0.650  Val_loss=0.645  Val_Acc=0.619  eta=0.00010 global_step=438\n",
      "Epoch 439:  Train_loss=0.649  Val_loss=0.645  Val_Acc=0.619  eta=0.00010 global_step=439\n",
      "Epoch 440:  Train_loss=0.649  Val_loss=0.645  Val_Acc=0.619  eta=0.00010 global_step=440\n",
      "Epoch 441:  Train_loss=0.649  Val_loss=0.644  Val_Acc=0.619  eta=0.00010 global_step=441\n",
      "Epoch 442:  Train_loss=0.649  Val_loss=0.644  Val_Acc=0.619  eta=0.00010 global_step=442\n",
      "Epoch 443:  Train_loss=0.648  Val_loss=0.644  Val_Acc=0.619  eta=0.00010 global_step=443\n",
      "Epoch 444:  Train_loss=0.648  Val_loss=0.644  Val_Acc=0.619  eta=0.00010 global_step=444\n",
      "Epoch 445:  Train_loss=0.648  Val_loss=0.643  Val_Acc=0.619  eta=0.00010 global_step=445\n",
      "Epoch 446:  Train_loss=0.648  Val_loss=0.643  Val_Acc=0.619  eta=0.00010 global_step=446\n",
      "Epoch 447:  Train_loss=0.648  Val_loss=0.643  Val_Acc=0.619  eta=0.00010 global_step=447\n",
      "Epoch 448:  Train_loss=0.647  Val_loss=0.643  Val_Acc=0.619  eta=0.00010 global_step=448\n",
      "Epoch 449:  Train_loss=0.647  Val_loss=0.642  Val_Acc=0.619  eta=0.00010 global_step=449\n",
      "Epoch 450:  Train_loss=0.647  Val_loss=0.642  Val_Acc=0.619  eta=0.00010 global_step=450\n",
      "Epoch 451:  Train_loss=0.647  Val_loss=0.642  Val_Acc=0.619  eta=0.00010 global_step=451\n",
      "Epoch 452:  Train_loss=0.646  Val_loss=0.642  Val_Acc=0.619  eta=0.00010 global_step=452\n",
      "Epoch 453:  Train_loss=0.646  Val_loss=0.641  Val_Acc=0.619  eta=0.00010 global_step=453\n",
      "Epoch 454:  Train_loss=0.646  Val_loss=0.641  Val_Acc=0.619  eta=0.00010 global_step=454\n",
      "Epoch 455:  Train_loss=0.646  Val_loss=0.641  Val_Acc=0.619  eta=0.00010 global_step=455\n",
      "Epoch 456:  Train_loss=0.645  Val_loss=0.641  Val_Acc=0.619  eta=0.00010 global_step=456\n",
      "Epoch 457:  Train_loss=0.645  Val_loss=0.640  Val_Acc=0.619  eta=0.00010 global_step=457\n",
      "Epoch 458:  Train_loss=0.645  Val_loss=0.640  Val_Acc=0.619  eta=0.00010 global_step=458\n",
      "Epoch 459:  Train_loss=0.645  Val_loss=0.640  Val_Acc=0.619  eta=0.00010 global_step=459\n",
      "Epoch 460:  Train_loss=0.644  Val_loss=0.640  Val_Acc=0.619  eta=0.00010 global_step=460\n",
      "Epoch 461:  Train_loss=0.644  Val_loss=0.639  Val_Acc=0.619  eta=0.00010 global_step=461\n",
      "Epoch 462:  Train_loss=0.644  Val_loss=0.639  Val_Acc=0.619  eta=0.00010 global_step=462\n",
      "Epoch 463:  Train_loss=0.644  Val_loss=0.639  Val_Acc=0.619  eta=0.00010 global_step=463\n",
      "Epoch 464:  Train_loss=0.643  Val_loss=0.638  Val_Acc=0.619  eta=0.00010 global_step=464\n",
      "Epoch 465:  Train_loss=0.643  Val_loss=0.638  Val_Acc=0.619  eta=0.00010 global_step=465\n",
      "Epoch 466:  Train_loss=0.643  Val_loss=0.638  Val_Acc=0.619  eta=0.00010 global_step=466\n",
      "Epoch 467:  Train_loss=0.643  Val_loss=0.638  Val_Acc=0.619  eta=0.00010 global_step=467\n",
      "Epoch 468:  Train_loss=0.642  Val_loss=0.637  Val_Acc=0.619  eta=0.00010 global_step=468\n",
      "Epoch 469:  Train_loss=0.642  Val_loss=0.637  Val_Acc=0.619  eta=0.00010 global_step=469\n",
      "Epoch 470:  Train_loss=0.642  Val_loss=0.637  Val_Acc=0.619  eta=0.00010 global_step=470\n",
      "Epoch 471:  Train_loss=0.642  Val_loss=0.637  Val_Acc=0.619  eta=0.00010 global_step=471\n",
      "Epoch 472:  Train_loss=0.641  Val_loss=0.636  Val_Acc=0.619  eta=0.00010 global_step=472\n",
      "Epoch 473:  Train_loss=0.641  Val_loss=0.636  Val_Acc=0.619  eta=0.00010 global_step=473\n",
      "Epoch 474:  Train_loss=0.641  Val_loss=0.636  Val_Acc=0.619  eta=0.00010 global_step=474\n",
      "Epoch 475:  Train_loss=0.641  Val_loss=0.636  Val_Acc=0.619  eta=0.00010 global_step=475\n",
      "Epoch 476:  Train_loss=0.640  Val_loss=0.635  Val_Acc=0.619  eta=0.00010 global_step=476\n",
      "Epoch 477:  Train_loss=0.640  Val_loss=0.635  Val_Acc=0.619  eta=0.00010 global_step=477\n",
      "Epoch 478:  Train_loss=0.640  Val_loss=0.635  Val_Acc=0.619  eta=0.00010 global_step=478\n",
      "Epoch 479:  Train_loss=0.640  Val_loss=0.634  Val_Acc=0.619  eta=0.00010 global_step=479\n",
      "Epoch 480:  Train_loss=0.639  Val_loss=0.634  Val_Acc=0.619  eta=0.00010 global_step=480\n",
      "Epoch 481:  Train_loss=0.639  Val_loss=0.634  Val_Acc=0.619  eta=0.00010 global_step=481\n",
      "Epoch 482:  Train_loss=0.639  Val_loss=0.634  Val_Acc=0.619  eta=0.00010 global_step=482\n",
      "Epoch 483:  Train_loss=0.639  Val_loss=0.633  Val_Acc=0.619  eta=0.00010 global_step=483\n",
      "Epoch 484:  Train_loss=0.638  Val_loss=0.633  Val_Acc=0.619  eta=0.00010 global_step=484\n",
      "Epoch 485:  Train_loss=0.638  Val_loss=0.633  Val_Acc=0.619  eta=0.00010 global_step=485\n",
      "Epoch 486:  Train_loss=0.638  Val_loss=0.633  Val_Acc=0.619  eta=0.00010 global_step=486\n",
      "Epoch 487:  Train_loss=0.638  Val_loss=0.632  Val_Acc=0.619  eta=0.00010 global_step=487\n",
      "Epoch 488:  Train_loss=0.637  Val_loss=0.632  Val_Acc=0.619  eta=0.00010 global_step=488\n",
      "Epoch 489:  Train_loss=0.637  Val_loss=0.632  Val_Acc=0.619  eta=0.00010 global_step=489\n",
      "Epoch 490:  Train_loss=0.637  Val_loss=0.631  Val_Acc=0.619  eta=0.00010 global_step=490\n",
      "Epoch 491:  Train_loss=0.637  Val_loss=0.631  Val_Acc=0.619  eta=0.00010 global_step=491\n",
      "Epoch 492:  Train_loss=0.636  Val_loss=0.631  Val_Acc=0.619  eta=0.00010 global_step=492\n",
      "Epoch 493:  Train_loss=0.636  Val_loss=0.631  Val_Acc=0.619  eta=0.00010 global_step=493\n",
      "Epoch 494:  Train_loss=0.636  Val_loss=0.630  Val_Acc=0.619  eta=0.00010 global_step=494\n",
      "Epoch 495:  Train_loss=0.636  Val_loss=0.630  Val_Acc=0.619  eta=0.00010 global_step=495\n",
      "Epoch 496:  Train_loss=0.635  Val_loss=0.630  Val_Acc=0.619  eta=0.00010 global_step=496\n",
      "Epoch 497:  Train_loss=0.635  Val_loss=0.630  Val_Acc=0.619  eta=0.00010 global_step=497\n",
      "Epoch 498:  Train_loss=0.635  Val_loss=0.629  Val_Acc=0.619  eta=0.00010 global_step=498\n",
      "Epoch 499:  Train_loss=0.635  Val_loss=0.629  Val_Acc=0.619  eta=0.00010 global_step=499\n",
      "Epoch 500:  Train_loss=0.634  Val_loss=0.629  Val_Acc=0.619  eta=0.00010 global_step=500\n",
      "Epoch 501:  Train_loss=0.634  Val_loss=0.628  Val_Acc=0.619  eta=0.00010 global_step=501\n",
      "Epoch 502:  Train_loss=0.634  Val_loss=0.628  Val_Acc=0.619  eta=0.00010 global_step=502\n",
      "Epoch 503:  Train_loss=0.634  Val_loss=0.628  Val_Acc=0.619  eta=0.00010 global_step=503\n",
      "Epoch 504:  Train_loss=0.633  Val_loss=0.628  Val_Acc=0.619  eta=0.00010 global_step=504\n",
      "Epoch 505:  Train_loss=0.633  Val_loss=0.627  Val_Acc=0.619  eta=0.00010 global_step=505\n",
      "Epoch 506:  Train_loss=0.633  Val_loss=0.627  Val_Acc=0.619  eta=0.00010 global_step=506\n",
      "Epoch 507:  Train_loss=0.633  Val_loss=0.627  Val_Acc=0.619  eta=0.00010 global_step=507\n",
      "Epoch 508:  Train_loss=0.632  Val_loss=0.626  Val_Acc=0.619  eta=0.00010 global_step=508\n",
      "Epoch 509:  Train_loss=0.632  Val_loss=0.626  Val_Acc=0.619  eta=0.00010 global_step=509\n",
      "Epoch 510:  Train_loss=0.632  Val_loss=0.626  Val_Acc=0.619  eta=0.00010 global_step=510\n",
      "Epoch 511:  Train_loss=0.632  Val_loss=0.626  Val_Acc=0.619  eta=0.00010 global_step=511\n",
      "Epoch 512:  Train_loss=0.631  Val_loss=0.625  Val_Acc=0.619  eta=0.00010 global_step=512\n",
      "Epoch 513:  Train_loss=0.631  Val_loss=0.625  Val_Acc=0.619  eta=0.00010 global_step=513\n",
      "Epoch 514:  Train_loss=0.631  Val_loss=0.625  Val_Acc=0.619  eta=0.00010 global_step=514\n",
      "Epoch 515:  Train_loss=0.631  Val_loss=0.625  Val_Acc=0.619  eta=0.00010 global_step=515\n",
      "Epoch 516:  Train_loss=0.630  Val_loss=0.624  Val_Acc=0.619  eta=0.00010 global_step=516\n",
      "Epoch 517:  Train_loss=0.630  Val_loss=0.624  Val_Acc=0.619  eta=0.00010 global_step=517\n",
      "Epoch 518:  Train_loss=0.630  Val_loss=0.624  Val_Acc=0.619  eta=0.00010 global_step=518\n",
      "Epoch 519:  Train_loss=0.630  Val_loss=0.623  Val_Acc=0.619  eta=0.00010 global_step=519\n",
      "Epoch 520:  Train_loss=0.629  Val_loss=0.623  Val_Acc=0.619  eta=0.00010 global_step=520\n",
      "Epoch 521:  Train_loss=0.629  Val_loss=0.623  Val_Acc=0.619  eta=0.00010 global_step=521\n",
      "Epoch 522:  Train_loss=0.629  Val_loss=0.623  Val_Acc=0.619  eta=0.00010 global_step=522\n",
      "Epoch 523:  Train_loss=0.628  Val_loss=0.622  Val_Acc=0.619  eta=0.00010 global_step=523\n",
      "Epoch 524:  Train_loss=0.628  Val_loss=0.622  Val_Acc=0.619  eta=0.00010 global_step=524\n",
      "Epoch 525:  Train_loss=0.628  Val_loss=0.622  Val_Acc=0.619  eta=0.00010 global_step=525\n",
      "Epoch 526:  Train_loss=0.628  Val_loss=0.621  Val_Acc=0.619  eta=0.00010 global_step=526\n",
      "Epoch 527:  Train_loss=0.627  Val_loss=0.621  Val_Acc=0.619  eta=0.00010 global_step=527\n",
      "Epoch 528:  Train_loss=0.627  Val_loss=0.621  Val_Acc=0.619  eta=0.00010 global_step=528\n",
      "Epoch 529:  Train_loss=0.627  Val_loss=0.621  Val_Acc=0.619  eta=0.00010 global_step=529\n",
      "Epoch 530:  Train_loss=0.627  Val_loss=0.620  Val_Acc=0.619  eta=0.00010 global_step=530\n",
      "Epoch 531:  Train_loss=0.626  Val_loss=0.620  Val_Acc=0.619  eta=0.00010 global_step=531\n",
      "Epoch 532:  Train_loss=0.626  Val_loss=0.620  Val_Acc=0.619  eta=0.00010 global_step=532\n",
      "Epoch 533:  Train_loss=0.626  Val_loss=0.619  Val_Acc=0.619  eta=0.00010 global_step=533\n",
      "Epoch 534:  Train_loss=0.626  Val_loss=0.619  Val_Acc=0.619  eta=0.00010 global_step=534\n",
      "Epoch 535:  Train_loss=0.625  Val_loss=0.619  Val_Acc=0.619  eta=0.00010 global_step=535\n",
      "Epoch 536:  Train_loss=0.625  Val_loss=0.619  Val_Acc=0.619  eta=0.00010 global_step=536\n",
      "Epoch 537:  Train_loss=0.625  Val_loss=0.618  Val_Acc=0.619  eta=0.00010 global_step=537\n",
      "Epoch 538:  Train_loss=0.625  Val_loss=0.618  Val_Acc=0.619  eta=0.00010 global_step=538\n",
      "Epoch 539:  Train_loss=0.624  Val_loss=0.618  Val_Acc=0.619  eta=0.00010 global_step=539\n",
      "Epoch 540:  Train_loss=0.624  Val_loss=0.617  Val_Acc=0.619  eta=0.00010 global_step=540\n",
      "Epoch 541:  Train_loss=0.624  Val_loss=0.617  Val_Acc=0.619  eta=0.00010 global_step=541\n",
      "Epoch 542:  Train_loss=0.623  Val_loss=0.617  Val_Acc=0.619  eta=0.00010 global_step=542\n",
      "Epoch 543:  Train_loss=0.623  Val_loss=0.617  Val_Acc=0.619  eta=0.00010 global_step=543\n",
      "Epoch 544:  Train_loss=0.623  Val_loss=0.616  Val_Acc=0.619  eta=0.00010 global_step=544\n",
      "Epoch 545:  Train_loss=0.623  Val_loss=0.616  Val_Acc=0.619  eta=0.00010 global_step=545\n",
      "Epoch 546:  Train_loss=0.622  Val_loss=0.616  Val_Acc=0.619  eta=0.00010 global_step=546\n",
      "Epoch 547:  Train_loss=0.622  Val_loss=0.615  Val_Acc=0.619  eta=0.00010 global_step=547\n",
      "Epoch 548:  Train_loss=0.622  Val_loss=0.615  Val_Acc=0.619  eta=0.00010 global_step=548\n",
      "Epoch 549:  Train_loss=0.622  Val_loss=0.615  Val_Acc=0.619  eta=0.00010 global_step=549\n",
      "Epoch 550:  Train_loss=0.621  Val_loss=0.615  Val_Acc=0.619  eta=0.00010 global_step=550\n",
      "Epoch 551:  Train_loss=0.621  Val_loss=0.614  Val_Acc=0.619  eta=0.00010 global_step=551\n",
      "Epoch 552:  Train_loss=0.621  Val_loss=0.614  Val_Acc=0.619  eta=0.00010 global_step=552\n",
      "Epoch 553:  Train_loss=0.620  Val_loss=0.614  Val_Acc=0.619  eta=0.00010 global_step=553\n",
      "Epoch 554:  Train_loss=0.620  Val_loss=0.613  Val_Acc=0.619  eta=0.00010 global_step=554\n",
      "Epoch 555:  Train_loss=0.620  Val_loss=0.613  Val_Acc=0.619  eta=0.00010 global_step=555\n",
      "Epoch 556:  Train_loss=0.620  Val_loss=0.613  Val_Acc=0.619  eta=0.00010 global_step=556\n",
      "Epoch 557:  Train_loss=0.619  Val_loss=0.613  Val_Acc=0.619  eta=0.00010 global_step=557\n",
      "Epoch 558:  Train_loss=0.619  Val_loss=0.612  Val_Acc=0.619  eta=0.00010 global_step=558\n",
      "Epoch 559:  Train_loss=0.619  Val_loss=0.612  Val_Acc=0.619  eta=0.00010 global_step=559\n",
      "Epoch 560:  Train_loss=0.619  Val_loss=0.612  Val_Acc=0.619  eta=0.00010 global_step=560\n",
      "Epoch 561:  Train_loss=0.618  Val_loss=0.611  Val_Acc=0.619  eta=0.00010 global_step=561\n",
      "Epoch 562:  Train_loss=0.618  Val_loss=0.611  Val_Acc=0.619  eta=0.00010 global_step=562\n",
      "Epoch 563:  Train_loss=0.618  Val_loss=0.611  Val_Acc=0.619  eta=0.00010 global_step=563\n",
      "Epoch 564:  Train_loss=0.617  Val_loss=0.611  Val_Acc=0.619  eta=0.00010 global_step=564\n",
      "Epoch 565:  Train_loss=0.617  Val_loss=0.610  Val_Acc=0.619  eta=0.00010 global_step=565\n",
      "Epoch 566:  Train_loss=0.617  Val_loss=0.610  Val_Acc=0.619  eta=0.00010 global_step=566\n",
      "Epoch 567:  Train_loss=0.617  Val_loss=0.610  Val_Acc=0.619  eta=0.00010 global_step=567\n",
      "Epoch 568:  Train_loss=0.616  Val_loss=0.609  Val_Acc=0.619  eta=0.00010 global_step=568\n",
      "Epoch 569:  Train_loss=0.616  Val_loss=0.609  Val_Acc=0.619  eta=0.00010 global_step=569\n",
      "Epoch 570:  Train_loss=0.616  Val_loss=0.609  Val_Acc=0.619  eta=0.00010 global_step=570\n",
      "Epoch 571:  Train_loss=0.616  Val_loss=0.608  Val_Acc=0.619  eta=0.00010 global_step=571\n",
      "Epoch 572:  Train_loss=0.615  Val_loss=0.608  Val_Acc=0.619  eta=0.00010 global_step=572\n",
      "Epoch 573:  Train_loss=0.615  Val_loss=0.608  Val_Acc=0.619  eta=0.00010 global_step=573\n",
      "Epoch 574:  Train_loss=0.615  Val_loss=0.608  Val_Acc=0.619  eta=0.00010 global_step=574\n",
      "Epoch 575:  Train_loss=0.614  Val_loss=0.607  Val_Acc=0.619  eta=0.00010 global_step=575\n",
      "Epoch 576:  Train_loss=0.614  Val_loss=0.607  Val_Acc=0.619  eta=0.00010 global_step=576\n",
      "Epoch 577:  Train_loss=0.614  Val_loss=0.607  Val_Acc=0.619  eta=0.00010 global_step=577\n",
      "Epoch 578:  Train_loss=0.614  Val_loss=0.606  Val_Acc=0.619  eta=0.00010 global_step=578\n",
      "Epoch 579:  Train_loss=0.613  Val_loss=0.606  Val_Acc=0.619  eta=0.00010 global_step=579\n",
      "Epoch 580:  Train_loss=0.613  Val_loss=0.606  Val_Acc=0.619  eta=0.00010 global_step=580\n",
      "Epoch 581:  Train_loss=0.613  Val_loss=0.606  Val_Acc=0.619  eta=0.00010 global_step=581\n",
      "Epoch 582:  Train_loss=0.613  Val_loss=0.605  Val_Acc=0.619  eta=0.00010 global_step=582\n",
      "Epoch 583:  Train_loss=0.612  Val_loss=0.605  Val_Acc=0.619  eta=0.00010 global_step=583\n",
      "Epoch 584:  Train_loss=0.612  Val_loss=0.605  Val_Acc=0.619  eta=0.00010 global_step=584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 585:  Train_loss=0.612  Val_loss=0.604  Val_Acc=0.619  eta=0.00010 global_step=585\n",
      "Epoch 586:  Train_loss=0.611  Val_loss=0.604  Val_Acc=0.619  eta=0.00010 global_step=586\n",
      "Epoch 587:  Train_loss=0.611  Val_loss=0.604  Val_Acc=0.619  eta=0.00010 global_step=587\n",
      "Epoch 588:  Train_loss=0.611  Val_loss=0.604  Val_Acc=0.619  eta=0.00010 global_step=588\n",
      "Epoch 589:  Train_loss=0.611  Val_loss=0.603  Val_Acc=0.619  eta=0.00010 global_step=589\n",
      "Epoch 590:  Train_loss=0.610  Val_loss=0.603  Val_Acc=0.619  eta=0.00010 global_step=590\n",
      "Epoch 591:  Train_loss=0.610  Val_loss=0.603  Val_Acc=0.619  eta=0.00010 global_step=591\n",
      "Epoch 592:  Train_loss=0.610  Val_loss=0.602  Val_Acc=0.619  eta=0.00010 global_step=592\n",
      "Epoch 593:  Train_loss=0.610  Val_loss=0.602  Val_Acc=0.619  eta=0.00010 global_step=593\n",
      "Epoch 594:  Train_loss=0.609  Val_loss=0.602  Val_Acc=0.619  eta=0.00010 global_step=594\n",
      "Epoch 595:  Train_loss=0.609  Val_loss=0.601  Val_Acc=0.619  eta=0.00010 global_step=595\n",
      "Epoch 596:  Train_loss=0.609  Val_loss=0.601  Val_Acc=0.619  eta=0.00010 global_step=596\n",
      "Epoch 597:  Train_loss=0.608  Val_loss=0.601  Val_Acc=0.619  eta=0.00010 global_step=597\n",
      "Epoch 598:  Train_loss=0.608  Val_loss=0.601  Val_Acc=0.619  eta=0.00010 global_step=598\n",
      "Epoch 599:  Train_loss=0.608  Val_loss=0.600  Val_Acc=0.619  eta=0.00010 global_step=599\n",
      "Epoch 600:  Train_loss=0.608  Val_loss=0.600  Val_Acc=0.619  eta=0.00010 global_step=600\n",
      "Epoch 601:  Train_loss=0.607  Val_loss=0.600  Val_Acc=0.619  eta=0.00010 global_step=601\n",
      "Epoch 602:  Train_loss=0.607  Val_loss=0.599  Val_Acc=0.619  eta=0.00010 global_step=602\n",
      "Epoch 603:  Train_loss=0.607  Val_loss=0.599  Val_Acc=0.619  eta=0.00010 global_step=603\n",
      "Epoch 604:  Train_loss=0.607  Val_loss=0.599  Val_Acc=0.619  eta=0.00010 global_step=604\n",
      "Epoch 605:  Train_loss=0.606  Val_loss=0.599  Val_Acc=0.619  eta=0.00010 global_step=605\n",
      "Epoch 606:  Train_loss=0.606  Val_loss=0.598  Val_Acc=0.627  eta=0.00010 global_step=606\n",
      "Epoch 607:  Train_loss=0.606  Val_loss=0.598  Val_Acc=0.627  eta=0.00010 global_step=607\n",
      "Epoch 608:  Train_loss=0.605  Val_loss=0.598  Val_Acc=0.627  eta=0.00010 global_step=608\n",
      "Epoch 609:  Train_loss=0.605  Val_loss=0.597  Val_Acc=0.627  eta=0.00010 global_step=609\n",
      "Epoch 610:  Train_loss=0.605  Val_loss=0.597  Val_Acc=0.627  eta=0.00010 global_step=610\n",
      "Epoch 611:  Train_loss=0.605  Val_loss=0.597  Val_Acc=0.627  eta=0.00010 global_step=611\n",
      "Epoch 612:  Train_loss=0.604  Val_loss=0.597  Val_Acc=0.627  eta=0.00010 global_step=612\n",
      "Epoch 613:  Train_loss=0.604  Val_loss=0.596  Val_Acc=0.627  eta=0.00010 global_step=613\n",
      "Epoch 614:  Train_loss=0.604  Val_loss=0.596  Val_Acc=0.627  eta=0.00010 global_step=614\n",
      "Epoch 615:  Train_loss=0.604  Val_loss=0.596  Val_Acc=0.627  eta=0.00010 global_step=615\n",
      "Epoch 616:  Train_loss=0.603  Val_loss=0.595  Val_Acc=0.627  eta=0.00010 global_step=616\n",
      "Epoch 617:  Train_loss=0.603  Val_loss=0.595  Val_Acc=0.627  eta=0.00010 global_step=617\n",
      "Epoch 618:  Train_loss=0.603  Val_loss=0.595  Val_Acc=0.627  eta=0.00010 global_step=618\n",
      "Epoch 619:  Train_loss=0.602  Val_loss=0.595  Val_Acc=0.627  eta=0.00010 global_step=619\n",
      "Epoch 620:  Train_loss=0.602  Val_loss=0.594  Val_Acc=0.627  eta=0.00010 global_step=620\n",
      "Epoch 621:  Train_loss=0.602  Val_loss=0.594  Val_Acc=0.634  eta=0.00010 global_step=621\n",
      "Epoch 622:  Train_loss=0.602  Val_loss=0.594  Val_Acc=0.634  eta=0.00010 global_step=622\n",
      "Epoch 623:  Train_loss=0.601  Val_loss=0.593  Val_Acc=0.634  eta=0.00010 global_step=623\n",
      "Epoch 624:  Train_loss=0.601  Val_loss=0.593  Val_Acc=0.634  eta=0.00010 global_step=624\n",
      "Epoch 625:  Train_loss=0.601  Val_loss=0.593  Val_Acc=0.634  eta=0.00010 global_step=625\n",
      "Epoch 626:  Train_loss=0.601  Val_loss=0.593  Val_Acc=0.634  eta=0.00010 global_step=626\n",
      "Epoch 627:  Train_loss=0.600  Val_loss=0.592  Val_Acc=0.634  eta=0.00010 global_step=627\n",
      "Epoch 628:  Train_loss=0.600  Val_loss=0.592  Val_Acc=0.634  eta=0.00010 global_step=628\n",
      "Epoch 629:  Train_loss=0.600  Val_loss=0.592  Val_Acc=0.634  eta=0.00010 global_step=629\n",
      "Epoch 630:  Train_loss=0.599  Val_loss=0.591  Val_Acc=0.634  eta=0.00010 global_step=630\n",
      "Epoch 631:  Train_loss=0.599  Val_loss=0.591  Val_Acc=0.634  eta=0.00010 global_step=631\n",
      "Epoch 632:  Train_loss=0.599  Val_loss=0.591  Val_Acc=0.634  eta=0.00010 global_step=632\n",
      "Epoch 633:  Train_loss=0.599  Val_loss=0.591  Val_Acc=0.642  eta=0.00010 global_step=633\n",
      "Epoch 634:  Train_loss=0.598  Val_loss=0.590  Val_Acc=0.642  eta=0.00010 global_step=634\n",
      "Epoch 635:  Train_loss=0.598  Val_loss=0.590  Val_Acc=0.642  eta=0.00010 global_step=635\n",
      "Epoch 636:  Train_loss=0.598  Val_loss=0.590  Val_Acc=0.642  eta=0.00010 global_step=636\n",
      "Epoch 637:  Train_loss=0.598  Val_loss=0.589  Val_Acc=0.649  eta=0.00010 global_step=637\n",
      "Epoch 638:  Train_loss=0.597  Val_loss=0.589  Val_Acc=0.649  eta=0.00010 global_step=638\n",
      "Epoch 639:  Train_loss=0.597  Val_loss=0.589  Val_Acc=0.649  eta=0.00010 global_step=639\n",
      "Epoch 640:  Train_loss=0.597  Val_loss=0.589  Val_Acc=0.649  eta=0.00010 global_step=640\n",
      "Epoch 641:  Train_loss=0.596  Val_loss=0.588  Val_Acc=0.649  eta=0.00010 global_step=641\n",
      "Epoch 642:  Train_loss=0.596  Val_loss=0.588  Val_Acc=0.657  eta=0.00010 global_step=642\n",
      "Epoch 643:  Train_loss=0.596  Val_loss=0.588  Val_Acc=0.657  eta=0.00010 global_step=643\n",
      "Epoch 644:  Train_loss=0.596  Val_loss=0.587  Val_Acc=0.657  eta=0.00010 global_step=644\n",
      "Epoch 645:  Train_loss=0.595  Val_loss=0.587  Val_Acc=0.657  eta=0.00010 global_step=645\n",
      "Epoch 646:  Train_loss=0.595  Val_loss=0.587  Val_Acc=0.657  eta=0.00010 global_step=646\n",
      "Epoch 647:  Train_loss=0.595  Val_loss=0.587  Val_Acc=0.657  eta=0.00010 global_step=647\n",
      "Epoch 648:  Train_loss=0.595  Val_loss=0.586  Val_Acc=0.657  eta=0.00010 global_step=648\n",
      "Epoch 649:  Train_loss=0.594  Val_loss=0.586  Val_Acc=0.657  eta=0.00010 global_step=649\n",
      "Epoch 650:  Train_loss=0.594  Val_loss=0.586  Val_Acc=0.657  eta=0.00010 global_step=650\n",
      "Epoch 651:  Train_loss=0.594  Val_loss=0.585  Val_Acc=0.657  eta=0.00010 global_step=651\n",
      "Epoch 652:  Train_loss=0.594  Val_loss=0.585  Val_Acc=0.657  eta=0.00010 global_step=652\n",
      "Epoch 653:  Train_loss=0.593  Val_loss=0.585  Val_Acc=0.657  eta=0.00010 global_step=653\n",
      "Epoch 654:  Train_loss=0.593  Val_loss=0.585  Val_Acc=0.657  eta=0.00010 global_step=654\n",
      "Epoch 655:  Train_loss=0.593  Val_loss=0.584  Val_Acc=0.657  eta=0.00010 global_step=655\n",
      "Epoch 656:  Train_loss=0.593  Val_loss=0.584  Val_Acc=0.657  eta=0.00010 global_step=656\n",
      "Epoch 657:  Train_loss=0.592  Val_loss=0.584  Val_Acc=0.657  eta=0.00010 global_step=657\n",
      "Epoch 658:  Train_loss=0.592  Val_loss=0.584  Val_Acc=0.664  eta=0.00010 global_step=658\n",
      "Epoch 659:  Train_loss=0.592  Val_loss=0.583  Val_Acc=0.664  eta=0.00010 global_step=659\n",
      "Epoch 660:  Train_loss=0.591  Val_loss=0.583  Val_Acc=0.664  eta=0.00010 global_step=660\n",
      "Epoch 661:  Train_loss=0.591  Val_loss=0.583  Val_Acc=0.664  eta=0.00010 global_step=661\n",
      "Epoch 662:  Train_loss=0.591  Val_loss=0.582  Val_Acc=0.664  eta=0.00010 global_step=662\n",
      "Epoch 663:  Train_loss=0.591  Val_loss=0.582  Val_Acc=0.664  eta=0.00010 global_step=663\n",
      "Epoch 664:  Train_loss=0.590  Val_loss=0.582  Val_Acc=0.664  eta=0.00010 global_step=664\n",
      "Epoch 665:  Train_loss=0.590  Val_loss=0.582  Val_Acc=0.664  eta=0.00010 global_step=665\n",
      "Epoch 666:  Train_loss=0.590  Val_loss=0.581  Val_Acc=0.664  eta=0.00010 global_step=666\n",
      "Epoch 667:  Train_loss=0.590  Val_loss=0.581  Val_Acc=0.664  eta=0.00010 global_step=667\n",
      "Epoch 668:  Train_loss=0.589  Val_loss=0.581  Val_Acc=0.664  eta=0.00010 global_step=668\n",
      "Epoch 669:  Train_loss=0.589  Val_loss=0.580  Val_Acc=0.664  eta=0.00010 global_step=669\n",
      "Epoch 670:  Train_loss=0.589  Val_loss=0.580  Val_Acc=0.672  eta=0.00010 global_step=670\n",
      "Epoch 671:  Train_loss=0.589  Val_loss=0.580  Val_Acc=0.679  eta=0.00010 global_step=671\n",
      "Epoch 672:  Train_loss=0.588  Val_loss=0.580  Val_Acc=0.687  eta=0.00010 global_step=672\n",
      "Epoch 673:  Train_loss=0.588  Val_loss=0.579  Val_Acc=0.687  eta=0.00010 global_step=673\n",
      "Epoch 674:  Train_loss=0.588  Val_loss=0.579  Val_Acc=0.687  eta=0.00010 global_step=674\n",
      "Epoch 675:  Train_loss=0.588  Val_loss=0.579  Val_Acc=0.687  eta=0.00010 global_step=675\n",
      "Epoch 676:  Train_loss=0.587  Val_loss=0.579  Val_Acc=0.687  eta=0.00010 global_step=676\n",
      "Epoch 677:  Train_loss=0.587  Val_loss=0.578  Val_Acc=0.687  eta=0.00010 global_step=677\n",
      "Epoch 678:  Train_loss=0.587  Val_loss=0.578  Val_Acc=0.687  eta=0.00010 global_step=678\n",
      "Epoch 679:  Train_loss=0.587  Val_loss=0.578  Val_Acc=0.679  eta=0.00010 global_step=679\n",
      "Epoch 680:  Train_loss=0.586  Val_loss=0.577  Val_Acc=0.679  eta=0.00010 global_step=680\n",
      "Epoch 681:  Train_loss=0.586  Val_loss=0.577  Val_Acc=0.679  eta=0.00010 global_step=681\n",
      "Epoch 682:  Train_loss=0.586  Val_loss=0.577  Val_Acc=0.679  eta=0.00010 global_step=682\n",
      "Epoch 683:  Train_loss=0.585  Val_loss=0.577  Val_Acc=0.679  eta=0.00010 global_step=683\n",
      "Epoch 684:  Train_loss=0.585  Val_loss=0.576  Val_Acc=0.679  eta=0.00010 global_step=684\n",
      "Epoch 685:  Train_loss=0.585  Val_loss=0.576  Val_Acc=0.679  eta=0.00010 global_step=685\n",
      "Epoch 686:  Train_loss=0.585  Val_loss=0.576  Val_Acc=0.679  eta=0.00010 global_step=686\n",
      "Epoch 687:  Train_loss=0.584  Val_loss=0.576  Val_Acc=0.679  eta=0.00010 global_step=687\n",
      "Epoch 688:  Train_loss=0.584  Val_loss=0.575  Val_Acc=0.679  eta=0.00010 global_step=688\n",
      "Epoch 689:  Train_loss=0.584  Val_loss=0.575  Val_Acc=0.679  eta=0.00010 global_step=689\n",
      "Epoch 690:  Train_loss=0.584  Val_loss=0.575  Val_Acc=0.679  eta=0.00010 global_step=690\n",
      "Epoch 691:  Train_loss=0.583  Val_loss=0.574  Val_Acc=0.679  eta=0.00010 global_step=691\n",
      "Epoch 692:  Train_loss=0.583  Val_loss=0.574  Val_Acc=0.679  eta=0.00010 global_step=692\n",
      "Epoch 693:  Train_loss=0.583  Val_loss=0.574  Val_Acc=0.679  eta=0.00010 global_step=693\n",
      "Epoch 694:  Train_loss=0.583  Val_loss=0.574  Val_Acc=0.679  eta=0.00010 global_step=694\n",
      "Epoch 695:  Train_loss=0.582  Val_loss=0.573  Val_Acc=0.679  eta=0.00010 global_step=695\n",
      "Epoch 696:  Train_loss=0.582  Val_loss=0.573  Val_Acc=0.679  eta=0.00010 global_step=696\n",
      "Epoch 697:  Train_loss=0.582  Val_loss=0.573  Val_Acc=0.679  eta=0.00010 global_step=697\n",
      "Epoch 698:  Train_loss=0.582  Val_loss=0.573  Val_Acc=0.679  eta=0.00010 global_step=698\n",
      "Epoch 699:  Train_loss=0.581  Val_loss=0.572  Val_Acc=0.679  eta=0.00010 global_step=699\n",
      "Epoch 700:  Train_loss=0.581  Val_loss=0.572  Val_Acc=0.679  eta=0.00010 global_step=700\n",
      "Epoch 701:  Train_loss=0.581  Val_loss=0.572  Val_Acc=0.679  eta=0.00010 global_step=701\n",
      "Epoch 702:  Train_loss=0.581  Val_loss=0.572  Val_Acc=0.679  eta=0.00010 global_step=702\n",
      "Epoch 703:  Train_loss=0.580  Val_loss=0.571  Val_Acc=0.679  eta=0.00010 global_step=703\n",
      "Epoch 704:  Train_loss=0.580  Val_loss=0.571  Val_Acc=0.679  eta=0.00010 global_step=704\n",
      "Epoch 705:  Train_loss=0.580  Val_loss=0.571  Val_Acc=0.679  eta=0.00010 global_step=705\n",
      "Epoch 706:  Train_loss=0.580  Val_loss=0.570  Val_Acc=0.687  eta=0.00010 global_step=706\n",
      "Epoch 707:  Train_loss=0.579  Val_loss=0.570  Val_Acc=0.687  eta=0.00010 global_step=707\n",
      "Epoch 708:  Train_loss=0.579  Val_loss=0.570  Val_Acc=0.687  eta=0.00010 global_step=708\n",
      "Epoch 709:  Train_loss=0.579  Val_loss=0.570  Val_Acc=0.687  eta=0.00010 global_step=709\n",
      "Epoch 710:  Train_loss=0.579  Val_loss=0.569  Val_Acc=0.687  eta=0.00010 global_step=710\n",
      "Epoch 711:  Train_loss=0.578  Val_loss=0.569  Val_Acc=0.694  eta=0.00010 global_step=711\n",
      "Epoch 712:  Train_loss=0.578  Val_loss=0.569  Val_Acc=0.694  eta=0.00010 global_step=712\n",
      "Epoch 713:  Train_loss=0.578  Val_loss=0.569  Val_Acc=0.694  eta=0.00010 global_step=713\n",
      "Epoch 714:  Train_loss=0.578  Val_loss=0.568  Val_Acc=0.694  eta=0.00010 global_step=714\n",
      "Epoch 715:  Train_loss=0.577  Val_loss=0.568  Val_Acc=0.694  eta=0.00010 global_step=715\n",
      "Epoch 716:  Train_loss=0.577  Val_loss=0.568  Val_Acc=0.694  eta=0.00010 global_step=716\n",
      "Epoch 717:  Train_loss=0.577  Val_loss=0.568  Val_Acc=0.694  eta=0.00010 global_step=717\n",
      "Epoch 718:  Train_loss=0.577  Val_loss=0.567  Val_Acc=0.694  eta=0.00010 global_step=718\n",
      "Epoch 719:  Train_loss=0.576  Val_loss=0.567  Val_Acc=0.694  eta=0.00010 global_step=719\n",
      "Epoch 720:  Train_loss=0.576  Val_loss=0.567  Val_Acc=0.694  eta=0.00010 global_step=720\n",
      "Epoch 721:  Train_loss=0.576  Val_loss=0.567  Val_Acc=0.694  eta=0.00010 global_step=721\n",
      "Epoch 722:  Train_loss=0.576  Val_loss=0.566  Val_Acc=0.694  eta=0.00010 global_step=722\n",
      "Epoch 723:  Train_loss=0.575  Val_loss=0.566  Val_Acc=0.694  eta=0.00010 global_step=723\n",
      "Epoch 724:  Train_loss=0.575  Val_loss=0.566  Val_Acc=0.694  eta=0.00010 global_step=724\n",
      "Epoch 725:  Train_loss=0.575  Val_loss=0.566  Val_Acc=0.694  eta=0.00010 global_step=725\n",
      "Epoch 726:  Train_loss=0.575  Val_loss=0.565  Val_Acc=0.694  eta=0.00010 global_step=726\n",
      "Epoch 727:  Train_loss=0.574  Val_loss=0.565  Val_Acc=0.694  eta=0.00010 global_step=727\n",
      "Epoch 728:  Train_loss=0.574  Val_loss=0.565  Val_Acc=0.694  eta=0.00010 global_step=728\n",
      "Epoch 729:  Train_loss=0.574  Val_loss=0.564  Val_Acc=0.694  eta=0.00010 global_step=729\n",
      "Epoch 730:  Train_loss=0.574  Val_loss=0.564  Val_Acc=0.694  eta=0.00010 global_step=730\n",
      "Epoch 731:  Train_loss=0.573  Val_loss=0.564  Val_Acc=0.694  eta=0.00010 global_step=731\n",
      "Epoch 732:  Train_loss=0.573  Val_loss=0.564  Val_Acc=0.694  eta=0.00010 global_step=732\n",
      "Epoch 733:  Train_loss=0.573  Val_loss=0.563  Val_Acc=0.701  eta=0.00010 global_step=733\n",
      "Epoch 734:  Train_loss=0.573  Val_loss=0.563  Val_Acc=0.701  eta=0.00010 global_step=734\n",
      "Epoch 735:  Train_loss=0.573  Val_loss=0.563  Val_Acc=0.701  eta=0.00010 global_step=735\n",
      "Epoch 736:  Train_loss=0.572  Val_loss=0.563  Val_Acc=0.701  eta=0.00010 global_step=736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 737:  Train_loss=0.572  Val_loss=0.562  Val_Acc=0.701  eta=0.00010 global_step=737\n",
      "Epoch 738:  Train_loss=0.572  Val_loss=0.562  Val_Acc=0.701  eta=0.00010 global_step=738\n",
      "Epoch 739:  Train_loss=0.572  Val_loss=0.562  Val_Acc=0.701  eta=0.00010 global_step=739\n",
      "Epoch 740:  Train_loss=0.571  Val_loss=0.562  Val_Acc=0.701  eta=0.00010 global_step=740\n",
      "Epoch 741:  Train_loss=0.571  Val_loss=0.561  Val_Acc=0.701  eta=0.00010 global_step=741\n",
      "Epoch 742:  Train_loss=0.571  Val_loss=0.561  Val_Acc=0.701  eta=0.00010 global_step=742\n",
      "Epoch 743:  Train_loss=0.571  Val_loss=0.561  Val_Acc=0.701  eta=0.00010 global_step=743\n",
      "Epoch 744:  Train_loss=0.570  Val_loss=0.561  Val_Acc=0.701  eta=0.00010 global_step=744\n",
      "Epoch 745:  Train_loss=0.570  Val_loss=0.560  Val_Acc=0.701  eta=0.00010 global_step=745\n",
      "Epoch 746:  Train_loss=0.570  Val_loss=0.560  Val_Acc=0.701  eta=0.00010 global_step=746\n",
      "Epoch 747:  Train_loss=0.570  Val_loss=0.560  Val_Acc=0.701  eta=0.00010 global_step=747\n",
      "Epoch 748:  Train_loss=0.569  Val_loss=0.560  Val_Acc=0.701  eta=0.00010 global_step=748\n",
      "Epoch 749:  Train_loss=0.569  Val_loss=0.559  Val_Acc=0.701  eta=0.00010 global_step=749\n",
      "Epoch 750:  Train_loss=0.569  Val_loss=0.559  Val_Acc=0.701  eta=0.00010 global_step=750\n",
      "Epoch 751:  Train_loss=0.569  Val_loss=0.559  Val_Acc=0.701  eta=0.00010 global_step=751\n",
      "Epoch 752:  Train_loss=0.568  Val_loss=0.559  Val_Acc=0.701  eta=0.00010 global_step=752\n",
      "Epoch 753:  Train_loss=0.568  Val_loss=0.558  Val_Acc=0.701  eta=0.00010 global_step=753\n",
      "Epoch 754:  Train_loss=0.568  Val_loss=0.558  Val_Acc=0.701  eta=0.00010 global_step=754\n",
      "Epoch 755:  Train_loss=0.568  Val_loss=0.558  Val_Acc=0.701  eta=0.00010 global_step=755\n",
      "Epoch 756:  Train_loss=0.568  Val_loss=0.558  Val_Acc=0.701  eta=0.00010 global_step=756\n",
      "Epoch 757:  Train_loss=0.567  Val_loss=0.557  Val_Acc=0.701  eta=0.00010 global_step=757\n",
      "Epoch 758:  Train_loss=0.567  Val_loss=0.557  Val_Acc=0.701  eta=0.00010 global_step=758\n",
      "Epoch 759:  Train_loss=0.567  Val_loss=0.557  Val_Acc=0.701  eta=0.00010 global_step=759\n",
      "Epoch 760:  Train_loss=0.567  Val_loss=0.557  Val_Acc=0.701  eta=0.00010 global_step=760\n",
      "Epoch 761:  Train_loss=0.566  Val_loss=0.556  Val_Acc=0.701  eta=0.00010 global_step=761\n",
      "Epoch 762:  Train_loss=0.566  Val_loss=0.556  Val_Acc=0.701  eta=0.00010 global_step=762\n",
      "Epoch 763:  Train_loss=0.566  Val_loss=0.556  Val_Acc=0.701  eta=0.00010 global_step=763\n",
      "Epoch 764:  Train_loss=0.566  Val_loss=0.556  Val_Acc=0.701  eta=0.00010 global_step=764\n",
      "Epoch 765:  Train_loss=0.565  Val_loss=0.555  Val_Acc=0.701  eta=0.00010 global_step=765\n",
      "Epoch 766:  Train_loss=0.565  Val_loss=0.555  Val_Acc=0.701  eta=0.00010 global_step=766\n",
      "Epoch 767:  Train_loss=0.565  Val_loss=0.555  Val_Acc=0.701  eta=0.00010 global_step=767\n",
      "Epoch 768:  Train_loss=0.565  Val_loss=0.555  Val_Acc=0.701  eta=0.00010 global_step=768\n",
      "Epoch 769:  Train_loss=0.565  Val_loss=0.555  Val_Acc=0.701  eta=0.00010 global_step=769\n",
      "Epoch 770:  Train_loss=0.564  Val_loss=0.554  Val_Acc=0.701  eta=0.00010 global_step=770\n",
      "Epoch 771:  Train_loss=0.564  Val_loss=0.554  Val_Acc=0.701  eta=0.00010 global_step=771\n",
      "Epoch 772:  Train_loss=0.564  Val_loss=0.554  Val_Acc=0.701  eta=0.00010 global_step=772\n",
      "Epoch 773:  Train_loss=0.564  Val_loss=0.554  Val_Acc=0.701  eta=0.00010 global_step=773\n",
      "Epoch 774:  Train_loss=0.563  Val_loss=0.553  Val_Acc=0.701  eta=0.00010 global_step=774\n",
      "Epoch 775:  Train_loss=0.563  Val_loss=0.553  Val_Acc=0.701  eta=0.00010 global_step=775\n",
      "Epoch 776:  Train_loss=0.563  Val_loss=0.553  Val_Acc=0.701  eta=0.00010 global_step=776\n",
      "Epoch 777:  Train_loss=0.563  Val_loss=0.553  Val_Acc=0.701  eta=0.00010 global_step=777\n",
      "Epoch 778:  Train_loss=0.562  Val_loss=0.552  Val_Acc=0.701  eta=0.00010 global_step=778\n",
      "Epoch 779:  Train_loss=0.562  Val_loss=0.552  Val_Acc=0.701  eta=0.00010 global_step=779\n",
      "Epoch 780:  Train_loss=0.562  Val_loss=0.552  Val_Acc=0.701  eta=0.00010 global_step=780\n",
      "Epoch 781:  Train_loss=0.562  Val_loss=0.552  Val_Acc=0.709  eta=0.00010 global_step=781\n",
      "Epoch 782:  Train_loss=0.562  Val_loss=0.551  Val_Acc=0.709  eta=0.00010 global_step=782\n",
      "Epoch 783:  Train_loss=0.561  Val_loss=0.551  Val_Acc=0.709  eta=0.00010 global_step=783\n",
      "Epoch 784:  Train_loss=0.561  Val_loss=0.551  Val_Acc=0.709  eta=0.00010 global_step=784\n",
      "Epoch 785:  Train_loss=0.561  Val_loss=0.551  Val_Acc=0.709  eta=0.00010 global_step=785\n",
      "Epoch 786:  Train_loss=0.561  Val_loss=0.550  Val_Acc=0.709  eta=0.00010 global_step=786\n",
      "Epoch 787:  Train_loss=0.560  Val_loss=0.550  Val_Acc=0.709  eta=0.00010 global_step=787\n",
      "Epoch 788:  Train_loss=0.560  Val_loss=0.550  Val_Acc=0.709  eta=0.00010 global_step=788\n",
      "Epoch 789:  Train_loss=0.560  Val_loss=0.550  Val_Acc=0.709  eta=0.00010 global_step=789\n",
      "Epoch 790:  Train_loss=0.560  Val_loss=0.550  Val_Acc=0.716  eta=0.00010 global_step=790\n",
      "Epoch 791:  Train_loss=0.560  Val_loss=0.549  Val_Acc=0.716  eta=0.00010 global_step=791\n",
      "Epoch 792:  Train_loss=0.559  Val_loss=0.549  Val_Acc=0.716  eta=0.00010 global_step=792\n",
      "Epoch 793:  Train_loss=0.559  Val_loss=0.549  Val_Acc=0.716  eta=0.00010 global_step=793\n",
      "Epoch 794:  Train_loss=0.559  Val_loss=0.549  Val_Acc=0.716  eta=0.00010 global_step=794\n",
      "Epoch 795:  Train_loss=0.559  Val_loss=0.548  Val_Acc=0.716  eta=0.00010 global_step=795\n",
      "Epoch 796:  Train_loss=0.558  Val_loss=0.548  Val_Acc=0.716  eta=0.00010 global_step=796\n",
      "Epoch 797:  Train_loss=0.558  Val_loss=0.548  Val_Acc=0.716  eta=0.00010 global_step=797\n",
      "Epoch 798:  Train_loss=0.558  Val_loss=0.548  Val_Acc=0.716  eta=0.00010 global_step=798\n",
      "Epoch 799:  Train_loss=0.558  Val_loss=0.547  Val_Acc=0.716  eta=0.00010 global_step=799\n",
      "Epoch 800:  Train_loss=0.558  Val_loss=0.547  Val_Acc=0.716  eta=0.00010 global_step=800\n",
      "Epoch 801:  Train_loss=0.557  Val_loss=0.547  Val_Acc=0.716  eta=0.00010 global_step=801\n",
      "Epoch 802:  Train_loss=0.557  Val_loss=0.547  Val_Acc=0.716  eta=0.00010 global_step=802\n",
      "Epoch 803:  Train_loss=0.557  Val_loss=0.547  Val_Acc=0.716  eta=0.00010 global_step=803\n",
      "Epoch 804:  Train_loss=0.557  Val_loss=0.546  Val_Acc=0.716  eta=0.00010 global_step=804\n",
      "Epoch 805:  Train_loss=0.556  Val_loss=0.546  Val_Acc=0.724  eta=0.00010 global_step=805\n",
      "Epoch 806:  Train_loss=0.556  Val_loss=0.546  Val_Acc=0.724  eta=0.00010 global_step=806\n",
      "Epoch 807:  Train_loss=0.556  Val_loss=0.546  Val_Acc=0.724  eta=0.00010 global_step=807\n",
      "Epoch 808:  Train_loss=0.556  Val_loss=0.545  Val_Acc=0.724  eta=0.00010 global_step=808\n",
      "Epoch 809:  Train_loss=0.556  Val_loss=0.545  Val_Acc=0.731  eta=0.00010 global_step=809\n",
      "Epoch 810:  Train_loss=0.555  Val_loss=0.545  Val_Acc=0.731  eta=0.00010 global_step=810\n",
      "Epoch 811:  Train_loss=0.555  Val_loss=0.545  Val_Acc=0.731  eta=0.00010 global_step=811\n",
      "Epoch 812:  Train_loss=0.555  Val_loss=0.544  Val_Acc=0.731  eta=0.00010 global_step=812\n",
      "Epoch 813:  Train_loss=0.555  Val_loss=0.544  Val_Acc=0.731  eta=0.00010 global_step=813\n",
      "Epoch 814:  Train_loss=0.555  Val_loss=0.544  Val_Acc=0.731  eta=0.00010 global_step=814\n",
      "Epoch 815:  Train_loss=0.554  Val_loss=0.544  Val_Acc=0.731  eta=0.00010 global_step=815\n",
      "Epoch 816:  Train_loss=0.554  Val_loss=0.544  Val_Acc=0.731  eta=0.00010 global_step=816\n",
      "Epoch 817:  Train_loss=0.554  Val_loss=0.543  Val_Acc=0.731  eta=0.00010 global_step=817\n",
      "Epoch 818:  Train_loss=0.554  Val_loss=0.543  Val_Acc=0.731  eta=0.00010 global_step=818\n",
      "Epoch 819:  Train_loss=0.553  Val_loss=0.543  Val_Acc=0.731  eta=0.00010 global_step=819\n",
      "Epoch 820:  Train_loss=0.553  Val_loss=0.543  Val_Acc=0.731  eta=0.00010 global_step=820\n",
      "Epoch 821:  Train_loss=0.553  Val_loss=0.542  Val_Acc=0.731  eta=0.00010 global_step=821\n",
      "Epoch 822:  Train_loss=0.553  Val_loss=0.542  Val_Acc=0.731  eta=0.00010 global_step=822\n",
      "Epoch 823:  Train_loss=0.553  Val_loss=0.542  Val_Acc=0.731  eta=0.00010 global_step=823\n",
      "Epoch 824:  Train_loss=0.552  Val_loss=0.542  Val_Acc=0.731  eta=0.00010 global_step=824\n",
      "Epoch 825:  Train_loss=0.552  Val_loss=0.542  Val_Acc=0.731  eta=0.00010 global_step=825\n",
      "Epoch 826:  Train_loss=0.552  Val_loss=0.541  Val_Acc=0.731  eta=0.00010 global_step=826\n",
      "Epoch 827:  Train_loss=0.552  Val_loss=0.541  Val_Acc=0.731  eta=0.00010 global_step=827\n",
      "Epoch 828:  Train_loss=0.552  Val_loss=0.541  Val_Acc=0.731  eta=0.00010 global_step=828\n",
      "Epoch 829:  Train_loss=0.551  Val_loss=0.541  Val_Acc=0.731  eta=0.00010 global_step=829\n",
      "Epoch 830:  Train_loss=0.551  Val_loss=0.540  Val_Acc=0.731  eta=0.00010 global_step=830\n",
      "Epoch 831:  Train_loss=0.551  Val_loss=0.540  Val_Acc=0.731  eta=0.00010 global_step=831\n",
      "Epoch 832:  Train_loss=0.551  Val_loss=0.540  Val_Acc=0.731  eta=0.00010 global_step=832\n",
      "Epoch 833:  Train_loss=0.550  Val_loss=0.540  Val_Acc=0.731  eta=0.00010 global_step=833\n",
      "Epoch 834:  Train_loss=0.550  Val_loss=0.540  Val_Acc=0.731  eta=0.00010 global_step=834\n",
      "Epoch 835:  Train_loss=0.550  Val_loss=0.539  Val_Acc=0.731  eta=0.00010 global_step=835\n",
      "Epoch 836:  Train_loss=0.550  Val_loss=0.539  Val_Acc=0.731  eta=0.00010 global_step=836\n",
      "Epoch 837:  Train_loss=0.550  Val_loss=0.539  Val_Acc=0.731  eta=0.00010 global_step=837\n",
      "Epoch 838:  Train_loss=0.549  Val_loss=0.539  Val_Acc=0.731  eta=0.00010 global_step=838\n",
      "Epoch 839:  Train_loss=0.549  Val_loss=0.538  Val_Acc=0.731  eta=0.00010 global_step=839\n",
      "Epoch 840:  Train_loss=0.549  Val_loss=0.538  Val_Acc=0.731  eta=0.00010 global_step=840\n",
      "Epoch 841:  Train_loss=0.549  Val_loss=0.538  Val_Acc=0.731  eta=0.00010 global_step=841\n",
      "Epoch 842:  Train_loss=0.549  Val_loss=0.538  Val_Acc=0.731  eta=0.00010 global_step=842\n",
      "Epoch 843:  Train_loss=0.548  Val_loss=0.538  Val_Acc=0.731  eta=0.00010 global_step=843\n",
      "Epoch 844:  Train_loss=0.548  Val_loss=0.537  Val_Acc=0.731  eta=0.00010 global_step=844\n",
      "Epoch 845:  Train_loss=0.548  Val_loss=0.537  Val_Acc=0.731  eta=0.00010 global_step=845\n",
      "Epoch 846:  Train_loss=0.548  Val_loss=0.537  Val_Acc=0.731  eta=0.00010 global_step=846\n",
      "Epoch 847:  Train_loss=0.548  Val_loss=0.537  Val_Acc=0.731  eta=0.00010 global_step=847\n",
      "Epoch 848:  Train_loss=0.547  Val_loss=0.536  Val_Acc=0.731  eta=0.00010 global_step=848\n",
      "Epoch 849:  Train_loss=0.547  Val_loss=0.536  Val_Acc=0.731  eta=0.00010 global_step=849\n",
      "Epoch 850:  Train_loss=0.547  Val_loss=0.536  Val_Acc=0.731  eta=0.00010 global_step=850\n",
      "Epoch 851:  Train_loss=0.547  Val_loss=0.536  Val_Acc=0.731  eta=0.00010 global_step=851\n",
      "Epoch 852:  Train_loss=0.547  Val_loss=0.536  Val_Acc=0.731  eta=0.00010 global_step=852\n",
      "Epoch 853:  Train_loss=0.546  Val_loss=0.535  Val_Acc=0.731  eta=0.00010 global_step=853\n",
      "Epoch 854:  Train_loss=0.546  Val_loss=0.535  Val_Acc=0.731  eta=0.00010 global_step=854\n",
      "Epoch 855:  Train_loss=0.546  Val_loss=0.535  Val_Acc=0.731  eta=0.00010 global_step=855\n",
      "Epoch 856:  Train_loss=0.546  Val_loss=0.535  Val_Acc=0.731  eta=0.00010 global_step=856\n",
      "Epoch 857:  Train_loss=0.546  Val_loss=0.535  Val_Acc=0.731  eta=0.00010 global_step=857\n",
      "Epoch 858:  Train_loss=0.545  Val_loss=0.534  Val_Acc=0.731  eta=0.00010 global_step=858\n",
      "Epoch 859:  Train_loss=0.545  Val_loss=0.534  Val_Acc=0.731  eta=0.00010 global_step=859\n",
      "Epoch 860:  Train_loss=0.545  Val_loss=0.534  Val_Acc=0.731  eta=0.00010 global_step=860\n",
      "Epoch 861:  Train_loss=0.545  Val_loss=0.534  Val_Acc=0.731  eta=0.00010 global_step=861\n",
      "Epoch 862:  Train_loss=0.545  Val_loss=0.533  Val_Acc=0.731  eta=0.00010 global_step=862\n",
      "Epoch 863:  Train_loss=0.544  Val_loss=0.533  Val_Acc=0.731  eta=0.00010 global_step=863\n",
      "Epoch 864:  Train_loss=0.544  Val_loss=0.533  Val_Acc=0.731  eta=0.00010 global_step=864\n",
      "Epoch 865:  Train_loss=0.544  Val_loss=0.533  Val_Acc=0.731  eta=0.00010 global_step=865\n",
      "Epoch 866:  Train_loss=0.544  Val_loss=0.533  Val_Acc=0.731  eta=0.00010 global_step=866\n",
      "Epoch 867:  Train_loss=0.544  Val_loss=0.532  Val_Acc=0.731  eta=0.00010 global_step=867\n",
      "Epoch 868:  Train_loss=0.543  Val_loss=0.532  Val_Acc=0.731  eta=0.00010 global_step=868\n",
      "Epoch 869:  Train_loss=0.543  Val_loss=0.532  Val_Acc=0.731  eta=0.00010 global_step=869\n",
      "Epoch 870:  Train_loss=0.543  Val_loss=0.532  Val_Acc=0.731  eta=0.00010 global_step=870\n",
      "Epoch 871:  Train_loss=0.543  Val_loss=0.532  Val_Acc=0.731  eta=0.00010 global_step=871\n",
      "Epoch 872:  Train_loss=0.543  Val_loss=0.531  Val_Acc=0.731  eta=0.00010 global_step=872\n",
      "Epoch 873:  Train_loss=0.542  Val_loss=0.531  Val_Acc=0.739  eta=0.00010 global_step=873\n",
      "Epoch 874:  Train_loss=0.542  Val_loss=0.531  Val_Acc=0.739  eta=0.00010 global_step=874\n",
      "Epoch 875:  Train_loss=0.542  Val_loss=0.531  Val_Acc=0.739  eta=0.00010 global_step=875\n",
      "Epoch 876:  Train_loss=0.542  Val_loss=0.530  Val_Acc=0.739  eta=0.00010 global_step=876\n",
      "Epoch 877:  Train_loss=0.542  Val_loss=0.530  Val_Acc=0.739  eta=0.00010 global_step=877\n",
      "Epoch 878:  Train_loss=0.541  Val_loss=0.530  Val_Acc=0.739  eta=0.00010 global_step=878\n",
      "Epoch 879:  Train_loss=0.541  Val_loss=0.530  Val_Acc=0.739  eta=0.00010 global_step=879\n",
      "Epoch 880:  Train_loss=0.541  Val_loss=0.530  Val_Acc=0.739  eta=0.00010 global_step=880\n",
      "Epoch 881:  Train_loss=0.541  Val_loss=0.529  Val_Acc=0.739  eta=0.00010 global_step=881\n",
      "Epoch 882:  Train_loss=0.541  Val_loss=0.529  Val_Acc=0.739  eta=0.00010 global_step=882\n",
      "Epoch 883:  Train_loss=0.540  Val_loss=0.529  Val_Acc=0.739  eta=0.00010 global_step=883\n",
      "Epoch 884:  Train_loss=0.540  Val_loss=0.529  Val_Acc=0.739  eta=0.00010 global_step=884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 885:  Train_loss=0.540  Val_loss=0.529  Val_Acc=0.754  eta=0.00010 global_step=885\n",
      "Epoch 886:  Train_loss=0.540  Val_loss=0.528  Val_Acc=0.754  eta=0.00010 global_step=886\n",
      "Epoch 887:  Train_loss=0.540  Val_loss=0.528  Val_Acc=0.754  eta=0.00010 global_step=887\n",
      "Epoch 888:  Train_loss=0.540  Val_loss=0.528  Val_Acc=0.754  eta=0.00010 global_step=888\n",
      "Epoch 889:  Train_loss=0.539  Val_loss=0.528  Val_Acc=0.754  eta=0.00010 global_step=889\n",
      "Epoch 890:  Train_loss=0.539  Val_loss=0.528  Val_Acc=0.754  eta=0.00010 global_step=890\n",
      "Epoch 891:  Train_loss=0.539  Val_loss=0.527  Val_Acc=0.754  eta=0.00010 global_step=891\n",
      "Epoch 892:  Train_loss=0.539  Val_loss=0.527  Val_Acc=0.754  eta=0.00010 global_step=892\n",
      "Epoch 893:  Train_loss=0.539  Val_loss=0.527  Val_Acc=0.754  eta=0.00010 global_step=893\n",
      "Epoch 894:  Train_loss=0.538  Val_loss=0.527  Val_Acc=0.754  eta=0.00010 global_step=894\n",
      "Epoch 895:  Train_loss=0.538  Val_loss=0.527  Val_Acc=0.754  eta=0.00010 global_step=895\n",
      "Epoch 896:  Train_loss=0.538  Val_loss=0.526  Val_Acc=0.754  eta=0.00010 global_step=896\n",
      "Epoch 897:  Train_loss=0.538  Val_loss=0.526  Val_Acc=0.754  eta=0.00010 global_step=897\n",
      "Epoch 898:  Train_loss=0.538  Val_loss=0.526  Val_Acc=0.754  eta=0.00010 global_step=898\n",
      "Epoch 899:  Train_loss=0.537  Val_loss=0.526  Val_Acc=0.754  eta=0.00010 global_step=899\n",
      "Epoch 900:  Train_loss=0.537  Val_loss=0.526  Val_Acc=0.754  eta=0.00010 global_step=900\n",
      "Epoch 901:  Train_loss=0.537  Val_loss=0.525  Val_Acc=0.754  eta=0.00010 global_step=901\n",
      "Epoch 902:  Train_loss=0.537  Val_loss=0.525  Val_Acc=0.754  eta=0.00010 global_step=902\n",
      "Epoch 903:  Train_loss=0.537  Val_loss=0.525  Val_Acc=0.754  eta=0.00010 global_step=903\n",
      "Epoch 904:  Train_loss=0.536  Val_loss=0.525  Val_Acc=0.754  eta=0.00010 global_step=904\n",
      "Epoch 905:  Train_loss=0.536  Val_loss=0.524  Val_Acc=0.754  eta=0.00010 global_step=905\n",
      "Epoch 906:  Train_loss=0.536  Val_loss=0.524  Val_Acc=0.754  eta=0.00010 global_step=906\n",
      "Epoch 907:  Train_loss=0.536  Val_loss=0.524  Val_Acc=0.754  eta=0.00010 global_step=907\n",
      "Epoch 908:  Train_loss=0.536  Val_loss=0.524  Val_Acc=0.761  eta=0.00010 global_step=908\n",
      "Epoch 909:  Train_loss=0.536  Val_loss=0.524  Val_Acc=0.761  eta=0.00010 global_step=909\n",
      "Epoch 910:  Train_loss=0.535  Val_loss=0.523  Val_Acc=0.761  eta=0.00010 global_step=910\n",
      "Epoch 911:  Train_loss=0.535  Val_loss=0.523  Val_Acc=0.761  eta=0.00010 global_step=911\n",
      "Epoch 912:  Train_loss=0.535  Val_loss=0.523  Val_Acc=0.761  eta=0.00010 global_step=912\n",
      "Epoch 913:  Train_loss=0.535  Val_loss=0.523  Val_Acc=0.769  eta=0.00010 global_step=913\n",
      "Epoch 914:  Train_loss=0.535  Val_loss=0.523  Val_Acc=0.769  eta=0.00010 global_step=914\n",
      "Epoch 915:  Train_loss=0.534  Val_loss=0.522  Val_Acc=0.769  eta=0.00010 global_step=915\n",
      "Epoch 916:  Train_loss=0.534  Val_loss=0.522  Val_Acc=0.769  eta=0.00010 global_step=916\n",
      "Epoch 917:  Train_loss=0.534  Val_loss=0.522  Val_Acc=0.769  eta=0.00010 global_step=917\n",
      "Epoch 918:  Train_loss=0.534  Val_loss=0.522  Val_Acc=0.769  eta=0.00010 global_step=918\n",
      "Epoch 919:  Train_loss=0.534  Val_loss=0.522  Val_Acc=0.769  eta=0.00010 global_step=919\n",
      "Epoch 920:  Train_loss=0.533  Val_loss=0.522  Val_Acc=0.769  eta=0.00010 global_step=920\n",
      "Epoch 921:  Train_loss=0.533  Val_loss=0.521  Val_Acc=0.769  eta=0.00010 global_step=921\n",
      "Epoch 922:  Train_loss=0.533  Val_loss=0.521  Val_Acc=0.769  eta=0.00010 global_step=922\n",
      "Epoch 923:  Train_loss=0.533  Val_loss=0.521  Val_Acc=0.769  eta=0.00010 global_step=923\n",
      "Epoch 924:  Train_loss=0.533  Val_loss=0.521  Val_Acc=0.769  eta=0.00010 global_step=924\n",
      "Epoch 925:  Train_loss=0.533  Val_loss=0.521  Val_Acc=0.769  eta=0.00010 global_step=925\n",
      "Epoch 926:  Train_loss=0.532  Val_loss=0.520  Val_Acc=0.769  eta=0.00010 global_step=926\n",
      "Epoch 927:  Train_loss=0.532  Val_loss=0.520  Val_Acc=0.769  eta=0.00010 global_step=927\n",
      "Epoch 928:  Train_loss=0.532  Val_loss=0.520  Val_Acc=0.769  eta=0.00010 global_step=928\n",
      "Epoch 929:  Train_loss=0.532  Val_loss=0.520  Val_Acc=0.769  eta=0.00010 global_step=929\n",
      "Epoch 930:  Train_loss=0.532  Val_loss=0.520  Val_Acc=0.769  eta=0.00010 global_step=930\n",
      "Epoch 931:  Train_loss=0.531  Val_loss=0.519  Val_Acc=0.769  eta=0.00010 global_step=931\n",
      "Epoch 932:  Train_loss=0.531  Val_loss=0.519  Val_Acc=0.769  eta=0.00010 global_step=932\n",
      "Epoch 933:  Train_loss=0.531  Val_loss=0.519  Val_Acc=0.769  eta=0.00010 global_step=933\n",
      "Epoch 934:  Train_loss=0.531  Val_loss=0.519  Val_Acc=0.769  eta=0.00010 global_step=934\n",
      "Epoch 935:  Train_loss=0.531  Val_loss=0.519  Val_Acc=0.769  eta=0.00010 global_step=935\n",
      "Epoch 936:  Train_loss=0.531  Val_loss=0.518  Val_Acc=0.769  eta=0.00010 global_step=936\n",
      "Epoch 937:  Train_loss=0.530  Val_loss=0.518  Val_Acc=0.769  eta=0.00010 global_step=937\n",
      "Epoch 938:  Train_loss=0.530  Val_loss=0.518  Val_Acc=0.769  eta=0.00010 global_step=938\n",
      "Epoch 939:  Train_loss=0.530  Val_loss=0.518  Val_Acc=0.769  eta=0.00010 global_step=939\n",
      "Epoch 940:  Train_loss=0.530  Val_loss=0.518  Val_Acc=0.769  eta=0.00010 global_step=940\n",
      "Epoch 941:  Train_loss=0.530  Val_loss=0.517  Val_Acc=0.769  eta=0.00010 global_step=941\n",
      "Epoch 942:  Train_loss=0.529  Val_loss=0.517  Val_Acc=0.769  eta=0.00010 global_step=942\n",
      "Epoch 943:  Train_loss=0.529  Val_loss=0.517  Val_Acc=0.769  eta=0.00010 global_step=943\n",
      "Epoch 944:  Train_loss=0.529  Val_loss=0.517  Val_Acc=0.769  eta=0.00010 global_step=944\n",
      "Epoch 945:  Train_loss=0.529  Val_loss=0.517  Val_Acc=0.769  eta=0.00010 global_step=945\n",
      "Epoch 946:  Train_loss=0.529  Val_loss=0.516  Val_Acc=0.769  eta=0.00010 global_step=946\n",
      "Epoch 947:  Train_loss=0.529  Val_loss=0.516  Val_Acc=0.769  eta=0.00010 global_step=947\n",
      "Epoch 948:  Train_loss=0.528  Val_loss=0.516  Val_Acc=0.769  eta=0.00010 global_step=948\n",
      "Epoch 949:  Train_loss=0.528  Val_loss=0.516  Val_Acc=0.769  eta=0.00010 global_step=949\n",
      "Epoch 950:  Train_loss=0.528  Val_loss=0.516  Val_Acc=0.769  eta=0.00010 global_step=950\n",
      "Epoch 951:  Train_loss=0.528  Val_loss=0.516  Val_Acc=0.769  eta=0.00010 global_step=951\n",
      "Epoch 952:  Train_loss=0.528  Val_loss=0.515  Val_Acc=0.769  eta=0.00010 global_step=952\n",
      "Epoch 953:  Train_loss=0.528  Val_loss=0.515  Val_Acc=0.769  eta=0.00010 global_step=953\n",
      "Epoch 954:  Train_loss=0.527  Val_loss=0.515  Val_Acc=0.769  eta=0.00010 global_step=954\n",
      "Epoch 955:  Train_loss=0.527  Val_loss=0.515  Val_Acc=0.769  eta=0.00010 global_step=955\n",
      "Epoch 956:  Train_loss=0.527  Val_loss=0.515  Val_Acc=0.776  eta=0.00010 global_step=956\n",
      "Epoch 957:  Train_loss=0.527  Val_loss=0.514  Val_Acc=0.784  eta=0.00010 global_step=957\n",
      "Epoch 958:  Train_loss=0.527  Val_loss=0.514  Val_Acc=0.784  eta=0.00010 global_step=958\n",
      "Epoch 959:  Train_loss=0.526  Val_loss=0.514  Val_Acc=0.784  eta=0.00010 global_step=959\n",
      "Epoch 960:  Train_loss=0.526  Val_loss=0.514  Val_Acc=0.784  eta=0.00010 global_step=960\n",
      "Epoch 961:  Train_loss=0.526  Val_loss=0.514  Val_Acc=0.784  eta=0.00010 global_step=961\n",
      "Epoch 962:  Train_loss=0.526  Val_loss=0.513  Val_Acc=0.784  eta=0.00010 global_step=962\n",
      "Epoch 963:  Train_loss=0.526  Val_loss=0.513  Val_Acc=0.784  eta=0.00010 global_step=963\n",
      "Epoch 964:  Train_loss=0.526  Val_loss=0.513  Val_Acc=0.784  eta=0.00010 global_step=964\n",
      "Epoch 965:  Train_loss=0.525  Val_loss=0.513  Val_Acc=0.784  eta=0.00010 global_step=965\n",
      "Epoch 966:  Train_loss=0.525  Val_loss=0.513  Val_Acc=0.784  eta=0.00010 global_step=966\n",
      "Epoch 967:  Train_loss=0.525  Val_loss=0.513  Val_Acc=0.784  eta=0.00010 global_step=967\n",
      "Epoch 968:  Train_loss=0.525  Val_loss=0.512  Val_Acc=0.784  eta=0.00010 global_step=968\n",
      "Epoch 969:  Train_loss=0.525  Val_loss=0.512  Val_Acc=0.784  eta=0.00010 global_step=969\n",
      "Epoch 970:  Train_loss=0.525  Val_loss=0.512  Val_Acc=0.784  eta=0.00010 global_step=970\n",
      "Epoch 971:  Train_loss=0.524  Val_loss=0.512  Val_Acc=0.784  eta=0.00010 global_step=971\n",
      "Epoch 972:  Train_loss=0.524  Val_loss=0.512  Val_Acc=0.784  eta=0.00010 global_step=972\n",
      "Epoch 973:  Train_loss=0.524  Val_loss=0.511  Val_Acc=0.784  eta=0.00010 global_step=973\n",
      "Epoch 974:  Train_loss=0.524  Val_loss=0.511  Val_Acc=0.776  eta=0.00010 global_step=974\n",
      "Epoch 975:  Train_loss=0.524  Val_loss=0.511  Val_Acc=0.776  eta=0.00010 global_step=975\n",
      "Epoch 976:  Train_loss=0.524  Val_loss=0.511  Val_Acc=0.776  eta=0.00010 global_step=976\n",
      "Epoch 977:  Train_loss=0.523  Val_loss=0.511  Val_Acc=0.776  eta=0.00010 global_step=977\n",
      "Epoch 978:  Train_loss=0.523  Val_loss=0.510  Val_Acc=0.776  eta=0.00010 global_step=978\n",
      "Epoch 979:  Train_loss=0.523  Val_loss=0.510  Val_Acc=0.776  eta=0.00010 global_step=979\n",
      "Epoch 980:  Train_loss=0.523  Val_loss=0.510  Val_Acc=0.776  eta=0.00010 global_step=980\n",
      "Epoch 981:  Train_loss=0.523  Val_loss=0.510  Val_Acc=0.776  eta=0.00010 global_step=981\n",
      "Epoch 982:  Train_loss=0.523  Val_loss=0.510  Val_Acc=0.776  eta=0.00010 global_step=982\n",
      "Epoch 983:  Train_loss=0.522  Val_loss=0.510  Val_Acc=0.776  eta=0.00010 global_step=983\n",
      "Epoch 984:  Train_loss=0.522  Val_loss=0.509  Val_Acc=0.776  eta=0.00010 global_step=984\n",
      "Epoch 985:  Train_loss=0.522  Val_loss=0.509  Val_Acc=0.776  eta=0.00010 global_step=985\n",
      "Epoch 986:  Train_loss=0.522  Val_loss=0.509  Val_Acc=0.776  eta=0.00010 global_step=986\n",
      "Epoch 987:  Train_loss=0.522  Val_loss=0.509  Val_Acc=0.776  eta=0.00010 global_step=987\n",
      "Epoch 988:  Train_loss=0.521  Val_loss=0.509  Val_Acc=0.776  eta=0.00010 global_step=988\n",
      "Epoch 989:  Train_loss=0.521  Val_loss=0.509  Val_Acc=0.776  eta=0.00010 global_step=989\n",
      "Epoch 990:  Train_loss=0.521  Val_loss=0.508  Val_Acc=0.776  eta=0.00010 global_step=990\n",
      "Epoch 991:  Train_loss=0.521  Val_loss=0.508  Val_Acc=0.776  eta=0.00010 global_step=991\n",
      "Epoch 992:  Train_loss=0.521  Val_loss=0.508  Val_Acc=0.776  eta=0.00010 global_step=992\n",
      "Epoch 993:  Train_loss=0.521  Val_loss=0.508  Val_Acc=0.776  eta=0.00010 global_step=993\n",
      "Epoch 994:  Train_loss=0.520  Val_loss=0.508  Val_Acc=0.776  eta=0.00010 global_step=994\n",
      "Epoch 995:  Train_loss=0.520  Val_loss=0.507  Val_Acc=0.776  eta=0.00010 global_step=995\n",
      "Epoch 996:  Train_loss=0.520  Val_loss=0.507  Val_Acc=0.776  eta=0.00010 global_step=996\n",
      "Epoch 997:  Train_loss=0.520  Val_loss=0.507  Val_Acc=0.776  eta=0.00010 global_step=997\n",
      "Epoch 998:  Train_loss=0.520  Val_loss=0.507  Val_Acc=0.784  eta=0.00010 global_step=998\n",
      "Epoch 999:  Train_loss=0.520  Val_loss=0.507  Val_Acc=0.784  eta=0.00010 global_step=999\n",
      "Epoch 1000:  Train_loss=0.519  Val_loss=0.507  Val_Acc=0.784  eta=0.00010 global_step=1000\n",
      "Epoch 1001:  Train_loss=0.519  Val_loss=0.506  Val_Acc=0.784  eta=0.00010 global_step=1001\n",
      "Epoch 1002:  Train_loss=0.519  Val_loss=0.506  Val_Acc=0.784  eta=0.00010 global_step=1002\n",
      "Epoch 1003:  Train_loss=0.519  Val_loss=0.506  Val_Acc=0.784  eta=0.00010 global_step=1003\n",
      "Epoch 1004:  Train_loss=0.519  Val_loss=0.506  Val_Acc=0.784  eta=0.00010 global_step=1004\n",
      "Epoch 1005:  Train_loss=0.519  Val_loss=0.506  Val_Acc=0.791  eta=0.00010 global_step=1005\n",
      "Epoch 1006:  Train_loss=0.518  Val_loss=0.505  Val_Acc=0.791  eta=0.00010 global_step=1006\n",
      "Epoch 1007:  Train_loss=0.518  Val_loss=0.505  Val_Acc=0.791  eta=0.00010 global_step=1007\n",
      "Epoch 1008:  Train_loss=0.518  Val_loss=0.505  Val_Acc=0.791  eta=0.00010 global_step=1008\n",
      "Epoch 1009:  Train_loss=0.518  Val_loss=0.505  Val_Acc=0.791  eta=0.00010 global_step=1009\n",
      "Epoch 1010:  Train_loss=0.518  Val_loss=0.505  Val_Acc=0.791  eta=0.00010 global_step=1010\n",
      "Epoch 1011:  Train_loss=0.518  Val_loss=0.505  Val_Acc=0.791  eta=0.00010 global_step=1011\n",
      "Epoch 1012:  Train_loss=0.517  Val_loss=0.504  Val_Acc=0.791  eta=0.00010 global_step=1012\n",
      "Epoch 1013:  Train_loss=0.517  Val_loss=0.504  Val_Acc=0.791  eta=0.00010 global_step=1013\n",
      "Epoch 1014:  Train_loss=0.517  Val_loss=0.504  Val_Acc=0.791  eta=0.00010 global_step=1014\n",
      "Epoch 1015:  Train_loss=0.517  Val_loss=0.504  Val_Acc=0.791  eta=0.00010 global_step=1015\n",
      "Epoch 1016:  Train_loss=0.517  Val_loss=0.504  Val_Acc=0.791  eta=0.00010 global_step=1016\n",
      "Epoch 1017:  Train_loss=0.517  Val_loss=0.504  Val_Acc=0.791  eta=0.00010 global_step=1017\n",
      "Epoch 1018:  Train_loss=0.517  Val_loss=0.503  Val_Acc=0.791  eta=0.00010 global_step=1018\n",
      "Epoch 1019:  Train_loss=0.516  Val_loss=0.503  Val_Acc=0.791  eta=0.00010 global_step=1019\n",
      "Epoch 1020:  Train_loss=0.516  Val_loss=0.503  Val_Acc=0.791  eta=0.00010 global_step=1020\n",
      "Epoch 1021:  Train_loss=0.516  Val_loss=0.503  Val_Acc=0.791  eta=0.00010 global_step=1021\n",
      "Epoch 1022:  Train_loss=0.516  Val_loss=0.503  Val_Acc=0.791  eta=0.00010 global_step=1022\n",
      "Epoch 1023:  Train_loss=0.516  Val_loss=0.502  Val_Acc=0.791  eta=0.00010 global_step=1023\n",
      "Epoch 1024:  Train_loss=0.516  Val_loss=0.502  Val_Acc=0.791  eta=0.00010 global_step=1024\n",
      "Epoch 1025:  Train_loss=0.515  Val_loss=0.502  Val_Acc=0.791  eta=0.00010 global_step=1025\n",
      "Epoch 1026:  Train_loss=0.515  Val_loss=0.502  Val_Acc=0.791  eta=0.00010 global_step=1026\n",
      "Epoch 1027:  Train_loss=0.515  Val_loss=0.502  Val_Acc=0.791  eta=0.00010 global_step=1027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1028:  Train_loss=0.515  Val_loss=0.502  Val_Acc=0.791  eta=0.00010 global_step=1028\n",
      "Epoch 1029:  Train_loss=0.515  Val_loss=0.501  Val_Acc=0.791  eta=0.00010 global_step=1029\n",
      "Epoch 1030:  Train_loss=0.515  Val_loss=0.501  Val_Acc=0.799  eta=0.00010 global_step=1030\n",
      "Epoch 1031:  Train_loss=0.514  Val_loss=0.501  Val_Acc=0.799  eta=0.00010 global_step=1031\n",
      "Epoch 1032:  Train_loss=0.514  Val_loss=0.501  Val_Acc=0.799  eta=0.00010 global_step=1032\n",
      "Epoch 1033:  Train_loss=0.514  Val_loss=0.501  Val_Acc=0.799  eta=0.00010 global_step=1033\n",
      "Epoch 1034:  Train_loss=0.514  Val_loss=0.501  Val_Acc=0.799  eta=0.00010 global_step=1034\n",
      "Epoch 1035:  Train_loss=0.514  Val_loss=0.500  Val_Acc=0.799  eta=0.00010 global_step=1035\n",
      "Epoch 1036:  Train_loss=0.514  Val_loss=0.500  Val_Acc=0.799  eta=0.00010 global_step=1036\n",
      "Epoch 1037:  Train_loss=0.513  Val_loss=0.500  Val_Acc=0.799  eta=0.00010 global_step=1037\n",
      "Epoch 1038:  Train_loss=0.513  Val_loss=0.500  Val_Acc=0.799  eta=0.00010 global_step=1038\n",
      "Epoch 1039:  Train_loss=0.513  Val_loss=0.500  Val_Acc=0.799  eta=0.00010 global_step=1039\n",
      "Epoch 1040:  Train_loss=0.513  Val_loss=0.500  Val_Acc=0.799  eta=0.00010 global_step=1040\n",
      "Epoch 1041:  Train_loss=0.513  Val_loss=0.499  Val_Acc=0.799  eta=0.00010 global_step=1041\n",
      "Epoch 1042:  Train_loss=0.513  Val_loss=0.499  Val_Acc=0.799  eta=0.00010 global_step=1042\n",
      "Epoch 1043:  Train_loss=0.512  Val_loss=0.499  Val_Acc=0.799  eta=0.00010 global_step=1043\n",
      "Epoch 1044:  Train_loss=0.512  Val_loss=0.499  Val_Acc=0.799  eta=0.00010 global_step=1044\n",
      "Epoch 1045:  Train_loss=0.512  Val_loss=0.499  Val_Acc=0.799  eta=0.00010 global_step=1045\n",
      "Epoch 1046:  Train_loss=0.512  Val_loss=0.499  Val_Acc=0.806  eta=0.00010 global_step=1046\n",
      "Epoch 1047:  Train_loss=0.512  Val_loss=0.498  Val_Acc=0.806  eta=0.00010 global_step=1047\n",
      "Epoch 1048:  Train_loss=0.512  Val_loss=0.498  Val_Acc=0.806  eta=0.00010 global_step=1048\n",
      "Epoch 1049:  Train_loss=0.511  Val_loss=0.498  Val_Acc=0.806  eta=0.00010 global_step=1049\n",
      "Epoch 1050:  Train_loss=0.511  Val_loss=0.498  Val_Acc=0.806  eta=0.00010 global_step=1050\n",
      "Epoch 1051:  Train_loss=0.511  Val_loss=0.498  Val_Acc=0.806  eta=0.00010 global_step=1051\n",
      "Epoch 1052:  Train_loss=0.511  Val_loss=0.497  Val_Acc=0.806  eta=0.00010 global_step=1052\n",
      "Epoch 1053:  Train_loss=0.511  Val_loss=0.497  Val_Acc=0.806  eta=0.00010 global_step=1053\n",
      "Epoch 1054:  Train_loss=0.511  Val_loss=0.497  Val_Acc=0.806  eta=0.00010 global_step=1054\n",
      "Epoch 1055:  Train_loss=0.511  Val_loss=0.497  Val_Acc=0.806  eta=0.00010 global_step=1055\n",
      "Epoch 1056:  Train_loss=0.510  Val_loss=0.497  Val_Acc=0.806  eta=0.00010 global_step=1056\n",
      "Epoch 1057:  Train_loss=0.510  Val_loss=0.497  Val_Acc=0.806  eta=0.00010 global_step=1057\n",
      "Epoch 1058:  Train_loss=0.510  Val_loss=0.496  Val_Acc=0.806  eta=0.00010 global_step=1058\n",
      "Epoch 1059:  Train_loss=0.510  Val_loss=0.496  Val_Acc=0.806  eta=0.00010 global_step=1059\n",
      "Epoch 1060:  Train_loss=0.510  Val_loss=0.496  Val_Acc=0.806  eta=0.00010 global_step=1060\n",
      "Epoch 1061:  Train_loss=0.510  Val_loss=0.496  Val_Acc=0.806  eta=0.00010 global_step=1061\n",
      "Epoch 1062:  Train_loss=0.509  Val_loss=0.496  Val_Acc=0.806  eta=0.00010 global_step=1062\n",
      "Epoch 1063:  Train_loss=0.509  Val_loss=0.496  Val_Acc=0.806  eta=0.00010 global_step=1063\n",
      "Epoch 1064:  Train_loss=0.509  Val_loss=0.495  Val_Acc=0.806  eta=0.00010 global_step=1064\n",
      "Epoch 1065:  Train_loss=0.509  Val_loss=0.495  Val_Acc=0.806  eta=0.00010 global_step=1065\n",
      "Epoch 1066:  Train_loss=0.509  Val_loss=0.495  Val_Acc=0.806  eta=0.00010 global_step=1066\n",
      "Epoch 1067:  Train_loss=0.509  Val_loss=0.495  Val_Acc=0.806  eta=0.00010 global_step=1067\n",
      "Epoch 1068:  Train_loss=0.508  Val_loss=0.495  Val_Acc=0.806  eta=0.00010 global_step=1068\n",
      "Epoch 1069:  Train_loss=0.508  Val_loss=0.495  Val_Acc=0.806  eta=0.00010 global_step=1069\n",
      "Epoch 1070:  Train_loss=0.508  Val_loss=0.494  Val_Acc=0.806  eta=0.00010 global_step=1070\n",
      "Epoch 1071:  Train_loss=0.508  Val_loss=0.494  Val_Acc=0.806  eta=0.00010 global_step=1071\n",
      "Epoch 1072:  Train_loss=0.508  Val_loss=0.494  Val_Acc=0.806  eta=0.00010 global_step=1072\n",
      "Epoch 1073:  Train_loss=0.508  Val_loss=0.494  Val_Acc=0.806  eta=0.00010 global_step=1073\n",
      "Epoch 1074:  Train_loss=0.508  Val_loss=0.494  Val_Acc=0.813  eta=0.00010 global_step=1074\n",
      "Epoch 1075:  Train_loss=0.507  Val_loss=0.494  Val_Acc=0.813  eta=0.00010 global_step=1075\n",
      "Epoch 1076:  Train_loss=0.507  Val_loss=0.493  Val_Acc=0.813  eta=0.00010 global_step=1076\n",
      "Epoch 1077:  Train_loss=0.507  Val_loss=0.493  Val_Acc=0.813  eta=0.00010 global_step=1077\n",
      "Epoch 1078:  Train_loss=0.507  Val_loss=0.493  Val_Acc=0.813  eta=0.00010 global_step=1078\n",
      "Epoch 1079:  Train_loss=0.507  Val_loss=0.493  Val_Acc=0.813  eta=0.00010 global_step=1079\n",
      "Epoch 1080:  Train_loss=0.507  Val_loss=0.493  Val_Acc=0.806  eta=0.00010 global_step=1080\n",
      "Epoch 1081:  Train_loss=0.506  Val_loss=0.493  Val_Acc=0.806  eta=0.00010 global_step=1081\n",
      "Epoch 1082:  Train_loss=0.506  Val_loss=0.492  Val_Acc=0.806  eta=0.00010 global_step=1082\n",
      "Epoch 1083:  Train_loss=0.506  Val_loss=0.492  Val_Acc=0.806  eta=0.00010 global_step=1083\n",
      "Epoch 1084:  Train_loss=0.506  Val_loss=0.492  Val_Acc=0.806  eta=0.00010 global_step=1084\n",
      "Epoch 1085:  Train_loss=0.506  Val_loss=0.492  Val_Acc=0.806  eta=0.00010 global_step=1085\n",
      "Epoch 1086:  Train_loss=0.506  Val_loss=0.492  Val_Acc=0.806  eta=0.00010 global_step=1086\n",
      "Epoch 1087:  Train_loss=0.506  Val_loss=0.492  Val_Acc=0.806  eta=0.00010 global_step=1087\n",
      "Epoch 1088:  Train_loss=0.505  Val_loss=0.492  Val_Acc=0.806  eta=0.00010 global_step=1088\n",
      "Epoch 1089:  Train_loss=0.505  Val_loss=0.491  Val_Acc=0.806  eta=0.00010 global_step=1089\n",
      "Epoch 1090:  Train_loss=0.505  Val_loss=0.491  Val_Acc=0.806  eta=0.00010 global_step=1090\n",
      "Epoch 1091:  Train_loss=0.505  Val_loss=0.491  Val_Acc=0.806  eta=0.00010 global_step=1091\n",
      "Epoch 1092:  Train_loss=0.505  Val_loss=0.491  Val_Acc=0.806  eta=0.00010 global_step=1092\n",
      "Epoch 1093:  Train_loss=0.505  Val_loss=0.491  Val_Acc=0.806  eta=0.00010 global_step=1093\n",
      "Epoch 1094:  Train_loss=0.504  Val_loss=0.491  Val_Acc=0.806  eta=0.00010 global_step=1094\n",
      "Epoch 1095:  Train_loss=0.504  Val_loss=0.490  Val_Acc=0.806  eta=0.00010 global_step=1095\n",
      "Epoch 1096:  Train_loss=0.504  Val_loss=0.490  Val_Acc=0.806  eta=0.00010 global_step=1096\n",
      "Epoch 1097:  Train_loss=0.504  Val_loss=0.490  Val_Acc=0.806  eta=0.00010 global_step=1097\n",
      "Epoch 1098:  Train_loss=0.504  Val_loss=0.490  Val_Acc=0.806  eta=0.00010 global_step=1098\n",
      "Epoch 1099:  Train_loss=0.504  Val_loss=0.490  Val_Acc=0.806  eta=0.00010 global_step=1099\n",
      "Epoch 1100:  Train_loss=0.504  Val_loss=0.490  Val_Acc=0.806  eta=0.00010 global_step=1100\n",
      "Epoch 1101:  Train_loss=0.503  Val_loss=0.489  Val_Acc=0.806  eta=0.00010 global_step=1101\n",
      "Epoch 1102:  Train_loss=0.503  Val_loss=0.489  Val_Acc=0.806  eta=0.00010 global_step=1102\n",
      "Epoch 1103:  Train_loss=0.503  Val_loss=0.489  Val_Acc=0.806  eta=0.00010 global_step=1103\n",
      "Epoch 1104:  Train_loss=0.503  Val_loss=0.489  Val_Acc=0.806  eta=0.00010 global_step=1104\n",
      "Epoch 1105:  Train_loss=0.503  Val_loss=0.489  Val_Acc=0.806  eta=0.00010 global_step=1105\n",
      "Epoch 1106:  Train_loss=0.503  Val_loss=0.489  Val_Acc=0.806  eta=0.00010 global_step=1106\n",
      "Epoch 1107:  Train_loss=0.503  Val_loss=0.488  Val_Acc=0.806  eta=0.00010 global_step=1107\n",
      "Epoch 1108:  Train_loss=0.502  Val_loss=0.488  Val_Acc=0.806  eta=0.00010 global_step=1108\n",
      "Epoch 1109:  Train_loss=0.502  Val_loss=0.488  Val_Acc=0.806  eta=0.00010 global_step=1109\n",
      "Epoch 1110:  Train_loss=0.502  Val_loss=0.488  Val_Acc=0.806  eta=0.00010 global_step=1110\n",
      "Epoch 1111:  Train_loss=0.502  Val_loss=0.488  Val_Acc=0.806  eta=0.00010 global_step=1111\n",
      "Epoch 1112:  Train_loss=0.502  Val_loss=0.488  Val_Acc=0.806  eta=0.00010 global_step=1112\n",
      "Epoch 1113:  Train_loss=0.502  Val_loss=0.487  Val_Acc=0.806  eta=0.00010 global_step=1113\n",
      "Epoch 1114:  Train_loss=0.501  Val_loss=0.487  Val_Acc=0.806  eta=0.00010 global_step=1114\n",
      "Epoch 1115:  Train_loss=0.501  Val_loss=0.487  Val_Acc=0.806  eta=0.00010 global_step=1115\n",
      "Epoch 1116:  Train_loss=0.501  Val_loss=0.487  Val_Acc=0.806  eta=0.00010 global_step=1116\n",
      "Epoch 1117:  Train_loss=0.501  Val_loss=0.487  Val_Acc=0.806  eta=0.00010 global_step=1117\n",
      "Epoch 1118:  Train_loss=0.501  Val_loss=0.487  Val_Acc=0.806  eta=0.00010 global_step=1118\n",
      "Epoch 1119:  Train_loss=0.501  Val_loss=0.487  Val_Acc=0.806  eta=0.00010 global_step=1119\n",
      "Epoch 1120:  Train_loss=0.501  Val_loss=0.486  Val_Acc=0.806  eta=0.00010 global_step=1120\n",
      "Epoch 1121:  Train_loss=0.500  Val_loss=0.486  Val_Acc=0.806  eta=0.00010 global_step=1121\n",
      "Epoch 1122:  Train_loss=0.500  Val_loss=0.486  Val_Acc=0.806  eta=0.00010 global_step=1122\n",
      "Epoch 1123:  Train_loss=0.500  Val_loss=0.486  Val_Acc=0.806  eta=0.00010 global_step=1123\n",
      "Epoch 1124:  Train_loss=0.500  Val_loss=0.486  Val_Acc=0.806  eta=0.00010 global_step=1124\n",
      "Epoch 1125:  Train_loss=0.500  Val_loss=0.486  Val_Acc=0.806  eta=0.00010 global_step=1125\n",
      "Epoch 1126:  Train_loss=0.500  Val_loss=0.485  Val_Acc=0.799  eta=0.00010 global_step=1126\n",
      "Epoch 1127:  Train_loss=0.500  Val_loss=0.485  Val_Acc=0.799  eta=0.00010 global_step=1127\n",
      "Epoch 1128:  Train_loss=0.499  Val_loss=0.485  Val_Acc=0.799  eta=0.00010 global_step=1128\n",
      "Epoch 1129:  Train_loss=0.499  Val_loss=0.485  Val_Acc=0.799  eta=0.00010 global_step=1129\n",
      "Epoch 1130:  Train_loss=0.499  Val_loss=0.485  Val_Acc=0.799  eta=0.00010 global_step=1130\n",
      "Epoch 1131:  Train_loss=0.499  Val_loss=0.485  Val_Acc=0.799  eta=0.00010 global_step=1131\n",
      "Epoch 1132:  Train_loss=0.499  Val_loss=0.484  Val_Acc=0.799  eta=0.00010 global_step=1132\n",
      "Epoch 1133:  Train_loss=0.499  Val_loss=0.484  Val_Acc=0.799  eta=0.00010 global_step=1133\n",
      "Epoch 1134:  Train_loss=0.499  Val_loss=0.484  Val_Acc=0.799  eta=0.00010 global_step=1134\n",
      "Epoch 1135:  Train_loss=0.498  Val_loss=0.484  Val_Acc=0.799  eta=0.00010 global_step=1135\n",
      "Epoch 1136:  Train_loss=0.498  Val_loss=0.484  Val_Acc=0.799  eta=0.00010 global_step=1136\n",
      "Epoch 1137:  Train_loss=0.498  Val_loss=0.484  Val_Acc=0.799  eta=0.00010 global_step=1137\n",
      "Epoch 1138:  Train_loss=0.498  Val_loss=0.484  Val_Acc=0.799  eta=0.00010 global_step=1138\n",
      "Epoch 1139:  Train_loss=0.498  Val_loss=0.483  Val_Acc=0.799  eta=0.00010 global_step=1139\n",
      "Epoch 1140:  Train_loss=0.498  Val_loss=0.483  Val_Acc=0.799  eta=0.00010 global_step=1140\n",
      "Epoch 1141:  Train_loss=0.498  Val_loss=0.483  Val_Acc=0.799  eta=0.00010 global_step=1141\n",
      "Epoch 1142:  Train_loss=0.497  Val_loss=0.483  Val_Acc=0.799  eta=0.00010 global_step=1142\n",
      "Epoch 1143:  Train_loss=0.497  Val_loss=0.483  Val_Acc=0.799  eta=0.00010 global_step=1143\n",
      "Epoch 1144:  Train_loss=0.497  Val_loss=0.483  Val_Acc=0.799  eta=0.00010 global_step=1144\n",
      "Epoch 1145:  Train_loss=0.497  Val_loss=0.482  Val_Acc=0.799  eta=0.00010 global_step=1145\n",
      "Epoch 1146:  Train_loss=0.497  Val_loss=0.482  Val_Acc=0.799  eta=0.00010 global_step=1146\n",
      "Epoch 1147:  Train_loss=0.497  Val_loss=0.482  Val_Acc=0.821  eta=0.00010 global_step=1147\n",
      "Epoch 1148:  Train_loss=0.496  Val_loss=0.482  Val_Acc=0.821  eta=0.00010 global_step=1148\n",
      "Epoch 1149:  Train_loss=0.496  Val_loss=0.482  Val_Acc=0.821  eta=0.00010 global_step=1149\n",
      "Epoch 1150:  Train_loss=0.496  Val_loss=0.482  Val_Acc=0.821  eta=0.00010 global_step=1150\n",
      "Epoch 1151:  Train_loss=0.496  Val_loss=0.482  Val_Acc=0.821  eta=0.00010 global_step=1151\n",
      "Epoch 1152:  Train_loss=0.496  Val_loss=0.481  Val_Acc=0.821  eta=0.00010 global_step=1152\n",
      "Epoch 1153:  Train_loss=0.496  Val_loss=0.481  Val_Acc=0.821  eta=0.00010 global_step=1153\n",
      "Epoch 1154:  Train_loss=0.496  Val_loss=0.481  Val_Acc=0.821  eta=0.00010 global_step=1154\n",
      "Epoch 1155:  Train_loss=0.495  Val_loss=0.481  Val_Acc=0.821  eta=0.00010 global_step=1155\n",
      "Epoch 1156:  Train_loss=0.495  Val_loss=0.481  Val_Acc=0.821  eta=0.00010 global_step=1156\n",
      "Epoch 1157:  Train_loss=0.495  Val_loss=0.481  Val_Acc=0.821  eta=0.00010 global_step=1157\n",
      "Epoch 1158:  Train_loss=0.495  Val_loss=0.480  Val_Acc=0.821  eta=0.00010 global_step=1158\n",
      "Epoch 1159:  Train_loss=0.495  Val_loss=0.480  Val_Acc=0.821  eta=0.00010 global_step=1159\n",
      "Epoch 1160:  Train_loss=0.495  Val_loss=0.480  Val_Acc=0.821  eta=0.00010 global_step=1160\n",
      "Epoch 1161:  Train_loss=0.495  Val_loss=0.480  Val_Acc=0.821  eta=0.00010 global_step=1161\n",
      "Epoch 1162:  Train_loss=0.495  Val_loss=0.480  Val_Acc=0.821  eta=0.00010 global_step=1162\n",
      "Epoch 1163:  Train_loss=0.494  Val_loss=0.480  Val_Acc=0.821  eta=0.00010 global_step=1163\n",
      "Epoch 1164:  Train_loss=0.494  Val_loss=0.480  Val_Acc=0.821  eta=0.00010 global_step=1164\n",
      "Epoch 1165:  Train_loss=0.494  Val_loss=0.479  Val_Acc=0.821  eta=0.00010 global_step=1165\n",
      "Epoch 1166:  Train_loss=0.494  Val_loss=0.479  Val_Acc=0.821  eta=0.00010 global_step=1166\n",
      "Epoch 1167:  Train_loss=0.494  Val_loss=0.479  Val_Acc=0.821  eta=0.00010 global_step=1167\n",
      "Epoch 1168:  Train_loss=0.494  Val_loss=0.479  Val_Acc=0.821  eta=0.00010 global_step=1168\n",
      "Epoch 1169:  Train_loss=0.494  Val_loss=0.479  Val_Acc=0.821  eta=0.00010 global_step=1169\n",
      "Epoch 1170:  Train_loss=0.493  Val_loss=0.479  Val_Acc=0.821  eta=0.00010 global_step=1170\n",
      "Epoch 1171:  Train_loss=0.493  Val_loss=0.478  Val_Acc=0.821  eta=0.00010 global_step=1171\n",
      "Epoch 1172:  Train_loss=0.493  Val_loss=0.478  Val_Acc=0.821  eta=0.00010 global_step=1172\n",
      "Epoch 1173:  Train_loss=0.493  Val_loss=0.478  Val_Acc=0.821  eta=0.00010 global_step=1173\n",
      "Epoch 1174:  Train_loss=0.493  Val_loss=0.478  Val_Acc=0.821  eta=0.00010 global_step=1174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1175:  Train_loss=0.493  Val_loss=0.478  Val_Acc=0.821  eta=0.00010 global_step=1175\n",
      "Epoch 1176:  Train_loss=0.493  Val_loss=0.478  Val_Acc=0.821  eta=0.00010 global_step=1176\n",
      "Epoch 1177:  Train_loss=0.492  Val_loss=0.478  Val_Acc=0.821  eta=0.00010 global_step=1177\n",
      "Epoch 1178:  Train_loss=0.492  Val_loss=0.477  Val_Acc=0.821  eta=0.00010 global_step=1178\n",
      "Epoch 1179:  Train_loss=0.492  Val_loss=0.477  Val_Acc=0.821  eta=0.00010 global_step=1179\n",
      "Epoch 1180:  Train_loss=0.492  Val_loss=0.477  Val_Acc=0.821  eta=0.00010 global_step=1180\n",
      "Epoch 1181:  Train_loss=0.492  Val_loss=0.477  Val_Acc=0.821  eta=0.00010 global_step=1181\n",
      "Epoch 1182:  Train_loss=0.492  Val_loss=0.477  Val_Acc=0.821  eta=0.00010 global_step=1182\n",
      "Epoch 1183:  Train_loss=0.492  Val_loss=0.477  Val_Acc=0.821  eta=0.00010 global_step=1183\n",
      "Epoch 1184:  Train_loss=0.491  Val_loss=0.477  Val_Acc=0.821  eta=0.00010 global_step=1184\n",
      "Epoch 1185:  Train_loss=0.491  Val_loss=0.476  Val_Acc=0.821  eta=0.00010 global_step=1185\n",
      "Epoch 1186:  Train_loss=0.491  Val_loss=0.476  Val_Acc=0.821  eta=0.00010 global_step=1186\n",
      "Epoch 1187:  Train_loss=0.491  Val_loss=0.476  Val_Acc=0.821  eta=0.00010 global_step=1187\n",
      "Epoch 1188:  Train_loss=0.491  Val_loss=0.476  Val_Acc=0.821  eta=0.00010 global_step=1188\n",
      "Epoch 1189:  Train_loss=0.491  Val_loss=0.476  Val_Acc=0.821  eta=0.00010 global_step=1189\n",
      "Epoch 1190:  Train_loss=0.491  Val_loss=0.476  Val_Acc=0.821  eta=0.00010 global_step=1190\n",
      "Epoch 1191:  Train_loss=0.490  Val_loss=0.476  Val_Acc=0.821  eta=0.00010 global_step=1191\n",
      "Epoch 1192:  Train_loss=0.490  Val_loss=0.475  Val_Acc=0.821  eta=0.00010 global_step=1192\n",
      "Epoch 1193:  Train_loss=0.490  Val_loss=0.475  Val_Acc=0.821  eta=0.00010 global_step=1193\n",
      "Epoch 1194:  Train_loss=0.490  Val_loss=0.475  Val_Acc=0.821  eta=0.00010 global_step=1194\n",
      "Epoch 1195:  Train_loss=0.490  Val_loss=0.475  Val_Acc=0.821  eta=0.00010 global_step=1195\n",
      "Epoch 1196:  Train_loss=0.490  Val_loss=0.475  Val_Acc=0.821  eta=0.00010 global_step=1196\n",
      "Epoch 1197:  Train_loss=0.490  Val_loss=0.475  Val_Acc=0.821  eta=0.00010 global_step=1197\n",
      "Epoch 1198:  Train_loss=0.490  Val_loss=0.475  Val_Acc=0.821  eta=0.00010 global_step=1198\n",
      "Epoch 1199:  Train_loss=0.489  Val_loss=0.474  Val_Acc=0.821  eta=0.00010 global_step=1199\n",
      "Epoch 1200:  Train_loss=0.489  Val_loss=0.474  Val_Acc=0.821  eta=0.00010 global_step=1200\n",
      "Epoch 1201:  Train_loss=0.489  Val_loss=0.474  Val_Acc=0.821  eta=0.00010 global_step=1201\n",
      "Epoch 1202:  Train_loss=0.489  Val_loss=0.474  Val_Acc=0.821  eta=0.00010 global_step=1202\n",
      "Epoch 1203:  Train_loss=0.489  Val_loss=0.474  Val_Acc=0.821  eta=0.00010 global_step=1203\n",
      "Epoch 1204:  Train_loss=0.489  Val_loss=0.474  Val_Acc=0.821  eta=0.00010 global_step=1204\n",
      "Epoch 1205:  Train_loss=0.489  Val_loss=0.474  Val_Acc=0.821  eta=0.00010 global_step=1205\n",
      "Epoch 1206:  Train_loss=0.488  Val_loss=0.473  Val_Acc=0.821  eta=0.00010 global_step=1206\n",
      "Epoch 1207:  Train_loss=0.488  Val_loss=0.473  Val_Acc=0.821  eta=0.00010 global_step=1207\n",
      "Epoch 1208:  Train_loss=0.488  Val_loss=0.473  Val_Acc=0.821  eta=0.00010 global_step=1208\n",
      "Epoch 1209:  Train_loss=0.488  Val_loss=0.473  Val_Acc=0.821  eta=0.00010 global_step=1209\n",
      "Epoch 1210:  Train_loss=0.488  Val_loss=0.473  Val_Acc=0.821  eta=0.00010 global_step=1210\n",
      "Epoch 1211:  Train_loss=0.488  Val_loss=0.473  Val_Acc=0.821  eta=0.00010 global_step=1211\n",
      "Epoch 1212:  Train_loss=0.488  Val_loss=0.473  Val_Acc=0.821  eta=0.00010 global_step=1212\n",
      "Epoch 1213:  Train_loss=0.488  Val_loss=0.472  Val_Acc=0.821  eta=0.00010 global_step=1213\n",
      "Epoch 1214:  Train_loss=0.487  Val_loss=0.472  Val_Acc=0.821  eta=0.00010 global_step=1214\n",
      "Epoch 1215:  Train_loss=0.487  Val_loss=0.472  Val_Acc=0.821  eta=0.00010 global_step=1215\n",
      "Epoch 1216:  Train_loss=0.487  Val_loss=0.472  Val_Acc=0.821  eta=0.00010 global_step=1216\n",
      "Epoch 1217:  Train_loss=0.487  Val_loss=0.472  Val_Acc=0.821  eta=0.00010 global_step=1217\n",
      "Epoch 1218:  Train_loss=0.487  Val_loss=0.472  Val_Acc=0.821  eta=0.00010 global_step=1218\n",
      "Epoch 1219:  Train_loss=0.487  Val_loss=0.472  Val_Acc=0.821  eta=0.00010 global_step=1219\n",
      "Epoch 1220:  Train_loss=0.487  Val_loss=0.471  Val_Acc=0.821  eta=0.00010 global_step=1220\n",
      "Epoch 1221:  Train_loss=0.486  Val_loss=0.471  Val_Acc=0.821  eta=0.00010 global_step=1221\n",
      "Epoch 1222:  Train_loss=0.486  Val_loss=0.471  Val_Acc=0.821  eta=0.00010 global_step=1222\n",
      "Epoch 1223:  Train_loss=0.486  Val_loss=0.471  Val_Acc=0.821  eta=0.00010 global_step=1223\n",
      "Epoch 1224:  Train_loss=0.486  Val_loss=0.471  Val_Acc=0.821  eta=0.00010 global_step=1224\n",
      "Epoch 1225:  Train_loss=0.486  Val_loss=0.471  Val_Acc=0.821  eta=0.00010 global_step=1225\n",
      "Epoch 1226:  Train_loss=0.486  Val_loss=0.471  Val_Acc=0.821  eta=0.00010 global_step=1226\n",
      "Epoch 1227:  Train_loss=0.486  Val_loss=0.470  Val_Acc=0.821  eta=0.00010 global_step=1227\n",
      "Epoch 1228:  Train_loss=0.486  Val_loss=0.470  Val_Acc=0.821  eta=0.00010 global_step=1228\n",
      "Epoch 1229:  Train_loss=0.485  Val_loss=0.470  Val_Acc=0.821  eta=0.00010 global_step=1229\n",
      "Epoch 1230:  Train_loss=0.485  Val_loss=0.470  Val_Acc=0.821  eta=0.00010 global_step=1230\n",
      "Epoch 1231:  Train_loss=0.485  Val_loss=0.470  Val_Acc=0.821  eta=0.00010 global_step=1231\n",
      "Epoch 1232:  Train_loss=0.485  Val_loss=0.470  Val_Acc=0.821  eta=0.00010 global_step=1232\n",
      "Epoch 1233:  Train_loss=0.485  Val_loss=0.470  Val_Acc=0.821  eta=0.00010 global_step=1233\n",
      "Epoch 1234:  Train_loss=0.485  Val_loss=0.470  Val_Acc=0.821  eta=0.00010 global_step=1234\n",
      "Epoch 1235:  Train_loss=0.485  Val_loss=0.469  Val_Acc=0.821  eta=0.00010 global_step=1235\n",
      "Epoch 1236:  Train_loss=0.485  Val_loss=0.469  Val_Acc=0.821  eta=0.00010 global_step=1236\n",
      "Epoch 1237:  Train_loss=0.484  Val_loss=0.469  Val_Acc=0.821  eta=0.00010 global_step=1237\n",
      "Epoch 1238:  Train_loss=0.484  Val_loss=0.469  Val_Acc=0.821  eta=0.00010 global_step=1238\n",
      "Epoch 1239:  Train_loss=0.484  Val_loss=0.469  Val_Acc=0.821  eta=0.00010 global_step=1239\n",
      "Epoch 1240:  Train_loss=0.484  Val_loss=0.469  Val_Acc=0.821  eta=0.00010 global_step=1240\n",
      "Epoch 1241:  Train_loss=0.484  Val_loss=0.469  Val_Acc=0.821  eta=0.00010 global_step=1241\n",
      "Epoch 1242:  Train_loss=0.484  Val_loss=0.468  Val_Acc=0.821  eta=0.00010 global_step=1242\n",
      "Epoch 1243:  Train_loss=0.484  Val_loss=0.468  Val_Acc=0.821  eta=0.00010 global_step=1243\n",
      "Epoch 1244:  Train_loss=0.483  Val_loss=0.468  Val_Acc=0.821  eta=0.00010 global_step=1244\n",
      "Epoch 1245:  Train_loss=0.483  Val_loss=0.468  Val_Acc=0.821  eta=0.00010 global_step=1245\n",
      "Epoch 1246:  Train_loss=0.483  Val_loss=0.468  Val_Acc=0.821  eta=0.00010 global_step=1246\n",
      "Epoch 1247:  Train_loss=0.483  Val_loss=0.468  Val_Acc=0.821  eta=0.00010 global_step=1247\n",
      "Epoch 1248:  Train_loss=0.483  Val_loss=0.468  Val_Acc=0.821  eta=0.00010 global_step=1248\n",
      "Epoch 1249:  Train_loss=0.483  Val_loss=0.467  Val_Acc=0.821  eta=0.00010 global_step=1249\n",
      "Epoch 1250:  Train_loss=0.483  Val_loss=0.467  Val_Acc=0.821  eta=0.00010 global_step=1250\n",
      "Epoch 1251:  Train_loss=0.483  Val_loss=0.467  Val_Acc=0.821  eta=0.00010 global_step=1251\n",
      "Epoch 1252:  Train_loss=0.482  Val_loss=0.467  Val_Acc=0.821  eta=0.00010 global_step=1252\n",
      "Epoch 1253:  Train_loss=0.482  Val_loss=0.467  Val_Acc=0.821  eta=0.00010 global_step=1253\n",
      "Epoch 1254:  Train_loss=0.482  Val_loss=0.467  Val_Acc=0.821  eta=0.00010 global_step=1254\n",
      "Epoch 1255:  Train_loss=0.482  Val_loss=0.467  Val_Acc=0.821  eta=0.00010 global_step=1255\n",
      "Epoch 1256:  Train_loss=0.482  Val_loss=0.467  Val_Acc=0.821  eta=0.00010 global_step=1256\n",
      "Epoch 1257:  Train_loss=0.482  Val_loss=0.466  Val_Acc=0.821  eta=0.00010 global_step=1257\n",
      "Epoch 1258:  Train_loss=0.482  Val_loss=0.466  Val_Acc=0.821  eta=0.00010 global_step=1258\n",
      "Epoch 1259:  Train_loss=0.482  Val_loss=0.466  Val_Acc=0.821  eta=0.00010 global_step=1259\n",
      "Epoch 1260:  Train_loss=0.481  Val_loss=0.466  Val_Acc=0.821  eta=0.00010 global_step=1260\n",
      "Epoch 1261:  Train_loss=0.481  Val_loss=0.466  Val_Acc=0.821  eta=0.00010 global_step=1261\n",
      "Epoch 1262:  Train_loss=0.481  Val_loss=0.466  Val_Acc=0.821  eta=0.00010 global_step=1262\n",
      "Epoch 1263:  Train_loss=0.481  Val_loss=0.466  Val_Acc=0.821  eta=0.00010 global_step=1263\n",
      "Epoch 1264:  Train_loss=0.481  Val_loss=0.465  Val_Acc=0.821  eta=0.00010 global_step=1264\n",
      "Epoch 1265:  Train_loss=0.481  Val_loss=0.465  Val_Acc=0.821  eta=0.00010 global_step=1265\n",
      "Epoch 1266:  Train_loss=0.481  Val_loss=0.465  Val_Acc=0.821  eta=0.00010 global_step=1266\n",
      "Epoch 1267:  Train_loss=0.481  Val_loss=0.465  Val_Acc=0.821  eta=0.00010 global_step=1267\n",
      "Epoch 1268:  Train_loss=0.480  Val_loss=0.465  Val_Acc=0.821  eta=0.00010 global_step=1268\n",
      "Epoch 1269:  Train_loss=0.480  Val_loss=0.465  Val_Acc=0.821  eta=0.00010 global_step=1269\n",
      "Epoch 1270:  Train_loss=0.480  Val_loss=0.465  Val_Acc=0.821  eta=0.00010 global_step=1270\n",
      "Epoch 1271:  Train_loss=0.480  Val_loss=0.464  Val_Acc=0.821  eta=0.00010 global_step=1271\n",
      "Epoch 1272:  Train_loss=0.480  Val_loss=0.464  Val_Acc=0.821  eta=0.00010 global_step=1272\n",
      "Epoch 1273:  Train_loss=0.480  Val_loss=0.464  Val_Acc=0.821  eta=0.00010 global_step=1273\n",
      "Epoch 1274:  Train_loss=0.480  Val_loss=0.464  Val_Acc=0.821  eta=0.00010 global_step=1274\n",
      "Epoch 1275:  Train_loss=0.480  Val_loss=0.464  Val_Acc=0.821  eta=0.00010 global_step=1275\n",
      "Epoch 1276:  Train_loss=0.479  Val_loss=0.464  Val_Acc=0.821  eta=0.00010 global_step=1276\n",
      "Epoch 1277:  Train_loss=0.479  Val_loss=0.464  Val_Acc=0.821  eta=0.00010 global_step=1277\n",
      "Epoch 1278:  Train_loss=0.479  Val_loss=0.464  Val_Acc=0.821  eta=0.00010 global_step=1278\n",
      "Epoch 1279:  Train_loss=0.479  Val_loss=0.463  Val_Acc=0.821  eta=0.00010 global_step=1279\n",
      "Epoch 1280:  Train_loss=0.479  Val_loss=0.463  Val_Acc=0.821  eta=0.00010 global_step=1280\n",
      "Epoch 1281:  Train_loss=0.479  Val_loss=0.463  Val_Acc=0.821  eta=0.00010 global_step=1281\n",
      "Epoch 1282:  Train_loss=0.479  Val_loss=0.463  Val_Acc=0.821  eta=0.00010 global_step=1282\n",
      "Epoch 1283:  Train_loss=0.479  Val_loss=0.463  Val_Acc=0.821  eta=0.00010 global_step=1283\n",
      "Epoch 1284:  Train_loss=0.478  Val_loss=0.463  Val_Acc=0.821  eta=0.00010 global_step=1284\n",
      "Epoch 1285:  Train_loss=0.478  Val_loss=0.463  Val_Acc=0.821  eta=0.00010 global_step=1285\n",
      "Epoch 1286:  Train_loss=0.478  Val_loss=0.463  Val_Acc=0.821  eta=0.00010 global_step=1286\n",
      "Epoch 1287:  Train_loss=0.478  Val_loss=0.462  Val_Acc=0.821  eta=0.00010 global_step=1287\n",
      "Epoch 1288:  Train_loss=0.478  Val_loss=0.462  Val_Acc=0.821  eta=0.00010 global_step=1288\n",
      "Epoch 1289:  Train_loss=0.478  Val_loss=0.462  Val_Acc=0.821  eta=0.00010 global_step=1289\n",
      "Epoch 1290:  Train_loss=0.478  Val_loss=0.462  Val_Acc=0.821  eta=0.00010 global_step=1290\n",
      "Epoch 1291:  Train_loss=0.478  Val_loss=0.462  Val_Acc=0.821  eta=0.00010 global_step=1291\n",
      "Epoch 1292:  Train_loss=0.477  Val_loss=0.462  Val_Acc=0.821  eta=0.00010 global_step=1292\n",
      "Epoch 1293:  Train_loss=0.477  Val_loss=0.462  Val_Acc=0.821  eta=0.00010 global_step=1293\n",
      "Epoch 1294:  Train_loss=0.477  Val_loss=0.461  Val_Acc=0.821  eta=0.00010 global_step=1294\n",
      "Epoch 1295:  Train_loss=0.477  Val_loss=0.461  Val_Acc=0.821  eta=0.00010 global_step=1295\n",
      "Epoch 1296:  Train_loss=0.477  Val_loss=0.461  Val_Acc=0.821  eta=0.00010 global_step=1296\n",
      "Epoch 1297:  Train_loss=0.477  Val_loss=0.461  Val_Acc=0.821  eta=0.00010 global_step=1297\n",
      "Epoch 1298:  Train_loss=0.477  Val_loss=0.461  Val_Acc=0.821  eta=0.00010 global_step=1298\n",
      "Epoch 1299:  Train_loss=0.477  Val_loss=0.461  Val_Acc=0.821  eta=0.00010 global_step=1299\n",
      "Epoch 1300:  Train_loss=0.476  Val_loss=0.461  Val_Acc=0.821  eta=0.00010 global_step=1300\n",
      "Epoch 1301:  Train_loss=0.476  Val_loss=0.461  Val_Acc=0.821  eta=0.00010 global_step=1301\n",
      "Epoch 1302:  Train_loss=0.476  Val_loss=0.460  Val_Acc=0.821  eta=0.00010 global_step=1302\n",
      "Epoch 1303:  Train_loss=0.476  Val_loss=0.460  Val_Acc=0.828  eta=0.00010 global_step=1303\n",
      "Epoch 1304:  Train_loss=0.476  Val_loss=0.460  Val_Acc=0.828  eta=0.00010 global_step=1304\n",
      "Epoch 1305:  Train_loss=0.476  Val_loss=0.460  Val_Acc=0.828  eta=0.00010 global_step=1305\n",
      "Epoch 1306:  Train_loss=0.476  Val_loss=0.460  Val_Acc=0.828  eta=0.00010 global_step=1306\n",
      "Epoch 1307:  Train_loss=0.476  Val_loss=0.460  Val_Acc=0.828  eta=0.00010 global_step=1307\n",
      "Epoch 1308:  Train_loss=0.476  Val_loss=0.460  Val_Acc=0.828  eta=0.00010 global_step=1308\n",
      "Epoch 1309:  Train_loss=0.475  Val_loss=0.460  Val_Acc=0.828  eta=0.00010 global_step=1309\n",
      "Epoch 1310:  Train_loss=0.475  Val_loss=0.459  Val_Acc=0.828  eta=0.00010 global_step=1310\n",
      "Epoch 1311:  Train_loss=0.475  Val_loss=0.459  Val_Acc=0.828  eta=0.00010 global_step=1311\n",
      "Epoch 1312:  Train_loss=0.475  Val_loss=0.459  Val_Acc=0.828  eta=0.00010 global_step=1312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1313:  Train_loss=0.475  Val_loss=0.459  Val_Acc=0.828  eta=0.00010 global_step=1313\n",
      "Epoch 1314:  Train_loss=0.475  Val_loss=0.459  Val_Acc=0.828  eta=0.00010 global_step=1314\n",
      "Epoch 1315:  Train_loss=0.475  Val_loss=0.459  Val_Acc=0.828  eta=0.00010 global_step=1315\n",
      "Epoch 1316:  Train_loss=0.475  Val_loss=0.459  Val_Acc=0.828  eta=0.00010 global_step=1316\n",
      "Epoch 1317:  Train_loss=0.474  Val_loss=0.459  Val_Acc=0.828  eta=0.00010 global_step=1317\n",
      "Epoch 1318:  Train_loss=0.474  Val_loss=0.458  Val_Acc=0.828  eta=0.00010 global_step=1318\n",
      "Epoch 1319:  Train_loss=0.474  Val_loss=0.458  Val_Acc=0.828  eta=0.00010 global_step=1319\n",
      "Epoch 1320:  Train_loss=0.474  Val_loss=0.458  Val_Acc=0.828  eta=0.00010 global_step=1320\n",
      "Epoch 1321:  Train_loss=0.474  Val_loss=0.458  Val_Acc=0.828  eta=0.00010 global_step=1321\n",
      "Epoch 1322:  Train_loss=0.474  Val_loss=0.458  Val_Acc=0.828  eta=0.00010 global_step=1322\n",
      "Epoch 1323:  Train_loss=0.474  Val_loss=0.458  Val_Acc=0.828  eta=0.00010 global_step=1323\n",
      "Epoch 1324:  Train_loss=0.474  Val_loss=0.458  Val_Acc=0.828  eta=0.00010 global_step=1324\n",
      "Epoch 1325:  Train_loss=0.474  Val_loss=0.458  Val_Acc=0.828  eta=0.00010 global_step=1325\n",
      "Epoch 1326:  Train_loss=0.473  Val_loss=0.457  Val_Acc=0.828  eta=0.00010 global_step=1326\n",
      "Epoch 1327:  Train_loss=0.473  Val_loss=0.457  Val_Acc=0.828  eta=0.00010 global_step=1327\n",
      "Epoch 1328:  Train_loss=0.473  Val_loss=0.457  Val_Acc=0.828  eta=0.00010 global_step=1328\n",
      "Epoch 1329:  Train_loss=0.473  Val_loss=0.457  Val_Acc=0.828  eta=0.00010 global_step=1329\n",
      "Epoch 1330:  Train_loss=0.473  Val_loss=0.457  Val_Acc=0.828  eta=0.00010 global_step=1330\n",
      "Epoch 1331:  Train_loss=0.473  Val_loss=0.457  Val_Acc=0.828  eta=0.00010 global_step=1331\n",
      "Epoch 1332:  Train_loss=0.473  Val_loss=0.457  Val_Acc=0.828  eta=0.00010 global_step=1332\n",
      "Epoch 1333:  Train_loss=0.473  Val_loss=0.457  Val_Acc=0.828  eta=0.00010 global_step=1333\n",
      "Epoch 1334:  Train_loss=0.472  Val_loss=0.456  Val_Acc=0.828  eta=0.00010 global_step=1334\n",
      "Epoch 1335:  Train_loss=0.472  Val_loss=0.456  Val_Acc=0.828  eta=0.00010 global_step=1335\n",
      "Epoch 1336:  Train_loss=0.472  Val_loss=0.456  Val_Acc=0.828  eta=0.00010 global_step=1336\n",
      "Epoch 1337:  Train_loss=0.472  Val_loss=0.456  Val_Acc=0.828  eta=0.00010 global_step=1337\n",
      "Epoch 1338:  Train_loss=0.472  Val_loss=0.456  Val_Acc=0.828  eta=0.00010 global_step=1338\n",
      "Epoch 1339:  Train_loss=0.472  Val_loss=0.456  Val_Acc=0.828  eta=0.00010 global_step=1339\n",
      "Epoch 1340:  Train_loss=0.472  Val_loss=0.456  Val_Acc=0.828  eta=0.00010 global_step=1340\n",
      "Epoch 1341:  Train_loss=0.472  Val_loss=0.456  Val_Acc=0.828  eta=0.00010 global_step=1341\n",
      "Epoch 1342:  Train_loss=0.472  Val_loss=0.455  Val_Acc=0.828  eta=0.00010 global_step=1342\n",
      "Epoch 1343:  Train_loss=0.471  Val_loss=0.455  Val_Acc=0.828  eta=0.00010 global_step=1343\n",
      "Epoch 1344:  Train_loss=0.471  Val_loss=0.455  Val_Acc=0.828  eta=0.00010 global_step=1344\n",
      "Epoch 1345:  Train_loss=0.471  Val_loss=0.455  Val_Acc=0.828  eta=0.00010 global_step=1345\n",
      "Epoch 1346:  Train_loss=0.471  Val_loss=0.455  Val_Acc=0.828  eta=0.00010 global_step=1346\n",
      "Epoch 1347:  Train_loss=0.471  Val_loss=0.455  Val_Acc=0.828  eta=0.00010 global_step=1347\n",
      "Epoch 1348:  Train_loss=0.471  Val_loss=0.455  Val_Acc=0.828  eta=0.00010 global_step=1348\n",
      "Epoch 1349:  Train_loss=0.471  Val_loss=0.455  Val_Acc=0.828  eta=0.00010 global_step=1349\n",
      "Epoch 1350:  Train_loss=0.471  Val_loss=0.455  Val_Acc=0.828  eta=0.00010 global_step=1350\n",
      "Epoch 1351:  Train_loss=0.471  Val_loss=0.454  Val_Acc=0.828  eta=0.00010 global_step=1351\n",
      "Epoch 1352:  Train_loss=0.470  Val_loss=0.454  Val_Acc=0.828  eta=0.00010 global_step=1352\n",
      "Epoch 1353:  Train_loss=0.470  Val_loss=0.454  Val_Acc=0.828  eta=0.00010 global_step=1353\n",
      "Epoch 1354:  Train_loss=0.470  Val_loss=0.454  Val_Acc=0.828  eta=0.00010 global_step=1354\n",
      "Epoch 1355:  Train_loss=0.470  Val_loss=0.454  Val_Acc=0.828  eta=0.00010 global_step=1355\n",
      "Epoch 1356:  Train_loss=0.470  Val_loss=0.454  Val_Acc=0.828  eta=0.00010 global_step=1356\n",
      "Epoch 1357:  Train_loss=0.470  Val_loss=0.454  Val_Acc=0.828  eta=0.00010 global_step=1357\n",
      "Epoch 1358:  Train_loss=0.470  Val_loss=0.454  Val_Acc=0.828  eta=0.00010 global_step=1358\n",
      "Epoch 1359:  Train_loss=0.470  Val_loss=0.453  Val_Acc=0.828  eta=0.00010 global_step=1359\n",
      "Epoch 1360:  Train_loss=0.469  Val_loss=0.453  Val_Acc=0.828  eta=0.00010 global_step=1360\n",
      "Epoch 1361:  Train_loss=0.469  Val_loss=0.453  Val_Acc=0.828  eta=0.00010 global_step=1361\n",
      "Epoch 1362:  Train_loss=0.469  Val_loss=0.453  Val_Acc=0.828  eta=0.00010 global_step=1362\n",
      "Epoch 1363:  Train_loss=0.469  Val_loss=0.453  Val_Acc=0.828  eta=0.00010 global_step=1363\n",
      "Epoch 1364:  Train_loss=0.469  Val_loss=0.453  Val_Acc=0.828  eta=0.00010 global_step=1364\n",
      "Epoch 1365:  Train_loss=0.469  Val_loss=0.453  Val_Acc=0.828  eta=0.00010 global_step=1365\n",
      "Epoch 1366:  Train_loss=0.469  Val_loss=0.453  Val_Acc=0.828  eta=0.00010 global_step=1366\n",
      "Epoch 1367:  Train_loss=0.469  Val_loss=0.453  Val_Acc=0.828  eta=0.00010 global_step=1367\n",
      "Epoch 1368:  Train_loss=0.469  Val_loss=0.452  Val_Acc=0.828  eta=0.00010 global_step=1368\n",
      "Epoch 1369:  Train_loss=0.469  Val_loss=0.452  Val_Acc=0.828  eta=0.00010 global_step=1369\n",
      "Epoch 1370:  Train_loss=0.468  Val_loss=0.452  Val_Acc=0.828  eta=0.00010 global_step=1370\n",
      "Epoch 1371:  Train_loss=0.468  Val_loss=0.452  Val_Acc=0.828  eta=0.00010 global_step=1371\n",
      "Epoch 1372:  Train_loss=0.468  Val_loss=0.452  Val_Acc=0.828  eta=0.00010 global_step=1372\n",
      "Epoch 1373:  Train_loss=0.468  Val_loss=0.452  Val_Acc=0.828  eta=0.00010 global_step=1373\n",
      "Epoch 1374:  Train_loss=0.468  Val_loss=0.452  Val_Acc=0.828  eta=0.00010 global_step=1374\n",
      "Epoch 1375:  Train_loss=0.468  Val_loss=0.452  Val_Acc=0.828  eta=0.00010 global_step=1375\n",
      "Epoch 1376:  Train_loss=0.468  Val_loss=0.452  Val_Acc=0.828  eta=0.00010 global_step=1376\n",
      "Epoch 1377:  Train_loss=0.468  Val_loss=0.451  Val_Acc=0.828  eta=0.00010 global_step=1377\n",
      "Epoch 1378:  Train_loss=0.468  Val_loss=0.451  Val_Acc=0.828  eta=0.00010 global_step=1378\n",
      "Epoch 1379:  Train_loss=0.467  Val_loss=0.451  Val_Acc=0.828  eta=0.00010 global_step=1379\n",
      "Epoch 1380:  Train_loss=0.467  Val_loss=0.451  Val_Acc=0.828  eta=0.00010 global_step=1380\n",
      "Epoch 1381:  Train_loss=0.467  Val_loss=0.451  Val_Acc=0.828  eta=0.00010 global_step=1381\n",
      "Epoch 1382:  Train_loss=0.467  Val_loss=0.451  Val_Acc=0.828  eta=0.00010 global_step=1382\n",
      "Epoch 1383:  Train_loss=0.467  Val_loss=0.451  Val_Acc=0.828  eta=0.00010 global_step=1383\n",
      "Epoch 1384:  Train_loss=0.467  Val_loss=0.451  Val_Acc=0.828  eta=0.00010 global_step=1384\n",
      "Epoch 1385:  Train_loss=0.467  Val_loss=0.451  Val_Acc=0.828  eta=0.00010 global_step=1385\n",
      "Epoch 1386:  Train_loss=0.467  Val_loss=0.450  Val_Acc=0.828  eta=0.00010 global_step=1386\n",
      "Epoch 1387:  Train_loss=0.467  Val_loss=0.450  Val_Acc=0.828  eta=0.00010 global_step=1387\n",
      "Epoch 1388:  Train_loss=0.466  Val_loss=0.450  Val_Acc=0.828  eta=0.00010 global_step=1388\n",
      "Epoch 1389:  Train_loss=0.466  Val_loss=0.450  Val_Acc=0.828  eta=0.00010 global_step=1389\n",
      "Epoch 1390:  Train_loss=0.466  Val_loss=0.450  Val_Acc=0.828  eta=0.00010 global_step=1390\n",
      "Epoch 1391:  Train_loss=0.466  Val_loss=0.450  Val_Acc=0.828  eta=0.00010 global_step=1391\n",
      "Epoch 1392:  Train_loss=0.466  Val_loss=0.450  Val_Acc=0.828  eta=0.00010 global_step=1392\n",
      "Epoch 1393:  Train_loss=0.466  Val_loss=0.450  Val_Acc=0.828  eta=0.00010 global_step=1393\n",
      "Epoch 1394:  Train_loss=0.466  Val_loss=0.450  Val_Acc=0.828  eta=0.00010 global_step=1394\n",
      "Epoch 1395:  Train_loss=0.466  Val_loss=0.449  Val_Acc=0.828  eta=0.00010 global_step=1395\n",
      "Epoch 1396:  Train_loss=0.466  Val_loss=0.449  Val_Acc=0.828  eta=0.00010 global_step=1396\n",
      "Epoch 1397:  Train_loss=0.465  Val_loss=0.449  Val_Acc=0.828  eta=0.00010 global_step=1397\n",
      "Epoch 1398:  Train_loss=0.465  Val_loss=0.449  Val_Acc=0.828  eta=0.00010 global_step=1398\n",
      "Epoch 1399:  Train_loss=0.465  Val_loss=0.449  Val_Acc=0.828  eta=0.00010 global_step=1399\n",
      "Epoch 1400:  Train_loss=0.465  Val_loss=0.449  Val_Acc=0.828  eta=0.00010 global_step=1400\n",
      "Epoch 1401:  Train_loss=0.465  Val_loss=0.449  Val_Acc=0.828  eta=0.00010 global_step=1401\n",
      "Epoch 1402:  Train_loss=0.465  Val_loss=0.449  Val_Acc=0.828  eta=0.00010 global_step=1402\n",
      "Epoch 1403:  Train_loss=0.465  Val_loss=0.448  Val_Acc=0.828  eta=0.00010 global_step=1403\n",
      "Epoch 1404:  Train_loss=0.465  Val_loss=0.448  Val_Acc=0.828  eta=0.00010 global_step=1404\n",
      "Epoch 1405:  Train_loss=0.465  Val_loss=0.448  Val_Acc=0.828  eta=0.00010 global_step=1405\n",
      "Epoch 1406:  Train_loss=0.465  Val_loss=0.448  Val_Acc=0.828  eta=0.00010 global_step=1406\n",
      "Epoch 1407:  Train_loss=0.464  Val_loss=0.448  Val_Acc=0.828  eta=0.00010 global_step=1407\n",
      "Epoch 1408:  Train_loss=0.464  Val_loss=0.448  Val_Acc=0.828  eta=0.00010 global_step=1408\n",
      "Epoch 1409:  Train_loss=0.464  Val_loss=0.448  Val_Acc=0.828  eta=0.00010 global_step=1409\n",
      "Epoch 1410:  Train_loss=0.464  Val_loss=0.448  Val_Acc=0.828  eta=0.00010 global_step=1410\n",
      "Epoch 1411:  Train_loss=0.464  Val_loss=0.448  Val_Acc=0.828  eta=0.00010 global_step=1411\n",
      "Epoch 1412:  Train_loss=0.464  Val_loss=0.448  Val_Acc=0.828  eta=0.00010 global_step=1412\n",
      "Epoch 1413:  Train_loss=0.464  Val_loss=0.447  Val_Acc=0.828  eta=0.00010 global_step=1413\n",
      "Epoch 1414:  Train_loss=0.464  Val_loss=0.447  Val_Acc=0.828  eta=0.00010 global_step=1414\n",
      "Epoch 1415:  Train_loss=0.464  Val_loss=0.447  Val_Acc=0.828  eta=0.00010 global_step=1415\n",
      "Epoch 1416:  Train_loss=0.463  Val_loss=0.447  Val_Acc=0.828  eta=0.00010 global_step=1416\n",
      "Epoch 1417:  Train_loss=0.463  Val_loss=0.447  Val_Acc=0.828  eta=0.00010 global_step=1417\n",
      "Epoch 1418:  Train_loss=0.463  Val_loss=0.447  Val_Acc=0.828  eta=0.00010 global_step=1418\n",
      "Epoch 1419:  Train_loss=0.463  Val_loss=0.447  Val_Acc=0.828  eta=0.00010 global_step=1419\n",
      "Epoch 1420:  Train_loss=0.463  Val_loss=0.447  Val_Acc=0.828  eta=0.00010 global_step=1420\n",
      "Epoch 1421:  Train_loss=0.463  Val_loss=0.447  Val_Acc=0.828  eta=0.00010 global_step=1421\n",
      "Epoch 1422:  Train_loss=0.463  Val_loss=0.446  Val_Acc=0.828  eta=0.00010 global_step=1422\n",
      "Epoch 1423:  Train_loss=0.463  Val_loss=0.446  Val_Acc=0.828  eta=0.00010 global_step=1423\n",
      "Epoch 1424:  Train_loss=0.463  Val_loss=0.446  Val_Acc=0.828  eta=0.00010 global_step=1424\n",
      "Epoch 1425:  Train_loss=0.463  Val_loss=0.446  Val_Acc=0.828  eta=0.00010 global_step=1425\n",
      "Epoch 1426:  Train_loss=0.462  Val_loss=0.446  Val_Acc=0.828  eta=0.00010 global_step=1426\n",
      "Epoch 1427:  Train_loss=0.462  Val_loss=0.446  Val_Acc=0.828  eta=0.00010 global_step=1427\n",
      "Epoch 1428:  Train_loss=0.462  Val_loss=0.446  Val_Acc=0.828  eta=0.00010 global_step=1428\n",
      "Epoch 1429:  Train_loss=0.462  Val_loss=0.446  Val_Acc=0.828  eta=0.00010 global_step=1429\n",
      "Epoch 1430:  Train_loss=0.462  Val_loss=0.446  Val_Acc=0.828  eta=0.00010 global_step=1430\n",
      "Epoch 1431:  Train_loss=0.462  Val_loss=0.445  Val_Acc=0.828  eta=0.00010 global_step=1431\n",
      "Epoch 1432:  Train_loss=0.462  Val_loss=0.445  Val_Acc=0.828  eta=0.00010 global_step=1432\n",
      "Epoch 1433:  Train_loss=0.462  Val_loss=0.445  Val_Acc=0.828  eta=0.00010 global_step=1433\n",
      "Epoch 1434:  Train_loss=0.462  Val_loss=0.445  Val_Acc=0.828  eta=0.00010 global_step=1434\n",
      "Epoch 1435:  Train_loss=0.462  Val_loss=0.445  Val_Acc=0.828  eta=0.00010 global_step=1435\n",
      "Epoch 1436:  Train_loss=0.461  Val_loss=0.445  Val_Acc=0.828  eta=0.00010 global_step=1436\n",
      "Epoch 1437:  Train_loss=0.461  Val_loss=0.445  Val_Acc=0.828  eta=0.00010 global_step=1437\n",
      "Epoch 1438:  Train_loss=0.461  Val_loss=0.445  Val_Acc=0.828  eta=0.00010 global_step=1438\n",
      "Epoch 1439:  Train_loss=0.461  Val_loss=0.445  Val_Acc=0.828  eta=0.00010 global_step=1439\n",
      "Epoch 1440:  Train_loss=0.461  Val_loss=0.444  Val_Acc=0.828  eta=0.00010 global_step=1440\n",
      "Epoch 1441:  Train_loss=0.461  Val_loss=0.444  Val_Acc=0.828  eta=0.00010 global_step=1441\n",
      "Epoch 1442:  Train_loss=0.461  Val_loss=0.444  Val_Acc=0.828  eta=0.00010 global_step=1442\n",
      "Epoch 1443:  Train_loss=0.461  Val_loss=0.444  Val_Acc=0.828  eta=0.00010 global_step=1443\n",
      "Epoch 1444:  Train_loss=0.461  Val_loss=0.444  Val_Acc=0.828  eta=0.00010 global_step=1444\n",
      "Epoch 1445:  Train_loss=0.461  Val_loss=0.444  Val_Acc=0.828  eta=0.00010 global_step=1445\n",
      "Epoch 1446:  Train_loss=0.460  Val_loss=0.444  Val_Acc=0.828  eta=0.00010 global_step=1446\n",
      "Epoch 1447:  Train_loss=0.460  Val_loss=0.444  Val_Acc=0.828  eta=0.00010 global_step=1447\n",
      "Epoch 1448:  Train_loss=0.460  Val_loss=0.444  Val_Acc=0.828  eta=0.00010 global_step=1448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1449:  Train_loss=0.460  Val_loss=0.444  Val_Acc=0.828  eta=0.00010 global_step=1449\n",
      "Epoch 1450:  Train_loss=0.460  Val_loss=0.443  Val_Acc=0.828  eta=0.00010 global_step=1450\n",
      "Epoch 1451:  Train_loss=0.460  Val_loss=0.443  Val_Acc=0.828  eta=0.00010 global_step=1451\n",
      "Epoch 1452:  Train_loss=0.460  Val_loss=0.443  Val_Acc=0.828  eta=0.00010 global_step=1452\n",
      "Epoch 1453:  Train_loss=0.460  Val_loss=0.443  Val_Acc=0.828  eta=0.00010 global_step=1453\n",
      "Epoch 1454:  Train_loss=0.460  Val_loss=0.443  Val_Acc=0.828  eta=0.00010 global_step=1454\n",
      "Epoch 1455:  Train_loss=0.460  Val_loss=0.443  Val_Acc=0.828  eta=0.00010 global_step=1455\n",
      "Epoch 1456:  Train_loss=0.460  Val_loss=0.443  Val_Acc=0.828  eta=0.00010 global_step=1456\n",
      "Epoch 1457:  Train_loss=0.459  Val_loss=0.443  Val_Acc=0.828  eta=0.00010 global_step=1457\n",
      "Epoch 1458:  Train_loss=0.459  Val_loss=0.443  Val_Acc=0.828  eta=0.00010 global_step=1458\n",
      "Epoch 1459:  Train_loss=0.459  Val_loss=0.442  Val_Acc=0.828  eta=0.00010 global_step=1459\n",
      "Epoch 1460:  Train_loss=0.459  Val_loss=0.442  Val_Acc=0.828  eta=0.00010 global_step=1460\n",
      "Epoch 1461:  Train_loss=0.459  Val_loss=0.442  Val_Acc=0.828  eta=0.00010 global_step=1461\n",
      "Epoch 1462:  Train_loss=0.459  Val_loss=0.442  Val_Acc=0.828  eta=0.00010 global_step=1462\n",
      "Epoch 1463:  Train_loss=0.459  Val_loss=0.442  Val_Acc=0.828  eta=0.00010 global_step=1463\n",
      "Epoch 1464:  Train_loss=0.459  Val_loss=0.442  Val_Acc=0.828  eta=0.00010 global_step=1464\n",
      "Epoch 1465:  Train_loss=0.459  Val_loss=0.442  Val_Acc=0.828  eta=0.00010 global_step=1465\n",
      "Epoch 1466:  Train_loss=0.459  Val_loss=0.442  Val_Acc=0.828  eta=0.00010 global_step=1466\n",
      "Epoch 1467:  Train_loss=0.458  Val_loss=0.442  Val_Acc=0.828  eta=0.00010 global_step=1467\n",
      "Epoch 1468:  Train_loss=0.458  Val_loss=0.442  Val_Acc=0.828  eta=0.00010 global_step=1468\n",
      "Epoch 1469:  Train_loss=0.458  Val_loss=0.441  Val_Acc=0.828  eta=0.00010 global_step=1469\n",
      "Epoch 1470:  Train_loss=0.458  Val_loss=0.441  Val_Acc=0.828  eta=0.00010 global_step=1470\n",
      "Epoch 1471:  Train_loss=0.458  Val_loss=0.441  Val_Acc=0.828  eta=0.00010 global_step=1471\n",
      "Epoch 1472:  Train_loss=0.458  Val_loss=0.441  Val_Acc=0.828  eta=0.00010 global_step=1472\n",
      "Epoch 1473:  Train_loss=0.458  Val_loss=0.441  Val_Acc=0.828  eta=0.00010 global_step=1473\n",
      "Epoch 1474:  Train_loss=0.458  Val_loss=0.441  Val_Acc=0.828  eta=0.00010 global_step=1474\n",
      "Epoch 1475:  Train_loss=0.458  Val_loss=0.441  Val_Acc=0.828  eta=0.00010 global_step=1475\n",
      "Epoch 1476:  Train_loss=0.458  Val_loss=0.441  Val_Acc=0.828  eta=0.00010 global_step=1476\n",
      "Epoch 1477:  Train_loss=0.458  Val_loss=0.441  Val_Acc=0.828  eta=0.00010 global_step=1477\n",
      "Epoch 1478:  Train_loss=0.457  Val_loss=0.441  Val_Acc=0.828  eta=0.00010 global_step=1478\n",
      "Epoch 1479:  Train_loss=0.457  Val_loss=0.440  Val_Acc=0.828  eta=0.00010 global_step=1479\n",
      "Epoch 1480:  Train_loss=0.457  Val_loss=0.440  Val_Acc=0.828  eta=0.00010 global_step=1480\n",
      "Epoch 1481:  Train_loss=0.457  Val_loss=0.440  Val_Acc=0.828  eta=0.00010 global_step=1481\n",
      "Epoch 1482:  Train_loss=0.457  Val_loss=0.440  Val_Acc=0.828  eta=0.00010 global_step=1482\n",
      "Epoch 1483:  Train_loss=0.457  Val_loss=0.440  Val_Acc=0.828  eta=0.00010 global_step=1483\n",
      "Epoch 1484:  Train_loss=0.457  Val_loss=0.440  Val_Acc=0.828  eta=0.00010 global_step=1484\n",
      "Epoch 1485:  Train_loss=0.457  Val_loss=0.440  Val_Acc=0.828  eta=0.00010 global_step=1485\n",
      "Epoch 1486:  Train_loss=0.457  Val_loss=0.440  Val_Acc=0.828  eta=0.00010 global_step=1486\n",
      "Epoch 1487:  Train_loss=0.457  Val_loss=0.440  Val_Acc=0.828  eta=0.00010 global_step=1487\n",
      "Epoch 1488:  Train_loss=0.456  Val_loss=0.440  Val_Acc=0.828  eta=0.00010 global_step=1488\n",
      "Epoch 1489:  Train_loss=0.456  Val_loss=0.439  Val_Acc=0.828  eta=0.00010 global_step=1489\n",
      "Epoch 1490:  Train_loss=0.456  Val_loss=0.439  Val_Acc=0.828  eta=0.00010 global_step=1490\n",
      "Epoch 1491:  Train_loss=0.456  Val_loss=0.439  Val_Acc=0.828  eta=0.00010 global_step=1491\n",
      "Epoch 1492:  Train_loss=0.456  Val_loss=0.439  Val_Acc=0.828  eta=0.00010 global_step=1492\n",
      "Epoch 1493:  Train_loss=0.456  Val_loss=0.439  Val_Acc=0.828  eta=0.00010 global_step=1493\n",
      "Epoch 1494:  Train_loss=0.456  Val_loss=0.439  Val_Acc=0.828  eta=0.00010 global_step=1494\n",
      "Epoch 1495:  Train_loss=0.456  Val_loss=0.439  Val_Acc=0.828  eta=0.00010 global_step=1495\n",
      "Epoch 1496:  Train_loss=0.456  Val_loss=0.439  Val_Acc=0.828  eta=0.00010 global_step=1496\n",
      "Epoch 1497:  Train_loss=0.456  Val_loss=0.439  Val_Acc=0.828  eta=0.00010 global_step=1497\n",
      "Epoch 1498:  Train_loss=0.456  Val_loss=0.439  Val_Acc=0.828  eta=0.00010 global_step=1498\n",
      "Epoch 1499:  Train_loss=0.455  Val_loss=0.438  Val_Acc=0.828  eta=0.00010 global_step=1499\n",
      "Epoch 1500:  Train_loss=0.455  Val_loss=0.438  Val_Acc=0.828  eta=0.00010 global_step=1500\n",
      "Epoch 1501:  Train_loss=0.455  Val_loss=0.438  Val_Acc=0.828  eta=0.00010 global_step=1501\n",
      "Epoch 1502:  Train_loss=0.455  Val_loss=0.438  Val_Acc=0.828  eta=0.00010 global_step=1502\n",
      "Epoch 1503:  Train_loss=0.455  Val_loss=0.438  Val_Acc=0.828  eta=0.00010 global_step=1503\n",
      "Epoch 1504:  Train_loss=0.455  Val_loss=0.438  Val_Acc=0.828  eta=0.00010 global_step=1504\n",
      "Epoch 1505:  Train_loss=0.455  Val_loss=0.438  Val_Acc=0.828  eta=0.00010 global_step=1505\n",
      "Epoch 1506:  Train_loss=0.455  Val_loss=0.438  Val_Acc=0.828  eta=0.00010 global_step=1506\n",
      "Epoch 1507:  Train_loss=0.455  Val_loss=0.438  Val_Acc=0.828  eta=0.00010 global_step=1507\n",
      "Epoch 1508:  Train_loss=0.455  Val_loss=0.438  Val_Acc=0.828  eta=0.00010 global_step=1508\n",
      "Epoch 1509:  Train_loss=0.455  Val_loss=0.438  Val_Acc=0.828  eta=0.00010 global_step=1509\n",
      "Epoch 1510:  Train_loss=0.455  Val_loss=0.437  Val_Acc=0.828  eta=0.00010 global_step=1510\n",
      "Epoch 1511:  Train_loss=0.454  Val_loss=0.437  Val_Acc=0.828  eta=0.00010 global_step=1511\n",
      "Epoch 1512:  Train_loss=0.454  Val_loss=0.437  Val_Acc=0.828  eta=0.00010 global_step=1512\n",
      "Epoch 1513:  Train_loss=0.454  Val_loss=0.437  Val_Acc=0.821  eta=0.00010 global_step=1513\n",
      "Epoch 1514:  Train_loss=0.454  Val_loss=0.437  Val_Acc=0.821  eta=0.00010 global_step=1514\n",
      "Epoch 1515:  Train_loss=0.454  Val_loss=0.437  Val_Acc=0.821  eta=0.00010 global_step=1515\n",
      "Epoch 1516:  Train_loss=0.454  Val_loss=0.437  Val_Acc=0.821  eta=0.00010 global_step=1516\n",
      "Epoch 1517:  Train_loss=0.454  Val_loss=0.437  Val_Acc=0.821  eta=0.00010 global_step=1517\n",
      "Epoch 1518:  Train_loss=0.454  Val_loss=0.437  Val_Acc=0.821  eta=0.00010 global_step=1518\n",
      "Epoch 1519:  Train_loss=0.454  Val_loss=0.437  Val_Acc=0.821  eta=0.00010 global_step=1519\n",
      "Epoch 1520:  Train_loss=0.454  Val_loss=0.437  Val_Acc=0.821  eta=0.00010 global_step=1520\n",
      "Epoch 1521:  Train_loss=0.454  Val_loss=0.436  Val_Acc=0.821  eta=0.00010 global_step=1521\n",
      "Epoch 1522:  Train_loss=0.453  Val_loss=0.436  Val_Acc=0.821  eta=0.00010 global_step=1522\n",
      "Epoch 1523:  Train_loss=0.453  Val_loss=0.436  Val_Acc=0.821  eta=0.00010 global_step=1523\n",
      "Epoch 1524:  Train_loss=0.453  Val_loss=0.436  Val_Acc=0.821  eta=0.00010 global_step=1524\n",
      "Epoch 1525:  Train_loss=0.453  Val_loss=0.436  Val_Acc=0.821  eta=0.00010 global_step=1525\n",
      "Epoch 1526:  Train_loss=0.453  Val_loss=0.436  Val_Acc=0.821  eta=0.00010 global_step=1526\n",
      "Epoch 1527:  Train_loss=0.453  Val_loss=0.436  Val_Acc=0.821  eta=0.00010 global_step=1527\n",
      "Epoch 1528:  Train_loss=0.453  Val_loss=0.436  Val_Acc=0.821  eta=0.00010 global_step=1528\n",
      "Epoch 1529:  Train_loss=0.453  Val_loss=0.436  Val_Acc=0.821  eta=0.00010 global_step=1529\n",
      "Epoch 1530:  Train_loss=0.453  Val_loss=0.436  Val_Acc=0.821  eta=0.00010 global_step=1530\n",
      "Epoch 1531:  Train_loss=0.453  Val_loss=0.436  Val_Acc=0.821  eta=0.00010 global_step=1531\n",
      "Epoch 1532:  Train_loss=0.453  Val_loss=0.435  Val_Acc=0.821  eta=0.00010 global_step=1532\n",
      "Epoch 1533:  Train_loss=0.453  Val_loss=0.435  Val_Acc=0.821  eta=0.00010 global_step=1533\n",
      "Epoch 1534:  Train_loss=0.452  Val_loss=0.435  Val_Acc=0.813  eta=0.00010 global_step=1534\n",
      "Epoch 1535:  Train_loss=0.452  Val_loss=0.435  Val_Acc=0.813  eta=0.00010 global_step=1535\n",
      "Epoch 1536:  Train_loss=0.452  Val_loss=0.435  Val_Acc=0.813  eta=0.00010 global_step=1536\n",
      "Epoch 1537:  Train_loss=0.452  Val_loss=0.435  Val_Acc=0.813  eta=0.00010 global_step=1537\n",
      "Epoch 1538:  Train_loss=0.452  Val_loss=0.435  Val_Acc=0.813  eta=0.00010 global_step=1538\n",
      "Epoch 1539:  Train_loss=0.452  Val_loss=0.435  Val_Acc=0.821  eta=0.00010 global_step=1539\n",
      "Epoch 1540:  Train_loss=0.452  Val_loss=0.435  Val_Acc=0.821  eta=0.00010 global_step=1540\n",
      "Epoch 1541:  Train_loss=0.452  Val_loss=0.435  Val_Acc=0.821  eta=0.00010 global_step=1541\n",
      "Epoch 1542:  Train_loss=0.452  Val_loss=0.435  Val_Acc=0.821  eta=0.00010 global_step=1542\n",
      "Epoch 1543:  Train_loss=0.452  Val_loss=0.434  Val_Acc=0.821  eta=0.00010 global_step=1543\n",
      "Epoch 1544:  Train_loss=0.452  Val_loss=0.434  Val_Acc=0.821  eta=0.00010 global_step=1544\n",
      "Epoch 1545:  Train_loss=0.451  Val_loss=0.434  Val_Acc=0.828  eta=0.00010 global_step=1545\n",
      "Epoch 1546:  Train_loss=0.451  Val_loss=0.434  Val_Acc=0.828  eta=0.00010 global_step=1546\n",
      "Epoch 1547:  Train_loss=0.451  Val_loss=0.434  Val_Acc=0.828  eta=0.00010 global_step=1547\n",
      "Epoch 1548:  Train_loss=0.451  Val_loss=0.434  Val_Acc=0.828  eta=0.00010 global_step=1548\n",
      "Epoch 1549:  Train_loss=0.451  Val_loss=0.434  Val_Acc=0.828  eta=0.00010 global_step=1549\n",
      "Epoch 1550:  Train_loss=0.451  Val_loss=0.434  Val_Acc=0.828  eta=0.00010 global_step=1550\n",
      "Epoch 1551:  Train_loss=0.451  Val_loss=0.434  Val_Acc=0.828  eta=0.00010 global_step=1551\n",
      "Epoch 1552:  Train_loss=0.451  Val_loss=0.434  Val_Acc=0.828  eta=0.00010 global_step=1552\n",
      "Epoch 1553:  Train_loss=0.451  Val_loss=0.434  Val_Acc=0.828  eta=0.00010 global_step=1553\n",
      "Epoch 1554:  Train_loss=0.451  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1554\n",
      "Epoch 1555:  Train_loss=0.451  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1555\n",
      "Epoch 1556:  Train_loss=0.451  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1556\n",
      "Epoch 1557:  Train_loss=0.450  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1557\n",
      "Epoch 1558:  Train_loss=0.450  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1558\n",
      "Epoch 1559:  Train_loss=0.450  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1559\n",
      "Epoch 1560:  Train_loss=0.450  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1560\n",
      "Epoch 1561:  Train_loss=0.450  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1561\n",
      "Epoch 1562:  Train_loss=0.450  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1562\n",
      "Epoch 1563:  Train_loss=0.450  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1563\n",
      "Epoch 1564:  Train_loss=0.450  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1564\n",
      "Epoch 1565:  Train_loss=0.450  Val_loss=0.433  Val_Acc=0.828  eta=0.00010 global_step=1565\n",
      "Epoch 1566:  Train_loss=0.450  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1566\n",
      "Epoch 1567:  Train_loss=0.450  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1567\n",
      "Epoch 1568:  Train_loss=0.450  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1568\n",
      "Epoch 1569:  Train_loss=0.450  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1569\n",
      "Epoch 1570:  Train_loss=0.449  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1570\n",
      "Epoch 1571:  Train_loss=0.449  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1571\n",
      "Epoch 1572:  Train_loss=0.449  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1572\n",
      "Epoch 1573:  Train_loss=0.449  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1573\n",
      "Epoch 1574:  Train_loss=0.449  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1574\n",
      "Epoch 1575:  Train_loss=0.449  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1575\n",
      "Epoch 1576:  Train_loss=0.449  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1576\n",
      "Epoch 1577:  Train_loss=0.449  Val_loss=0.432  Val_Acc=0.828  eta=0.00010 global_step=1577\n",
      "Epoch 1578:  Train_loss=0.449  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1578\n",
      "Epoch 1579:  Train_loss=0.449  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1579\n",
      "Epoch 1580:  Train_loss=0.449  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1580\n",
      "Epoch 1581:  Train_loss=0.449  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1581\n",
      "Epoch 1582:  Train_loss=0.448  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1582\n",
      "Epoch 1583:  Train_loss=0.448  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1583\n",
      "Epoch 1584:  Train_loss=0.448  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1584\n",
      "Epoch 1585:  Train_loss=0.448  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1585\n",
      "Epoch 1586:  Train_loss=0.448  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1586\n",
      "Epoch 1587:  Train_loss=0.448  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1588:  Train_loss=0.448  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1588\n",
      "Epoch 1589:  Train_loss=0.448  Val_loss=0.431  Val_Acc=0.828  eta=0.00010 global_step=1589\n",
      "Epoch 1590:  Train_loss=0.448  Val_loss=0.430  Val_Acc=0.828  eta=0.00010 global_step=1590\n",
      "Epoch 1591:  Train_loss=0.448  Val_loss=0.430  Val_Acc=0.828  eta=0.00010 global_step=1591\n",
      "Epoch 1592:  Train_loss=0.448  Val_loss=0.430  Val_Acc=0.828  eta=0.00010 global_step=1592\n",
      "Epoch 1593:  Train_loss=0.448  Val_loss=0.430  Val_Acc=0.828  eta=0.00010 global_step=1593\n",
      "Epoch 1594:  Train_loss=0.448  Val_loss=0.430  Val_Acc=0.828  eta=0.00010 global_step=1594\n",
      "Epoch 1595:  Train_loss=0.447  Val_loss=0.430  Val_Acc=0.828  eta=0.00010 global_step=1595\n",
      "Epoch 1596:  Train_loss=0.447  Val_loss=0.430  Val_Acc=0.828  eta=0.00010 global_step=1596\n",
      "Epoch 1597:  Train_loss=0.447  Val_loss=0.430  Val_Acc=0.821  eta=0.00010 global_step=1597\n",
      "Epoch 1598:  Train_loss=0.447  Val_loss=0.430  Val_Acc=0.821  eta=0.00010 global_step=1598\n",
      "Epoch 1599:  Train_loss=0.447  Val_loss=0.430  Val_Acc=0.821  eta=0.00010 global_step=1599\n",
      "Epoch 1600:  Train_loss=0.447  Val_loss=0.430  Val_Acc=0.821  eta=0.00010 global_step=1600\n",
      "Epoch 1601:  Train_loss=0.447  Val_loss=0.430  Val_Acc=0.821  eta=0.00010 global_step=1601\n",
      "Epoch 1602:  Train_loss=0.447  Val_loss=0.430  Val_Acc=0.821  eta=0.00010 global_step=1602\n",
      "Epoch 1603:  Train_loss=0.447  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1603\n",
      "Epoch 1604:  Train_loss=0.447  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1604\n",
      "Epoch 1605:  Train_loss=0.447  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1605\n",
      "Epoch 1606:  Train_loss=0.447  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1606\n",
      "Epoch 1607:  Train_loss=0.447  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1607\n",
      "Epoch 1608:  Train_loss=0.446  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1608\n",
      "Epoch 1609:  Train_loss=0.446  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1609\n",
      "Epoch 1610:  Train_loss=0.446  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1610\n",
      "Epoch 1611:  Train_loss=0.446  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1611\n",
      "Epoch 1612:  Train_loss=0.446  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1612\n",
      "Epoch 1613:  Train_loss=0.446  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1613\n",
      "Epoch 1614:  Train_loss=0.446  Val_loss=0.429  Val_Acc=0.821  eta=0.00010 global_step=1614\n",
      "Epoch 1615:  Train_loss=0.446  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1615\n",
      "Epoch 1616:  Train_loss=0.446  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1616\n",
      "Epoch 1617:  Train_loss=0.446  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1617\n",
      "Epoch 1618:  Train_loss=0.446  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1618\n",
      "Epoch 1619:  Train_loss=0.446  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1619\n",
      "Epoch 1620:  Train_loss=0.446  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1620\n",
      "Epoch 1621:  Train_loss=0.446  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1621\n",
      "Epoch 1622:  Train_loss=0.445  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1622\n",
      "Epoch 1623:  Train_loss=0.445  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1623\n",
      "Epoch 1624:  Train_loss=0.445  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1624\n",
      "Epoch 1625:  Train_loss=0.445  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1625\n",
      "Epoch 1626:  Train_loss=0.445  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1626\n",
      "Epoch 1627:  Train_loss=0.445  Val_loss=0.428  Val_Acc=0.821  eta=0.00010 global_step=1627\n",
      "Epoch 1628:  Train_loss=0.445  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1628\n",
      "Epoch 1629:  Train_loss=0.445  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1629\n",
      "Epoch 1630:  Train_loss=0.445  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1630\n",
      "Epoch 1631:  Train_loss=0.445  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1631\n",
      "Epoch 1632:  Train_loss=0.445  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1632\n",
      "Epoch 1633:  Train_loss=0.445  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1633\n",
      "Epoch 1634:  Train_loss=0.445  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1634\n",
      "Epoch 1635:  Train_loss=0.445  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1635\n",
      "Epoch 1636:  Train_loss=0.444  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1636\n",
      "Epoch 1637:  Train_loss=0.444  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1637\n",
      "Epoch 1638:  Train_loss=0.444  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1638\n",
      "Epoch 1639:  Train_loss=0.444  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1639\n",
      "Epoch 1640:  Train_loss=0.444  Val_loss=0.427  Val_Acc=0.821  eta=0.00010 global_step=1640\n",
      "Epoch 1641:  Train_loss=0.444  Val_loss=0.426  Val_Acc=0.821  eta=0.00010 global_step=1641\n",
      "Epoch 1642:  Train_loss=0.444  Val_loss=0.426  Val_Acc=0.821  eta=0.00010 global_step=1642\n",
      "Epoch 1643:  Train_loss=0.444  Val_loss=0.426  Val_Acc=0.821  eta=0.00010 global_step=1643\n",
      "Epoch 1644:  Train_loss=0.444  Val_loss=0.426  Val_Acc=0.821  eta=0.00010 global_step=1644\n",
      "Epoch 1645:  Train_loss=0.444  Val_loss=0.426  Val_Acc=0.828  eta=0.00010 global_step=1645\n",
      "Epoch 1646:  Train_loss=0.444  Val_loss=0.426  Val_Acc=0.828  eta=0.00010 global_step=1646\n",
      "Epoch 1647:  Train_loss=0.444  Val_loss=0.426  Val_Acc=0.828  eta=0.00010 global_step=1647\n",
      "Epoch 1648:  Train_loss=0.444  Val_loss=0.426  Val_Acc=0.828  eta=0.00010 global_step=1648\n",
      "Epoch 1649:  Train_loss=0.444  Val_loss=0.426  Val_Acc=0.828  eta=0.00010 global_step=1649\n",
      "Epoch 1650:  Train_loss=0.443  Val_loss=0.426  Val_Acc=0.828  eta=0.00010 global_step=1650\n",
      "Epoch 1651:  Train_loss=0.443  Val_loss=0.426  Val_Acc=0.828  eta=0.00010 global_step=1651\n",
      "Epoch 1652:  Train_loss=0.443  Val_loss=0.426  Val_Acc=0.828  eta=0.00010 global_step=1652\n",
      "Epoch 1653:  Train_loss=0.443  Val_loss=0.426  Val_Acc=0.828  eta=0.00010 global_step=1653\n",
      "Epoch 1654:  Train_loss=0.443  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1654\n",
      "Epoch 1655:  Train_loss=0.443  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1655\n",
      "Epoch 1656:  Train_loss=0.443  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1656\n",
      "Epoch 1657:  Train_loss=0.443  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1657\n",
      "Epoch 1658:  Train_loss=0.443  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1658\n",
      "Epoch 1659:  Train_loss=0.443  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1659\n",
      "Epoch 1660:  Train_loss=0.443  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1660\n",
      "Epoch 1661:  Train_loss=0.443  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1661\n",
      "Epoch 1662:  Train_loss=0.443  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1662\n",
      "Epoch 1663:  Train_loss=0.443  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1663\n",
      "Epoch 1664:  Train_loss=0.442  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1664\n",
      "Epoch 1665:  Train_loss=0.442  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1665\n",
      "Epoch 1666:  Train_loss=0.442  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1666\n",
      "Epoch 1667:  Train_loss=0.442  Val_loss=0.425  Val_Acc=0.828  eta=0.00010 global_step=1667\n",
      "Epoch 1668:  Train_loss=0.442  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1668\n",
      "Epoch 1669:  Train_loss=0.442  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1669\n",
      "Epoch 1670:  Train_loss=0.442  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1670\n",
      "Epoch 1671:  Train_loss=0.442  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1671\n",
      "Epoch 1672:  Train_loss=0.442  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1672\n",
      "Epoch 1673:  Train_loss=0.442  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1673\n",
      "Epoch 1674:  Train_loss=0.442  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1674\n",
      "Epoch 1675:  Train_loss=0.442  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1675\n",
      "Epoch 1676:  Train_loss=0.442  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1676\n",
      "Epoch 1677:  Train_loss=0.442  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1677\n",
      "Epoch 1678:  Train_loss=0.442  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1678\n",
      "Epoch 1679:  Train_loss=0.441  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1679\n",
      "Epoch 1680:  Train_loss=0.441  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1680\n",
      "Epoch 1681:  Train_loss=0.441  Val_loss=0.424  Val_Acc=0.828  eta=0.00010 global_step=1681\n",
      "Epoch 1682:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1682\n",
      "Epoch 1683:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1683\n",
      "Epoch 1684:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1684\n",
      "Epoch 1685:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1685\n",
      "Epoch 1686:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1686\n",
      "Epoch 1687:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1687\n",
      "Epoch 1688:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1688\n",
      "Epoch 1689:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1689\n",
      "Epoch 1690:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1690\n",
      "Epoch 1691:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1691\n",
      "Epoch 1692:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1692\n",
      "Epoch 1693:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1693\n",
      "Epoch 1694:  Train_loss=0.441  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1694\n",
      "Epoch 1695:  Train_loss=0.440  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1695\n",
      "Epoch 1696:  Train_loss=0.440  Val_loss=0.423  Val_Acc=0.828  eta=0.00010 global_step=1696\n",
      "Epoch 1697:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1697\n",
      "Epoch 1698:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1698\n",
      "Epoch 1699:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1699\n",
      "Epoch 1700:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1700\n",
      "Epoch 1701:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1701\n",
      "Epoch 1702:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1702\n",
      "Epoch 1703:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1703\n",
      "Epoch 1704:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1704\n",
      "Epoch 1705:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1705\n",
      "Epoch 1706:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1706\n",
      "Epoch 1707:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1707\n",
      "Epoch 1708:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1708\n",
      "Epoch 1709:  Train_loss=0.440  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1709\n",
      "Epoch 1710:  Train_loss=0.439  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1710\n",
      "Epoch 1711:  Train_loss=0.439  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1711\n",
      "Epoch 1712:  Train_loss=0.439  Val_loss=0.422  Val_Acc=0.828  eta=0.00010 global_step=1712\n",
      "Epoch 1713:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1713\n",
      "Epoch 1714:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1714\n",
      "Epoch 1715:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1715\n",
      "Epoch 1716:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1716\n",
      "Epoch 1717:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1717\n",
      "Epoch 1718:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1718\n",
      "Epoch 1719:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1719\n",
      "Epoch 1720:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1720\n",
      "Epoch 1721:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1721\n",
      "Epoch 1722:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1722\n",
      "Epoch 1723:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1723\n",
      "Epoch 1724:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1724\n",
      "Epoch 1725:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1725\n",
      "Epoch 1726:  Train_loss=0.439  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1726\n",
      "Epoch 1727:  Train_loss=0.438  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1727\n",
      "Epoch 1728:  Train_loss=0.438  Val_loss=0.421  Val_Acc=0.828  eta=0.00010 global_step=1728\n",
      "Epoch 1729:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1729\n",
      "Epoch 1730:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1731:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1731\n",
      "Epoch 1732:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1732\n",
      "Epoch 1733:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1733\n",
      "Epoch 1734:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1734\n",
      "Epoch 1735:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1735\n",
      "Epoch 1736:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1736\n",
      "Epoch 1737:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1737\n",
      "Epoch 1738:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1738\n",
      "Epoch 1739:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1739\n",
      "Epoch 1740:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1740\n",
      "Epoch 1741:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1741\n",
      "Epoch 1742:  Train_loss=0.438  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1742\n",
      "Epoch 1743:  Train_loss=0.437  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1743\n",
      "Epoch 1744:  Train_loss=0.437  Val_loss=0.420  Val_Acc=0.828  eta=0.00010 global_step=1744\n",
      "Epoch 1745:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1745\n",
      "Epoch 1746:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1746\n",
      "Epoch 1747:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1747\n",
      "Epoch 1748:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1748\n",
      "Epoch 1749:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1749\n",
      "Epoch 1750:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1750\n",
      "Epoch 1751:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1751\n",
      "Epoch 1752:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1752\n",
      "Epoch 1753:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1753\n",
      "Epoch 1754:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1754\n",
      "Epoch 1755:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1755\n",
      "Epoch 1756:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1756\n",
      "Epoch 1757:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1757\n",
      "Epoch 1758:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1758\n",
      "Epoch 1759:  Train_loss=0.437  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1759\n",
      "Epoch 1760:  Train_loss=0.436  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1760\n",
      "Epoch 1761:  Train_loss=0.436  Val_loss=0.419  Val_Acc=0.828  eta=0.00010 global_step=1761\n",
      "Epoch 1762:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.828  eta=0.00010 global_step=1762\n",
      "Epoch 1763:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.828  eta=0.00010 global_step=1763\n",
      "Epoch 1764:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1764\n",
      "Epoch 1765:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1765\n",
      "Epoch 1766:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1766\n",
      "Epoch 1767:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1767\n",
      "Epoch 1768:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1768\n",
      "Epoch 1769:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1769\n",
      "Epoch 1770:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1770\n",
      "Epoch 1771:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1771\n",
      "Epoch 1772:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1772\n",
      "Epoch 1773:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1773\n",
      "Epoch 1774:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1774\n",
      "Epoch 1775:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1775\n",
      "Epoch 1776:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1776\n",
      "Epoch 1777:  Train_loss=0.436  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1777\n",
      "Epoch 1778:  Train_loss=0.435  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1778\n",
      "Epoch 1779:  Train_loss=0.435  Val_loss=0.418  Val_Acc=0.821  eta=0.00010 global_step=1779\n",
      "Epoch 1780:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1780\n",
      "Epoch 1781:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1781\n",
      "Epoch 1782:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1782\n",
      "Epoch 1783:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1783\n",
      "Epoch 1784:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1784\n",
      "Epoch 1785:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1785\n",
      "Epoch 1786:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1786\n",
      "Epoch 1787:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1787\n",
      "Epoch 1788:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1788\n",
      "Epoch 1789:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1789\n",
      "Epoch 1790:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1790\n",
      "Epoch 1791:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1791\n",
      "Epoch 1792:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1792\n",
      "Epoch 1793:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1793\n",
      "Epoch 1794:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1794\n",
      "Epoch 1795:  Train_loss=0.435  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1795\n",
      "Epoch 1796:  Train_loss=0.434  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1796\n",
      "Epoch 1797:  Train_loss=0.434  Val_loss=0.417  Val_Acc=0.821  eta=0.00010 global_step=1797\n",
      "Epoch 1798:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1798\n",
      "Epoch 1799:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1799\n",
      "Epoch 1800:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1800\n",
      "Epoch 1801:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1801\n",
      "Epoch 1802:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1802\n",
      "Epoch 1803:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1803\n",
      "Epoch 1804:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1804\n",
      "Epoch 1805:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1805\n",
      "Epoch 1806:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1806\n",
      "Epoch 1807:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1807\n",
      "Epoch 1808:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1808\n",
      "Epoch 1809:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1809\n",
      "Epoch 1810:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.821  eta=0.00010 global_step=1810\n",
      "Epoch 1811:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.828  eta=0.00010 global_step=1811\n",
      "Epoch 1812:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.828  eta=0.00010 global_step=1812\n",
      "Epoch 1813:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.828  eta=0.00010 global_step=1813\n",
      "Epoch 1814:  Train_loss=0.434  Val_loss=0.416  Val_Acc=0.828  eta=0.00010 global_step=1814\n",
      "Epoch 1815:  Train_loss=0.433  Val_loss=0.416  Val_Acc=0.828  eta=0.00010 global_step=1815\n",
      "Epoch 1816:  Train_loss=0.433  Val_loss=0.416  Val_Acc=0.828  eta=0.00010 global_step=1816\n",
      "Epoch 1817:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1817\n",
      "Epoch 1818:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1818\n",
      "Epoch 1819:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1819\n",
      "Epoch 1820:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1820\n",
      "Epoch 1821:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1821\n",
      "Epoch 1822:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1822\n",
      "Epoch 1823:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1823\n",
      "Epoch 1824:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1824\n",
      "Epoch 1825:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1825\n",
      "Epoch 1826:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1826\n",
      "Epoch 1827:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1827\n",
      "Epoch 1828:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1828\n",
      "Epoch 1829:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1829\n",
      "Epoch 1830:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1830\n",
      "Epoch 1831:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1831\n",
      "Epoch 1832:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1832\n",
      "Epoch 1833:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1833\n",
      "Epoch 1834:  Train_loss=0.433  Val_loss=0.415  Val_Acc=0.828  eta=0.00010 global_step=1834\n",
      "Epoch 1835:  Train_loss=0.432  Val_loss=0.415  Val_Acc=0.821  eta=0.00010 global_step=1835\n",
      "Epoch 1836:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1836\n",
      "Epoch 1837:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1837\n",
      "Epoch 1838:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1838\n",
      "Epoch 1839:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1839\n",
      "Epoch 1840:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1840\n",
      "Epoch 1841:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1841\n",
      "Epoch 1842:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1842\n",
      "Epoch 1843:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1843\n",
      "Epoch 1844:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1844\n",
      "Epoch 1845:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1845\n",
      "Epoch 1846:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1846\n",
      "Epoch 1847:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1847\n",
      "Epoch 1848:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1848\n",
      "Epoch 1849:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1849\n",
      "Epoch 1850:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1850\n",
      "Epoch 1851:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1851\n",
      "Epoch 1852:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1852\n",
      "Epoch 1853:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1853\n",
      "Epoch 1854:  Train_loss=0.432  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1854\n",
      "Epoch 1855:  Train_loss=0.431  Val_loss=0.414  Val_Acc=0.821  eta=0.00010 global_step=1855\n",
      "Epoch 1856:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1856\n",
      "Epoch 1857:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1857\n",
      "Epoch 1858:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1858\n",
      "Epoch 1859:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1859\n",
      "Epoch 1860:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1860\n",
      "Epoch 1861:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1861\n",
      "Epoch 1862:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1862\n",
      "Epoch 1863:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1863\n",
      "Epoch 1864:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1864\n",
      "Epoch 1865:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1865\n",
      "Epoch 1866:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1866\n",
      "Epoch 1867:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1867\n",
      "Epoch 1868:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1868\n",
      "Epoch 1869:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1869\n",
      "Epoch 1870:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1870\n",
      "Epoch 1871:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1871\n",
      "Epoch 1872:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1872\n",
      "Epoch 1873:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1874:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1874\n",
      "Epoch 1875:  Train_loss=0.431  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1875\n",
      "Epoch 1876:  Train_loss=0.430  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1876\n",
      "Epoch 1877:  Train_loss=0.430  Val_loss=0.413  Val_Acc=0.821  eta=0.00010 global_step=1877\n",
      "Epoch 1878:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1878\n",
      "Epoch 1879:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1879\n",
      "Epoch 1880:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1880\n",
      "Epoch 1881:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1881\n",
      "Epoch 1882:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1882\n",
      "Epoch 1883:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1883\n",
      "Epoch 1884:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1884\n",
      "Epoch 1885:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1885\n",
      "Epoch 1886:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1886\n",
      "Epoch 1887:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1887\n",
      "Epoch 1888:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1888\n",
      "Epoch 1889:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1889\n",
      "Epoch 1890:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1890\n",
      "Epoch 1891:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1891\n",
      "Epoch 1892:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1892\n",
      "Epoch 1893:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1893\n",
      "Epoch 1894:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1894\n",
      "Epoch 1895:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1895\n",
      "Epoch 1896:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1896\n",
      "Epoch 1897:  Train_loss=0.430  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1897\n",
      "Epoch 1898:  Train_loss=0.429  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1898\n",
      "Epoch 1899:  Train_loss=0.429  Val_loss=0.412  Val_Acc=0.821  eta=0.00010 global_step=1899\n",
      "Epoch 1900:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1900\n",
      "Epoch 1901:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1901\n",
      "Epoch 1902:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1902\n",
      "Epoch 1903:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1903\n",
      "Epoch 1904:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1904\n",
      "Epoch 1905:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1905\n",
      "Epoch 1906:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1906\n",
      "Epoch 1907:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1907\n",
      "Epoch 1908:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1908\n",
      "Epoch 1909:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1909\n",
      "Epoch 1910:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1910\n",
      "Epoch 1911:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1911\n",
      "Epoch 1912:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1912\n",
      "Epoch 1913:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1913\n",
      "Epoch 1914:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1914\n",
      "Epoch 1915:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1915\n",
      "Epoch 1916:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1916\n",
      "Epoch 1917:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1917\n",
      "Epoch 1918:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1918\n",
      "Epoch 1919:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1919\n",
      "Epoch 1920:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1920\n",
      "Epoch 1921:  Train_loss=0.429  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1921\n",
      "Epoch 1922:  Train_loss=0.428  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1922\n",
      "Epoch 1923:  Train_loss=0.428  Val_loss=0.411  Val_Acc=0.821  eta=0.00010 global_step=1923\n",
      "Epoch 1924:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1924\n",
      "Epoch 1925:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1925\n",
      "Epoch 1926:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1926\n",
      "Epoch 1927:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1927\n",
      "Epoch 1928:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1928\n",
      "Epoch 1929:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1929\n",
      "Epoch 1930:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1930\n",
      "Epoch 1931:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1931\n",
      "Epoch 1932:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1932\n",
      "Epoch 1933:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1933\n",
      "Epoch 1934:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1934\n",
      "Epoch 1935:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1935\n",
      "Epoch 1936:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1936\n",
      "Epoch 1937:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1937\n",
      "Epoch 1938:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1938\n",
      "Epoch 1939:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1939\n",
      "Epoch 1940:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1940\n",
      "Epoch 1941:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1941\n",
      "Epoch 1942:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1942\n",
      "Epoch 1943:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1943\n",
      "Epoch 1944:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1944\n",
      "Epoch 1945:  Train_loss=0.428  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1945\n",
      "Epoch 1946:  Train_loss=0.427  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1946\n",
      "Epoch 1947:  Train_loss=0.427  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1947\n",
      "Epoch 1948:  Train_loss=0.427  Val_loss=0.410  Val_Acc=0.821  eta=0.00010 global_step=1948\n",
      "Epoch 1949:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1949\n",
      "Epoch 1950:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1950\n",
      "Epoch 1951:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1951\n",
      "Epoch 1952:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1952\n",
      "Epoch 1953:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1953\n",
      "Epoch 1954:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1954\n",
      "Epoch 1955:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1955\n",
      "Epoch 1956:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1956\n",
      "Epoch 1957:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1957\n",
      "Epoch 1958:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1958\n",
      "Epoch 1959:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1959\n",
      "Epoch 1960:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1960\n",
      "Epoch 1961:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1961\n",
      "Epoch 1962:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1962\n",
      "Epoch 1963:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1963\n",
      "Epoch 1964:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1964\n",
      "Epoch 1965:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1965\n",
      "Epoch 1966:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1966\n",
      "Epoch 1967:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1967\n",
      "Epoch 1968:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1968\n",
      "Epoch 1969:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1969\n",
      "Epoch 1970:  Train_loss=0.427  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1970\n",
      "Epoch 1971:  Train_loss=0.426  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1971\n",
      "Epoch 1972:  Train_loss=0.426  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1972\n",
      "Epoch 1973:  Train_loss=0.426  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1973\n",
      "Epoch 1974:  Train_loss=0.426  Val_loss=0.409  Val_Acc=0.821  eta=0.00010 global_step=1974\n",
      "Epoch 1975:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1975\n",
      "Epoch 1976:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1976\n",
      "Epoch 1977:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1977\n",
      "Epoch 1978:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1978\n",
      "Epoch 1979:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1979\n",
      "Epoch 1980:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1980\n",
      "Epoch 1981:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1981\n",
      "Epoch 1982:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1982\n",
      "Epoch 1983:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1983\n",
      "Epoch 1984:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1984\n",
      "Epoch 1985:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1985\n",
      "Epoch 1986:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1986\n",
      "Epoch 1987:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1987\n",
      "Epoch 1988:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1988\n",
      "Epoch 1989:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1989\n",
      "Epoch 1990:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1990\n",
      "Epoch 1991:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1991\n",
      "Epoch 1992:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1992\n",
      "Epoch 1993:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1993\n",
      "Epoch 1994:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1994\n",
      "Epoch 1995:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1995\n",
      "Epoch 1996:  Train_loss=0.426  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1996\n",
      "Epoch 1997:  Train_loss=0.425  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1997\n",
      "Epoch 1998:  Train_loss=0.425  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1998\n",
      "Epoch 1999:  Train_loss=0.425  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=1999\n",
      "Epoch 2000:  Train_loss=0.425  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=2000\n",
      "Epoch 2001:  Train_loss=0.425  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=2001\n",
      "Epoch 2002:  Train_loss=0.425  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=2002\n",
      "Epoch 2003:  Train_loss=0.425  Val_loss=0.408  Val_Acc=0.821  eta=0.00010 global_step=2003\n",
      "Epoch 2004:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2004\n",
      "Epoch 2005:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2005\n",
      "Epoch 2006:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2006\n",
      "Epoch 2007:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2007\n",
      "Epoch 2008:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2008\n",
      "Epoch 2009:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2009\n",
      "Epoch 2010:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2010\n",
      "Epoch 2011:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2011\n",
      "Epoch 2012:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2012\n",
      "Epoch 2013:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2013\n",
      "Epoch 2014:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2015:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2015\n",
      "Epoch 2016:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2016\n",
      "Epoch 2017:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2017\n",
      "Epoch 2018:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2018\n",
      "Epoch 2019:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2019\n",
      "Epoch 2020:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2020\n",
      "Epoch 2021:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2021\n",
      "Epoch 2022:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.821  eta=0.00010 global_step=2022\n",
      "Epoch 2023:  Train_loss=0.425  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2023\n",
      "Epoch 2024:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2024\n",
      "Epoch 2025:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2025\n",
      "Epoch 2026:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2026\n",
      "Epoch 2027:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2027\n",
      "Epoch 2028:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2028\n",
      "Epoch 2029:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2029\n",
      "Epoch 2030:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2030\n",
      "Epoch 2031:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2031\n",
      "Epoch 2032:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2032\n",
      "Epoch 2033:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2033\n",
      "Epoch 2034:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.828  eta=0.00010 global_step=2034\n",
      "Epoch 2035:  Train_loss=0.424  Val_loss=0.407  Val_Acc=0.836  eta=0.00010 global_step=2035\n",
      "Epoch 2036:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2036\n",
      "Epoch 2037:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2037\n",
      "Epoch 2038:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2038\n",
      "Epoch 2039:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2039\n",
      "Epoch 2040:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2040\n",
      "Epoch 2041:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2041\n",
      "Epoch 2042:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2042\n",
      "Epoch 2043:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2043\n",
      "Epoch 2044:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2044\n",
      "Epoch 2045:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2045\n",
      "Epoch 2046:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2046\n",
      "Epoch 2047:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2047\n",
      "Epoch 2048:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2048\n",
      "Epoch 2049:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2049\n",
      "Epoch 2050:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2050\n",
      "Epoch 2051:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2051\n",
      "Epoch 2052:  Train_loss=0.424  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2052\n",
      "Epoch 2053:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2053\n",
      "Epoch 2054:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2054\n",
      "Epoch 2055:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2055\n",
      "Epoch 2056:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2056\n",
      "Epoch 2057:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2057\n",
      "Epoch 2058:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2058\n",
      "Epoch 2059:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2059\n",
      "Epoch 2060:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2060\n",
      "Epoch 2061:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2061\n",
      "Epoch 2062:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2062\n",
      "Epoch 2063:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2063\n",
      "Epoch 2064:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2064\n",
      "Epoch 2065:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2065\n",
      "Epoch 2066:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2066\n",
      "Epoch 2067:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2067\n",
      "Epoch 2068:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2068\n",
      "Epoch 2069:  Train_loss=0.423  Val_loss=0.406  Val_Acc=0.836  eta=0.00010 global_step=2069\n",
      "Epoch 2070:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2070\n",
      "Epoch 2071:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2071\n",
      "Epoch 2072:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2072\n",
      "Epoch 2073:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2073\n",
      "Epoch 2074:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2074\n",
      "Epoch 2075:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2075\n",
      "Epoch 2076:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2076\n",
      "Epoch 2077:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2077\n",
      "Epoch 2078:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2078\n",
      "Epoch 2079:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2079\n",
      "Epoch 2080:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2080\n",
      "Epoch 2081:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2081\n",
      "Epoch 2082:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2082\n",
      "Epoch 2083:  Train_loss=0.423  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2083\n",
      "Epoch 2084:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2084\n",
      "Epoch 2085:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2085\n",
      "Epoch 2086:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2086\n",
      "Epoch 2087:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2087\n",
      "Epoch 2088:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2088\n",
      "Epoch 2089:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2089\n",
      "Epoch 2090:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2090\n",
      "Epoch 2091:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2091\n",
      "Epoch 2092:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2092\n",
      "Epoch 2093:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2093\n",
      "Epoch 2094:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2094\n",
      "Epoch 2095:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2095\n",
      "Epoch 2096:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2096\n",
      "Epoch 2097:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2097\n",
      "Epoch 2098:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2098\n",
      "Epoch 2099:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2099\n",
      "Epoch 2100:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2100\n",
      "Epoch 2101:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2101\n",
      "Epoch 2102:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2102\n",
      "Epoch 2103:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2103\n",
      "Epoch 2104:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2104\n",
      "Epoch 2105:  Train_loss=0.422  Val_loss=0.405  Val_Acc=0.836  eta=0.00010 global_step=2105\n",
      "Epoch 2106:  Train_loss=0.422  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2106\n",
      "Epoch 2107:  Train_loss=0.422  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2107\n",
      "Epoch 2108:  Train_loss=0.422  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2108\n",
      "Epoch 2109:  Train_loss=0.422  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2109\n",
      "Epoch 2110:  Train_loss=0.422  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2110\n",
      "Epoch 2111:  Train_loss=0.422  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2111\n",
      "Epoch 2112:  Train_loss=0.422  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2112\n",
      "Epoch 2113:  Train_loss=0.422  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2113\n",
      "Epoch 2114:  Train_loss=0.422  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2114\n",
      "Epoch 2115:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2115\n",
      "Epoch 2116:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2116\n",
      "Epoch 2117:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2117\n",
      "Epoch 2118:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2118\n",
      "Epoch 2119:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2119\n",
      "Epoch 2120:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2120\n",
      "Epoch 2121:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2121\n",
      "Epoch 2122:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2122\n",
      "Epoch 2123:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2123\n",
      "Epoch 2124:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2124\n",
      "Epoch 2125:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2125\n",
      "Epoch 2126:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2126\n",
      "Epoch 2127:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2127\n",
      "Epoch 2128:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2128\n",
      "Epoch 2129:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2129\n",
      "Epoch 2130:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2130\n",
      "Epoch 2131:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2131\n",
      "Epoch 2132:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2132\n",
      "Epoch 2133:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2133\n",
      "Epoch 2134:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2134\n",
      "Epoch 2135:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2135\n",
      "Epoch 2136:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2136\n",
      "Epoch 2137:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2137\n",
      "Epoch 2138:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2138\n",
      "Epoch 2139:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2139\n",
      "Epoch 2140:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2140\n",
      "Epoch 2141:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2141\n",
      "Epoch 2142:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2142\n",
      "Epoch 2143:  Train_loss=0.421  Val_loss=0.404  Val_Acc=0.836  eta=0.00010 global_step=2143\n",
      "Epoch 2144:  Train_loss=0.421  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2144\n",
      "Epoch 2145:  Train_loss=0.421  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2145\n",
      "Epoch 2146:  Train_loss=0.421  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2146\n",
      "Epoch 2147:  Train_loss=0.421  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2147\n",
      "Epoch 2148:  Train_loss=0.421  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2148\n",
      "Epoch 2149:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2149\n",
      "Epoch 2150:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2150\n",
      "Epoch 2151:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2151\n",
      "Epoch 2152:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2152\n",
      "Epoch 2153:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2153\n",
      "Epoch 2154:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2154\n",
      "Epoch 2155:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2155\n",
      "Epoch 2156:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2156\n",
      "Epoch 2157:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2157\n",
      "Epoch 2158:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2158\n",
      "Epoch 2159:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2159\n",
      "Epoch 2160:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2160\n",
      "Epoch 2161:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2161\n",
      "Epoch 2162:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2163:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2163\n",
      "Epoch 2164:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2164\n",
      "Epoch 2165:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2165\n",
      "Epoch 2166:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2166\n",
      "Epoch 2167:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2167\n",
      "Epoch 2168:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2168\n",
      "Epoch 2169:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2169\n",
      "Epoch 2170:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2170\n",
      "Epoch 2171:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2171\n",
      "Epoch 2172:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2172\n",
      "Epoch 2173:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2173\n",
      "Epoch 2174:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2174\n",
      "Epoch 2175:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2175\n",
      "Epoch 2176:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2176\n",
      "Epoch 2177:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2177\n",
      "Epoch 2178:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2178\n",
      "Epoch 2179:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2179\n",
      "Epoch 2180:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2180\n",
      "Epoch 2181:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2181\n",
      "Epoch 2182:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2182\n",
      "Epoch 2183:  Train_loss=0.420  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2183\n",
      "Epoch 2184:  Train_loss=0.419  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2184\n",
      "Epoch 2185:  Train_loss=0.419  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2185\n",
      "Epoch 2186:  Train_loss=0.419  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2186\n",
      "Epoch 2187:  Train_loss=0.419  Val_loss=0.403  Val_Acc=0.836  eta=0.00010 global_step=2187\n",
      "Epoch 2188:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2188\n",
      "Epoch 2189:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2189\n",
      "Epoch 2190:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2190\n",
      "Epoch 2191:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2191\n",
      "Epoch 2192:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2192\n",
      "Epoch 2193:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2193\n",
      "Epoch 2194:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2194\n",
      "Epoch 2195:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2195\n",
      "Epoch 2196:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2196\n",
      "Epoch 2197:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2197\n",
      "Epoch 2198:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2198\n",
      "Epoch 2199:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2199\n",
      "Epoch 2200:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2200\n",
      "Epoch 2201:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2201\n",
      "Epoch 2202:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2202\n",
      "Epoch 2203:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2203\n",
      "Epoch 2204:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2204\n",
      "Epoch 2205:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2205\n",
      "Epoch 2206:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2206\n",
      "Epoch 2207:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2207\n",
      "Epoch 2208:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2208\n",
      "Epoch 2209:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2209\n",
      "Epoch 2210:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2210\n",
      "Epoch 2211:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2211\n",
      "Epoch 2212:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2212\n",
      "Epoch 2213:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2213\n",
      "Epoch 2214:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2214\n",
      "Epoch 2215:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2215\n",
      "Epoch 2216:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2216\n",
      "Epoch 2217:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2217\n",
      "Epoch 2218:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2218\n",
      "Epoch 2219:  Train_loss=0.419  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2219\n",
      "Epoch 2220:  Train_loss=0.418  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2220\n",
      "Epoch 2221:  Train_loss=0.418  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2221\n",
      "Epoch 2222:  Train_loss=0.418  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2222\n",
      "Epoch 2223:  Train_loss=0.418  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2223\n",
      "Epoch 2224:  Train_loss=0.418  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2224\n",
      "Epoch 2225:  Train_loss=0.418  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2225\n",
      "Epoch 2226:  Train_loss=0.418  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2226\n",
      "Epoch 2227:  Train_loss=0.418  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2227\n",
      "Epoch 2228:  Train_loss=0.418  Val_loss=0.402  Val_Acc=0.836  eta=0.00010 global_step=2228\n",
      "Epoch 2229:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2229\n",
      "Epoch 2230:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2230\n",
      "Epoch 2231:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2231\n",
      "Epoch 2232:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2232\n",
      "Epoch 2233:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2233\n",
      "Epoch 2234:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2234\n",
      "Epoch 2235:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2235\n",
      "Epoch 2236:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2236\n",
      "Epoch 2237:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2237\n",
      "Epoch 2238:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2238\n",
      "Epoch 2239:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2239\n",
      "Epoch 2240:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2240\n",
      "Epoch 2241:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2241\n",
      "Epoch 2242:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2242\n",
      "Epoch 2243:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2243\n",
      "Epoch 2244:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2244\n",
      "Epoch 2245:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2245\n",
      "Epoch 2246:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2246\n",
      "Epoch 2247:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2247\n",
      "Epoch 2248:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2248\n",
      "Epoch 2249:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2249\n",
      "Epoch 2250:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2250\n",
      "Epoch 2251:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2251\n",
      "Epoch 2252:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2252\n",
      "Epoch 2253:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2253\n",
      "Epoch 2254:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2254\n",
      "Epoch 2255:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2255\n",
      "Epoch 2256:  Train_loss=0.418  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2256\n",
      "Epoch 2257:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2257\n",
      "Epoch 2258:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2258\n",
      "Epoch 2259:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2259\n",
      "Epoch 2260:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2260\n",
      "Epoch 2261:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2261\n",
      "Epoch 2262:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2262\n",
      "Epoch 2263:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2263\n",
      "Epoch 2264:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2264\n",
      "Epoch 2265:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2265\n",
      "Epoch 2266:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2266\n",
      "Epoch 2267:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2267\n",
      "Epoch 2268:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2268\n",
      "Epoch 2269:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2269\n",
      "Epoch 2270:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2270\n",
      "Epoch 2271:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2271\n",
      "Epoch 2272:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2272\n",
      "Epoch 2273:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2273\n",
      "Epoch 2274:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2274\n",
      "Epoch 2275:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2275\n",
      "Epoch 2276:  Train_loss=0.417  Val_loss=0.401  Val_Acc=0.836  eta=0.00010 global_step=2276\n",
      "Epoch 2277:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2277\n",
      "Epoch 2278:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2278\n",
      "Epoch 2279:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2279\n",
      "Epoch 2280:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2280\n",
      "Epoch 2281:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2281\n",
      "Epoch 2282:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2282\n",
      "Epoch 2283:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2283\n",
      "Epoch 2284:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2284\n",
      "Epoch 2285:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2285\n",
      "Epoch 2286:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2286\n",
      "Epoch 2287:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2287\n",
      "Epoch 2288:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2288\n",
      "Epoch 2289:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2289\n",
      "Epoch 2290:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2290\n",
      "Epoch 2291:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2291\n",
      "Epoch 2292:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2292\n",
      "Epoch 2293:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2293\n",
      "Epoch 2294:  Train_loss=0.417  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2294\n",
      "Epoch 2295:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2295\n",
      "Epoch 2296:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2296\n",
      "Epoch 2297:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2297\n",
      "Epoch 2298:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2298\n",
      "Epoch 2299:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2299\n",
      "Epoch 2300:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2300\n",
      "Epoch 2301:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2301\n",
      "Epoch 2302:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2302\n",
      "Epoch 2303:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2303\n",
      "Epoch 2304:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2304\n",
      "Epoch 2305:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2305\n",
      "Epoch 2306:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2306\n",
      "Epoch 2307:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2307\n",
      "Epoch 2308:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2309:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2309\n",
      "Epoch 2310:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2310\n",
      "Epoch 2311:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2311\n",
      "Epoch 2312:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2312\n",
      "Epoch 2313:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2313\n",
      "Epoch 2314:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2314\n",
      "Epoch 2315:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2315\n",
      "Epoch 2316:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2316\n",
      "Epoch 2317:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2317\n",
      "Epoch 2318:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2318\n",
      "Epoch 2319:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2319\n",
      "Epoch 2320:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2320\n",
      "Epoch 2321:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2321\n",
      "Epoch 2322:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2322\n",
      "Epoch 2323:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2323\n",
      "Epoch 2324:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2324\n",
      "Epoch 2325:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2325\n",
      "Epoch 2326:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2326\n",
      "Epoch 2327:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2327\n",
      "Epoch 2328:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2328\n",
      "Epoch 2329:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2329\n",
      "Epoch 2330:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2330\n",
      "Epoch 2331:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2331\n",
      "Epoch 2332:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2332\n",
      "Epoch 2333:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2333\n",
      "Epoch 2334:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2334\n",
      "Epoch 2335:  Train_loss=0.416  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2335\n",
      "Epoch 2336:  Train_loss=0.415  Val_loss=0.400  Val_Acc=0.836  eta=0.00010 global_step=2336\n",
      "Epoch 2337:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2337\n",
      "Epoch 2338:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2338\n",
      "Epoch 2339:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2339\n",
      "Epoch 2340:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2340\n",
      "Epoch 2341:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2341\n",
      "Epoch 2342:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2342\n",
      "Epoch 2343:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2343\n",
      "Epoch 2344:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2344\n",
      "Epoch 2345:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2345\n",
      "Epoch 2346:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2346\n",
      "Epoch 2347:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2347\n",
      "Epoch 2348:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2348\n",
      "Epoch 2349:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2349\n",
      "Epoch 2350:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2350\n",
      "Epoch 2351:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2351\n",
      "Epoch 2352:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2352\n",
      "Epoch 2353:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2353\n",
      "Epoch 2354:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2354\n",
      "Epoch 2355:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2355\n",
      "Epoch 2356:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2356\n",
      "Epoch 2357:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2357\n",
      "Epoch 2358:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2358\n",
      "Epoch 2359:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2359\n",
      "Epoch 2360:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2360\n",
      "Epoch 2361:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2361\n",
      "Epoch 2362:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2362\n",
      "Epoch 2363:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2363\n",
      "Epoch 2364:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2364\n",
      "Epoch 2365:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2365\n",
      "Epoch 2366:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2366\n",
      "Epoch 2367:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2367\n",
      "Epoch 2368:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2368\n",
      "Epoch 2369:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2369\n",
      "Epoch 2370:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2370\n",
      "Epoch 2371:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2371\n",
      "Epoch 2372:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2372\n",
      "Epoch 2373:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2373\n",
      "Epoch 2374:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2374\n",
      "Epoch 2375:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2375\n",
      "Epoch 2376:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2376\n",
      "Epoch 2377:  Train_loss=0.415  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2377\n",
      "Epoch 2378:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2378\n",
      "Epoch 2379:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2379\n",
      "Epoch 2380:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2380\n",
      "Epoch 2381:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2381\n",
      "Epoch 2382:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2382\n",
      "Epoch 2383:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2383\n",
      "Epoch 2384:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2384\n",
      "Epoch 2385:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2385\n",
      "Epoch 2386:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2386\n",
      "Epoch 2387:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2387\n",
      "Epoch 2388:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2388\n",
      "Epoch 2389:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2389\n",
      "Epoch 2390:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2390\n",
      "Epoch 2391:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2391\n",
      "Epoch 2392:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2392\n",
      "Epoch 2393:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2393\n",
      "Epoch 2394:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2394\n",
      "Epoch 2395:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2395\n",
      "Epoch 2396:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2396\n",
      "Epoch 2397:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2397\n",
      "Epoch 2398:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2398\n",
      "Epoch 2399:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2399\n",
      "Epoch 2400:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2400\n",
      "Epoch 2401:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2401\n",
      "Epoch 2402:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2402\n",
      "Epoch 2403:  Train_loss=0.414  Val_loss=0.399  Val_Acc=0.836  eta=0.00010 global_step=2403\n",
      "Epoch 2404:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2404\n",
      "Epoch 2405:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2405\n",
      "Epoch 2406:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2406\n",
      "Epoch 2407:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2407\n",
      "Epoch 2408:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2408\n",
      "Epoch 2409:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2409\n",
      "Epoch 2410:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2410\n",
      "Epoch 2411:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2411\n",
      "Epoch 2412:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2412\n",
      "Epoch 2413:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2413\n",
      "Epoch 2414:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2414\n",
      "Epoch 2415:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2415\n",
      "Epoch 2416:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2416\n",
      "Epoch 2417:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2417\n",
      "Epoch 2418:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2418\n",
      "Epoch 2419:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2419\n",
      "Epoch 2420:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2420\n",
      "Epoch 2421:  Train_loss=0.414  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2421\n",
      "Epoch 2422:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2422\n",
      "Epoch 2423:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2423\n",
      "Epoch 2424:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2424\n",
      "Epoch 2425:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2425\n",
      "Epoch 2426:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2426\n",
      "Epoch 2427:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2427\n",
      "Epoch 2428:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2428\n",
      "Epoch 2429:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2429\n",
      "Epoch 2430:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2430\n",
      "Epoch 2431:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2431\n",
      "Epoch 2432:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2432\n",
      "Epoch 2433:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2433\n",
      "Epoch 2434:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2434\n",
      "Epoch 2435:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2435\n",
      "Epoch 2436:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2436\n",
      "Epoch 2437:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2437\n",
      "Epoch 2438:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2438\n",
      "Epoch 2439:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2439\n",
      "Epoch 2440:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2440\n",
      "Epoch 2441:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2441\n",
      "Epoch 2442:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2442\n",
      "Epoch 2443:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2443\n",
      "Epoch 2444:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2444\n",
      "Epoch 2445:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2445\n",
      "Epoch 2446:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2446\n",
      "Epoch 2447:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2447\n",
      "Epoch 2448:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2448\n",
      "Epoch 2449:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2449\n",
      "Epoch 2450:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2450\n",
      "Epoch 2451:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2451\n",
      "Epoch 2452:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2452\n",
      "Epoch 2453:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2453\n",
      "Epoch 2454:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2454\n",
      "Epoch 2455:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2455\n",
      "Epoch 2456:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2457:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2457\n",
      "Epoch 2458:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2458\n",
      "Epoch 2459:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2459\n",
      "Epoch 2460:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2460\n",
      "Epoch 2461:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2461\n",
      "Epoch 2462:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2462\n",
      "Epoch 2463:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2463\n",
      "Epoch 2464:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2464\n",
      "Epoch 2465:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2465\n",
      "Epoch 2466:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2466\n",
      "Epoch 2467:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2467\n",
      "Epoch 2468:  Train_loss=0.413  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2468\n",
      "Epoch 2469:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2469\n",
      "Epoch 2470:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2470\n",
      "Epoch 2471:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2471\n",
      "Epoch 2472:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2472\n",
      "Epoch 2473:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2473\n",
      "Epoch 2474:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2474\n",
      "Epoch 2475:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2475\n",
      "Epoch 2476:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2476\n",
      "Epoch 2477:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2477\n",
      "Epoch 2478:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2478\n",
      "Epoch 2479:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2479\n",
      "Epoch 2480:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2480\n",
      "Epoch 2481:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2481\n",
      "Epoch 2482:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2482\n",
      "Epoch 2483:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2483\n",
      "Epoch 2484:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2484\n",
      "Epoch 2485:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2485\n",
      "Epoch 2486:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2486\n",
      "Epoch 2487:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2487\n",
      "Epoch 2488:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2488\n",
      "Epoch 2489:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2489\n",
      "Epoch 2490:  Train_loss=0.412  Val_loss=0.398  Val_Acc=0.836  eta=0.00010 global_step=2490\n",
      "Epoch 2491:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2491\n",
      "Epoch 2492:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2492\n",
      "Epoch 2493:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2493\n",
      "Epoch 2494:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2494\n",
      "Epoch 2495:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2495\n",
      "Epoch 2496:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2496\n",
      "Epoch 2497:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2497\n",
      "Epoch 2498:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2498\n",
      "Epoch 2499:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2499\n",
      "Epoch 2500:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2500\n",
      "Epoch 2501:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2501\n",
      "Epoch 2502:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2502\n",
      "Epoch 2503:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2503\n",
      "Epoch 2504:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2504\n",
      "Epoch 2505:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2505\n",
      "Epoch 2506:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2506\n",
      "Epoch 2507:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2507\n",
      "Epoch 2508:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2508\n",
      "Epoch 2509:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2509\n",
      "Epoch 2510:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2510\n",
      "Epoch 2511:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2511\n",
      "Epoch 2512:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2512\n",
      "Epoch 2513:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2513\n",
      "Epoch 2514:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2514\n",
      "Epoch 2515:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2515\n",
      "Epoch 2516:  Train_loss=0.412  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2516\n",
      "Epoch 2517:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2517\n",
      "Epoch 2518:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2518\n",
      "Epoch 2519:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2519\n",
      "Epoch 2520:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2520\n",
      "Epoch 2521:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2521\n",
      "Epoch 2522:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2522\n",
      "Epoch 2523:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2523\n",
      "Epoch 2524:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2524\n",
      "Epoch 2525:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2525\n",
      "Epoch 2526:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2526\n",
      "Epoch 2527:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2527\n",
      "Epoch 2528:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2528\n",
      "Epoch 2529:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2529\n",
      "Epoch 2530:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2530\n",
      "Epoch 2531:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2531\n",
      "Epoch 2532:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2532\n",
      "Epoch 2533:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2533\n",
      "Epoch 2534:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2534\n",
      "Epoch 2535:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2535\n",
      "Epoch 2536:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2536\n",
      "Epoch 2537:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2537\n",
      "Epoch 2538:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2538\n",
      "Epoch 2539:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2539\n",
      "Epoch 2540:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2540\n",
      "Epoch 2541:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2541\n",
      "Epoch 2542:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2542\n",
      "Epoch 2543:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2543\n",
      "Epoch 2544:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2544\n",
      "Epoch 2545:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2545\n",
      "Epoch 2546:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2546\n",
      "Epoch 2547:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2547\n",
      "Epoch 2548:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2548\n",
      "Epoch 2549:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2549\n",
      "Epoch 2550:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2550\n",
      "Epoch 2551:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2551\n",
      "Epoch 2552:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2552\n",
      "Epoch 2553:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2553\n",
      "Epoch 2554:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2554\n",
      "Epoch 2555:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2555\n",
      "Epoch 2556:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2556\n",
      "Epoch 2557:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2557\n",
      "Epoch 2558:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2558\n",
      "Epoch 2559:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2559\n",
      "Epoch 2560:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2560\n",
      "Epoch 2561:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2561\n",
      "Epoch 2562:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2562\n",
      "Epoch 2563:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2563\n",
      "Epoch 2564:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2564\n",
      "Epoch 2565:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2565\n",
      "Epoch 2566:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2566\n",
      "Epoch 2567:  Train_loss=0.411  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2567\n",
      "Epoch 2568:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2568\n",
      "Epoch 2569:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2569\n",
      "Epoch 2570:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2570\n",
      "Epoch 2571:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2571\n",
      "Epoch 2572:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2572\n",
      "Epoch 2573:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2573\n",
      "Epoch 2574:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2574\n",
      "Epoch 2575:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2575\n",
      "Epoch 2576:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2576\n",
      "Epoch 2577:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2577\n",
      "Epoch 2578:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2578\n",
      "Epoch 2579:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2579\n",
      "Epoch 2580:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2580\n",
      "Epoch 2581:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2581\n",
      "Epoch 2582:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2582\n",
      "Epoch 2583:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2583\n",
      "Epoch 2584:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2584\n",
      "Epoch 2585:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2585\n",
      "Epoch 2586:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2586\n",
      "Epoch 2587:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2587\n",
      "Epoch 2588:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2588\n",
      "Epoch 2589:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2589\n",
      "Epoch 2590:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2590\n",
      "Epoch 2591:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2591\n",
      "Epoch 2592:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2592\n",
      "Epoch 2593:  Train_loss=0.410  Val_loss=0.397  Val_Acc=0.836  eta=0.00010 global_step=2593\n",
      "Epoch 2594:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2594\n",
      "Epoch 2595:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2595\n",
      "Epoch 2596:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2596\n",
      "Epoch 2597:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2597\n",
      "Epoch 2598:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2598\n",
      "Epoch 2599:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2600:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2600\n",
      "Epoch 2601:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2601\n",
      "Epoch 2602:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2602\n",
      "Epoch 2603:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2603\n",
      "Epoch 2604:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2604\n",
      "Epoch 2605:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2605\n",
      "Epoch 2606:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2606\n",
      "Epoch 2607:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2607\n",
      "Epoch 2608:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.836  eta=0.00010 global_step=2608\n",
      "Epoch 2609:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2609\n",
      "Epoch 2610:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2610\n",
      "Epoch 2611:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2611\n",
      "Epoch 2612:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2612\n",
      "Epoch 2613:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2613\n",
      "Epoch 2614:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2614\n",
      "Epoch 2615:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2615\n",
      "Epoch 2616:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2616\n",
      "Epoch 2617:  Train_loss=0.410  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2617\n",
      "Epoch 2618:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2618\n",
      "Epoch 2619:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2619\n",
      "Epoch 2620:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2620\n",
      "Epoch 2621:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2621\n",
      "Epoch 2622:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2622\n",
      "Epoch 2623:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2623\n",
      "Epoch 2624:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2624\n",
      "Epoch 2625:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2625\n",
      "Epoch 2626:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2626\n",
      "Epoch 2627:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2627\n",
      "Epoch 2628:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2628\n",
      "Epoch 2629:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2629\n",
      "Epoch 2630:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2630\n",
      "Epoch 2631:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2631\n",
      "Epoch 2632:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2632\n",
      "Epoch 2633:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2633\n",
      "Epoch 2634:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2634\n",
      "Epoch 2635:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2635\n",
      "Epoch 2636:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2636\n",
      "Epoch 2637:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2637\n",
      "Epoch 2638:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2638\n",
      "Epoch 2639:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2639\n",
      "Epoch 2640:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2640\n",
      "Epoch 2641:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2641\n",
      "Epoch 2642:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2642\n",
      "Epoch 2643:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2643\n",
      "Epoch 2644:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2644\n",
      "Epoch 2645:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2645\n",
      "Epoch 2646:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2646\n",
      "Epoch 2647:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2647\n",
      "Epoch 2648:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2648\n",
      "Epoch 2649:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2649\n",
      "Epoch 2650:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2650\n",
      "Epoch 2651:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2651\n",
      "Epoch 2652:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2652\n",
      "Epoch 2653:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2653\n",
      "Epoch 2654:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2654\n",
      "Epoch 2655:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2655\n",
      "Epoch 2656:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2656\n",
      "Epoch 2657:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2657\n",
      "Epoch 2658:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2658\n",
      "Epoch 2659:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2659\n",
      "Epoch 2660:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2660\n",
      "Epoch 2661:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2661\n",
      "Epoch 2662:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2662\n",
      "Epoch 2663:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2663\n",
      "Epoch 2664:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2664\n",
      "Epoch 2665:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2665\n",
      "Epoch 2666:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2666\n",
      "Epoch 2667:  Train_loss=0.409  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2667\n",
      "Epoch 2668:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2668\n",
      "Epoch 2669:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2669\n",
      "Epoch 2670:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2670\n",
      "Epoch 2671:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2671\n",
      "Epoch 2672:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2672\n",
      "Epoch 2673:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2673\n",
      "Epoch 2674:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2674\n",
      "Epoch 2675:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2675\n",
      "Epoch 2676:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2676\n",
      "Epoch 2677:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2677\n",
      "Epoch 2678:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2678\n",
      "Epoch 2679:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2679\n",
      "Epoch 2680:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2680\n",
      "Epoch 2681:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2681\n",
      "Epoch 2682:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2682\n",
      "Epoch 2683:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2683\n",
      "Epoch 2684:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2684\n",
      "Epoch 2685:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2685\n",
      "Epoch 2686:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2686\n",
      "Epoch 2687:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2687\n",
      "Epoch 2688:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2688\n",
      "Epoch 2689:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2689\n",
      "Epoch 2690:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2690\n",
      "Epoch 2691:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2691\n",
      "Epoch 2692:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2692\n",
      "Epoch 2693:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2693\n",
      "Epoch 2694:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2694\n",
      "Epoch 2695:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2695\n",
      "Epoch 2696:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2696\n",
      "Epoch 2697:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2697\n",
      "Epoch 2698:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2698\n",
      "Epoch 2699:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2699\n",
      "Epoch 2700:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2700\n",
      "Epoch 2701:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2701\n",
      "Epoch 2702:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2702\n",
      "Epoch 2703:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2703\n",
      "Epoch 2704:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2704\n",
      "Epoch 2705:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2705\n",
      "Epoch 2706:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2706\n",
      "Epoch 2707:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2707\n",
      "Epoch 2708:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2708\n",
      "Epoch 2709:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2709\n",
      "Epoch 2710:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2710\n",
      "Epoch 2711:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2711\n",
      "Epoch 2712:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2712\n",
      "Epoch 2713:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2713\n",
      "Epoch 2714:  Train_loss=0.408  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2714\n",
      "Epoch 2715:  Train_loss=0.407  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2715\n",
      "Epoch 2716:  Train_loss=0.407  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2716\n",
      "Epoch 2717:  Train_loss=0.407  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2717\n",
      "Epoch 2718:  Train_loss=0.407  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2718\n",
      "Epoch 2719:  Train_loss=0.407  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2719\n",
      "Epoch 2720:  Train_loss=0.407  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2720\n",
      "Epoch 2721:  Train_loss=0.407  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2721\n",
      "Epoch 2722:  Train_loss=0.407  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2722\n",
      "Epoch 2723:  Train_loss=0.407  Val_loss=0.396  Val_Acc=0.828  eta=0.00010 global_step=2723\n",
      "Epoch 2724:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2724\n",
      "Epoch 2725:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2725\n",
      "Epoch 2726:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2726\n",
      "Epoch 2727:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2727\n",
      "Epoch 2728:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2728\n",
      "Epoch 2729:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2729\n",
      "Epoch 2730:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2730\n",
      "Epoch 2731:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2731\n",
      "Epoch 2732:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2732\n",
      "Epoch 2733:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2734:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2734\n",
      "Epoch 2735:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2735\n",
      "Epoch 2736:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2736\n",
      "Epoch 2737:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2737\n",
      "Epoch 2738:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2738\n",
      "Epoch 2739:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2739\n",
      "Epoch 2740:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2740\n",
      "Epoch 2741:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2741\n",
      "Epoch 2742:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2742\n",
      "Epoch 2743:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2743\n",
      "Epoch 2744:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2744\n",
      "Epoch 2745:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2745\n",
      "Epoch 2746:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2746\n",
      "Epoch 2747:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2747\n",
      "Epoch 2748:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2748\n",
      "Epoch 2749:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2749\n",
      "Epoch 2750:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2750\n",
      "Epoch 2751:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2751\n",
      "Epoch 2752:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2752\n",
      "Epoch 2753:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2753\n",
      "Epoch 2754:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2754\n",
      "Epoch 2755:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2755\n",
      "Epoch 2756:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2756\n",
      "Epoch 2757:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2757\n",
      "Epoch 2758:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2758\n",
      "Epoch 2759:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2759\n",
      "Epoch 2760:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2760\n",
      "Epoch 2761:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2761\n",
      "Epoch 2762:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2762\n",
      "Epoch 2763:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2763\n",
      "Epoch 2764:  Train_loss=0.407  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2764\n",
      "Epoch 2765:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2765\n",
      "Epoch 2766:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2766\n",
      "Epoch 2767:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2767\n",
      "Epoch 2768:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2768\n",
      "Epoch 2769:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2769\n",
      "Epoch 2770:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2770\n",
      "Epoch 2771:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2771\n",
      "Epoch 2772:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2772\n",
      "Epoch 2773:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2773\n",
      "Epoch 2774:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2774\n",
      "Epoch 2775:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2775\n",
      "Epoch 2776:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2776\n",
      "Epoch 2777:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2777\n",
      "Epoch 2778:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2778\n",
      "Epoch 2779:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2779\n",
      "Epoch 2780:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2780\n",
      "Epoch 2781:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2781\n",
      "Epoch 2782:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2782\n",
      "Epoch 2783:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2783\n",
      "Epoch 2784:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2784\n",
      "Epoch 2785:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2785\n",
      "Epoch 2786:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2786\n",
      "Epoch 2787:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2787\n",
      "Epoch 2788:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2788\n",
      "Epoch 2789:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2789\n",
      "Epoch 2790:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2790\n",
      "Epoch 2791:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2791\n",
      "Epoch 2792:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2792\n",
      "Epoch 2793:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2793\n",
      "Epoch 2794:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2794\n",
      "Epoch 2795:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2795\n",
      "Epoch 2796:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2796\n",
      "Epoch 2797:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2797\n",
      "Epoch 2798:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2798\n",
      "Epoch 2799:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2799\n",
      "Epoch 2800:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2800\n",
      "Epoch 2801:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2801\n",
      "Epoch 2802:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2802\n",
      "Epoch 2803:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2803\n",
      "Epoch 2804:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2804\n",
      "Epoch 2805:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2805\n",
      "Epoch 2806:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2806\n",
      "Epoch 2807:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2807\n",
      "Epoch 2808:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2808\n",
      "Epoch 2809:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2809\n",
      "Epoch 2810:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2810\n",
      "Epoch 2811:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2811\n",
      "Epoch 2812:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2812\n",
      "Epoch 2813:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2813\n",
      "Epoch 2814:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2814\n",
      "Epoch 2815:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2815\n",
      "Epoch 2816:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2816\n",
      "Epoch 2817:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2817\n",
      "Epoch 2818:  Train_loss=0.406  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2818\n",
      "Epoch 2819:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2819\n",
      "Epoch 2820:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2820\n",
      "Epoch 2821:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2821\n",
      "Epoch 2822:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2822\n",
      "Epoch 2823:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2823\n",
      "Epoch 2824:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2824\n",
      "Epoch 2825:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2825\n",
      "Epoch 2826:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2826\n",
      "Epoch 2827:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2827\n",
      "Epoch 2828:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2828\n",
      "Epoch 2829:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2829\n",
      "Epoch 2830:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2830\n",
      "Epoch 2831:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2831\n",
      "Epoch 2832:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2832\n",
      "Epoch 2833:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2833\n",
      "Epoch 2834:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2834\n",
      "Epoch 2835:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2835\n",
      "Epoch 2836:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2836\n",
      "Epoch 2837:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2837\n",
      "Epoch 2838:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2838\n",
      "Epoch 2839:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2839\n",
      "Epoch 2840:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2840\n",
      "Epoch 2841:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2841\n",
      "Epoch 2842:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2842\n",
      "Epoch 2843:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2843\n",
      "Epoch 2844:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2845:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2845\n",
      "Epoch 2846:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2846\n",
      "Epoch 2847:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2847\n",
      "Epoch 2848:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2848\n",
      "Epoch 2849:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2849\n",
      "Epoch 2850:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2850\n",
      "Epoch 2851:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2851\n",
      "Epoch 2852:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2852\n",
      "Epoch 2853:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2853\n",
      "Epoch 2854:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2854\n",
      "Epoch 2855:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2855\n",
      "Epoch 2856:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2856\n",
      "Epoch 2857:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2857\n",
      "Epoch 2858:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2858\n",
      "Epoch 2859:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2859\n",
      "Epoch 2860:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2860\n",
      "Epoch 2861:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2861\n",
      "Epoch 2862:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2862\n",
      "Epoch 2863:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2863\n",
      "Epoch 2864:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2864\n",
      "Epoch 2865:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2865\n",
      "Epoch 2866:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2866\n",
      "Epoch 2867:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2867\n",
      "Epoch 2868:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2868\n",
      "Epoch 2869:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2869\n",
      "Epoch 2870:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2870\n",
      "Epoch 2871:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2871\n",
      "Epoch 2872:  Train_loss=0.405  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2872\n",
      "Epoch 2873:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2873\n",
      "Epoch 2874:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2874\n",
      "Epoch 2875:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2875\n",
      "Epoch 2876:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2876\n",
      "Epoch 2877:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2877\n",
      "Epoch 2878:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2878\n",
      "Epoch 2879:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2879\n",
      "Epoch 2880:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2880\n",
      "Epoch 2881:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2881\n",
      "Epoch 2882:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2882\n",
      "Epoch 2883:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2883\n",
      "Epoch 2884:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2884\n",
      "Epoch 2885:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2885\n",
      "Epoch 2886:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2886\n",
      "Epoch 2887:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2887\n",
      "Epoch 2888:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2888\n",
      "Epoch 2889:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2889\n",
      "Epoch 2890:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2890\n",
      "Epoch 2891:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2891\n",
      "Epoch 2892:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2892\n",
      "Epoch 2893:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2893\n",
      "Epoch 2894:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2894\n",
      "Epoch 2895:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2895\n",
      "Epoch 2896:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2896\n",
      "Epoch 2897:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2897\n",
      "Epoch 2898:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2898\n",
      "Epoch 2899:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2899\n",
      "Epoch 2900:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2900\n",
      "Epoch 2901:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2901\n",
      "Epoch 2902:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2902\n",
      "Epoch 2903:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2903\n",
      "Epoch 2904:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2904\n",
      "Epoch 2905:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2905\n",
      "Epoch 2906:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2906\n",
      "Epoch 2907:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2907\n",
      "Epoch 2908:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2908\n",
      "Epoch 2909:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2909\n",
      "Epoch 2910:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2910\n",
      "Epoch 2911:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2911\n",
      "Epoch 2912:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2912\n",
      "Epoch 2913:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2913\n",
      "Epoch 2914:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2914\n",
      "Epoch 2915:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2915\n",
      "Epoch 2916:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2916\n",
      "Epoch 2917:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2917\n",
      "Epoch 2918:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2918\n",
      "Epoch 2919:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2919\n",
      "Epoch 2920:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2920\n",
      "Epoch 2921:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2921\n",
      "Epoch 2922:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2922\n",
      "Epoch 2923:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2923\n",
      "Epoch 2924:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2924\n",
      "Epoch 2925:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2925\n",
      "Epoch 2926:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2926\n",
      "Epoch 2927:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2927\n",
      "Epoch 2928:  Train_loss=0.404  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2928\n",
      "Epoch 2929:  Train_loss=0.403  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2929\n",
      "Epoch 2930:  Train_loss=0.403  Val_loss=0.395  Val_Acc=0.828  eta=0.00010 global_step=2930\n",
      "Epoch 2931:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2931\n",
      "Epoch 2932:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2932\n",
      "Epoch 2933:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2933\n",
      "Epoch 2934:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2934\n",
      "Epoch 2935:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2935\n",
      "Epoch 2936:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2936\n",
      "Epoch 2937:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2937\n",
      "Epoch 2938:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2938\n",
      "Epoch 2939:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2939\n",
      "Epoch 2940:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2940\n",
      "Epoch 2941:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2941\n",
      "Epoch 2942:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2942\n",
      "Epoch 2943:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2943\n",
      "Epoch 2944:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2944\n",
      "Epoch 2945:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2945\n",
      "Epoch 2946:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2946\n",
      "Epoch 2947:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2947\n",
      "Epoch 2948:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2948\n",
      "Epoch 2949:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2949\n",
      "Epoch 2950:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2950\n",
      "Epoch 2951:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2951\n",
      "Epoch 2952:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2952\n",
      "Epoch 2953:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2953\n",
      "Epoch 2954:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2954\n",
      "Epoch 2955:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2955\n",
      "Epoch 2956:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2956\n",
      "Epoch 2957:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2957\n",
      "Epoch 2958:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2958\n",
      "Epoch 2959:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2959\n",
      "Epoch 2960:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2960\n",
      "Epoch 2961:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2961\n",
      "Epoch 2962:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2962\n",
      "Epoch 2963:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2963\n",
      "Epoch 2964:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2964\n",
      "Epoch 2965:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2965\n",
      "Epoch 2966:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2966\n",
      "Epoch 2967:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2968:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2968\n",
      "Epoch 2969:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2969\n",
      "Epoch 2970:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2970\n",
      "Epoch 2971:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2971\n",
      "Epoch 2972:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2972\n",
      "Epoch 2973:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2973\n",
      "Epoch 2974:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2974\n",
      "Epoch 2975:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2975\n",
      "Epoch 2976:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2976\n",
      "Epoch 2977:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2977\n",
      "Epoch 2978:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2978\n",
      "Epoch 2979:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2979\n",
      "Epoch 2980:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2980\n",
      "Epoch 2981:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2981\n",
      "Epoch 2982:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2982\n",
      "Epoch 2983:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2983\n",
      "Epoch 2984:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2984\n",
      "Epoch 2985:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2985\n",
      "Epoch 2986:  Train_loss=0.403  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2986\n",
      "Epoch 2987:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2987\n",
      "Epoch 2988:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2988\n",
      "Epoch 2989:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2989\n",
      "Epoch 2990:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2990\n",
      "Epoch 2991:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2991\n",
      "Epoch 2992:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2992\n",
      "Epoch 2993:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2993\n",
      "Epoch 2994:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2994\n",
      "Epoch 2995:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2995\n",
      "Epoch 2996:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2996\n",
      "Epoch 2997:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2997\n",
      "Epoch 2998:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2998\n",
      "Epoch 2999:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=2999\n",
      "Epoch 3000:  Train_loss=0.402  Val_loss=0.394  Val_Acc=0.828  eta=0.00010 global_step=3000\n"
     ]
    }
   ],
   "source": [
    "for epoch_counter in range(3000):\n",
    "    train_data = {X: X_train, Y_true: y_train}\n",
    "    _, train_loss = sess.run([train_step, mean_loss], feed_dict=train_data)\n",
    "    \n",
    "    val_data = {X: X_val, Y_true: y_val}\n",
    "    val_loss, val_acc = sess.run([mean_loss, accuracy], feed_dict=val_data)\n",
    "    \n",
    "    print (\"Epoch %d:  Train_loss=%0.3f  Val_loss=%0.3f  Val_Acc=%0.3f  eta=%0.5f global_step=%d\"\n",
    "          %(epoch_counter+1,\n",
    "            train_loss,\n",
    "            val_loss,\n",
    "            val_acc,\n",
    "            learning_rate,\n",
    "            global_step.eval(session=sess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = tf.argmax(tf.nn.softmax(Y), 1)\n",
    "predictions = predict.eval(feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'PassengerId': test_id, 'Survived':predictions})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('my_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
